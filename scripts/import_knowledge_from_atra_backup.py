#!/usr/bin/env python3
"""
–ò–º–ø–æ—Ä—Ç —É–∑–ª–æ–≤ –∑–Ω–∞–Ω–∏–π –∏–∑ atra backups.
–ò—Å—Ç–æ—á–Ω–∏–∫–∏: ~/Documents/dev/atra/backups/knowledge_os_*.sql.gz

–ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨:
- –¢–æ–ª—å–∫–æ INSERT, –Ω–∏–∫–∞–∫–æ–≥–æ DDL (CREATE/ALTER/DROP/TRUNCATE)
- –í—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –≤ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü–µ
- domain_id, embedding –Ω–µ —Ç—Ä–æ–≥–∞–µ–º (–¥—Ä—É–≥–∞—è —Å—Ö–µ–º–∞/FK)
- DRY_RUN=1 ‚Äî —Ç–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –±—É–¥–µ—Ç –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ
"""
import asyncio
import gzip
import os
import re
import sys
from datetime import datetime
from pathlib import Path

# –ü—É—Ç–∏ –∫ –¥–∞–º–ø–∞–º atra (–º–æ–∂–Ω–æ –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —á–µ—Ä–µ–∑ ATRA_BACKUP_PATH)
ATRA_BACKUPS = Path(os.getenv("ATRA_BACKUP_PATH", str(Path.home() / "Documents" / "dev" / "atra" / "backups")))
DUMP_PATTERNS = [
    "knowledge_os_20260122_214735.sql.gz",  # 21 MB
    "knowledge_os_20260120_125507.sql.gz",  # 19 MB
]

DB_URL = os.getenv("DATABASE_URL", "postgresql://admin:secret@localhost:5432/knowledge_os")


def find_best_dump() -> Path | None:
    """–ù–∞–π—Ç–∏ —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π –¥–∞–º–ø knowledge_os."""
    # –ü—Ä—è–º–æ–π –ø—É—Ç—å –∏–∑ env
    direct = os.getenv("KNOWLEDGE_DUMP_PATH")
    if direct and Path(direct).exists():
        return Path(direct)
    candidates = []
    base = Path(ATRA_BACKUPS) if isinstance(ATRA_BACKUPS, str) else ATRA_BACKUPS
    for p in base.glob("knowledge_os_*.sql.gz"):
        if "remote" not in p.name and p.stat().st_size > 1_000_000:
            candidates.append((p.stat().st_mtime, p))
    # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º kn_dump.sql.gz (–ø—Ä–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–∏ –≤ /tmp)
    for p in base.glob("kn_dump.sql.gz"):
        if p.stat().st_size > 1_000_000:
            candidates.append((p.stat().st_mtime, p))
    return max(candidates)[1] if candidates else None


def extract_knowledge_nodes_from_dump(path: Path) -> list[dict]:
    """–ò–∑–≤–ª–µ—á—å —Å—Ç—Ä–æ–∫–∏ knowledge_nodes –∏–∑ pg_dump (COPY format)."""
    nodes = []
    in_copy = False
    columns = None
    
    with gzip.open(path, "rt", encoding="utf-8", errors="replace") as f:
        for line in f:
            if line.startswith("COPY public.knowledge_nodes "):
                in_copy = True
                # –ü–∞—Ä—Å–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫: COPY public.knowledge_nodes (col1, col2, ...) FROM stdin;
                m = re.search(r"\((.*?)\)", line)
                columns = [c.strip() for c in m.group(1).split(",")] if m else []
                continue
            if in_copy:
                if line.strip() == r"\.":
                    break
                # –¢–∞–±—É–ª—è—Ü–∏—è –º–µ–∂–¥—É –∫–æ–ª–æ–Ω–∫–∞–º–∏
                parts = line.rstrip("\n").split("\t")
                if len(parts) >= 3 and columns:
                    row = dict(zip(columns, parts))
                    nodes.append(row)
    return nodes


async def import_nodes(nodes: list[dict]) -> int:
    """
    –ò–º–ø–æ—Ä—Ç –≤ knowledge_postgres.
    –í–ê–ñ–ù–û: –≤—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û –≤ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü–µ.
    –ù–∏–∫–∞–∫–æ–≥–æ DDL ‚Äî –Ω–µ –º–µ–Ω—è–µ–º —Å—Ö–µ–º—É, –Ω–µ —Ç—Ä–æ–≥–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ.
    """
    try:
        import asyncpg
    except ImportError:
        print("pip install asyncpg")
        return 0

    conn = await asyncpg.connect(DB_URL)

    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã
    cols = await conn.fetch("""
        SELECT column_name FROM information_schema.columns
        WHERE table_name = 'knowledge_nodes' AND table_schema = 'public'
        ORDER BY ordinal_position
    """)
    target_columns = {r["column_name"] for r in cols}
    print(f"   –ö–æ–ª–æ–Ω–∫–∏ –≤ —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü–µ: {sorted(target_columns)}")

    # –¢–æ–ª—å–∫–æ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –µ—Å—Ç—å –≤ —Ü–µ–ª–µ–≤–æ–π —Ç–∞–±–ª–∏—Ü–µ.
    # –ù–µ —Ç—Ä–æ–≥–∞–µ–º: id, domain_id (FK), embedding (—Ä–∞–∑–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å).
    insert_cols = [c for c in ["content", "metadata", "confidence_score", "is_verified", "usage_count", "source_ref", "created_at"] if c in target_columns]

    if "content" not in insert_cols:
        print("‚ùå –ö–æ–ª–æ–Ω–∫–∞ content –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞ ‚Äî –ø—Ä–µ—Ä—ã–≤–∞–µ–º")
        await conn.close()
        return 0

    def _val(v, default=None):
        if v is None or v == "\\N" or v == "":
            return default
        return v

    dry_run = os.getenv("DRY_RUN", "").lower() in ("1", "true", "yes")
    if dry_run:
        would_insert = sum(1 for r in nodes if (_val(r.get("content"), "") or ""))
        print(f"   ‚ö†Ô∏è DRY_RUN ‚Äî –±—É–¥–µ—Ç –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ ~{would_insert} —É–∑–ª–æ–≤")
        await conn.close()
        return 0

    inserted = 0
    skipped = 0

    for r in nodes:
        try:
            content = _val(r.get("content"), "") or ""
            if not content:
                continue
            metadata = _val(r.get("metadata"), "{}")
            if isinstance(metadata, str) and not metadata.startswith("{"):
                metadata = "{}"
            conf = float(_val(r.get("confidence_score"), "0.5") or 0.5)
            verified = str(_val(r.get("is_verified"), "f")).lower() in ("t", "true", "1")
            usage = int(_val(r.get("usage_count"), "0") or 0)
            source_ref = _val(r.get("source_ref"))
            created_at = _val(r.get("created_at"))

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏
            cols_sql = []
            vals = []
            for col in ["content", "metadata", "confidence_score", "is_verified", "usage_count", "source_ref", "created_at"]:
                if col not in insert_cols:
                    continue
                if col == "content":
                    vals.append(content)
                elif col == "metadata":
                    vals.append(metadata)
                elif col == "confidence_score":
                    vals.append(conf)
                elif col == "is_verified":
                    vals.append(verified)
                elif col == "usage_count":
                    vals.append(usage)
                elif col == "source_ref":
                    vals.append(source_ref)
                elif col == "created_at":
                    try:
                        dt = datetime.fromisoformat(created_at.replace("+00", "+00:00")) if created_at else None
                    except Exception:
                        dt = None
                    vals.append(dt)
                cols_sql.append(col)

            # created_at NULL ‚Üí NOW() —á–µ—Ä–µ–∑ COALESCE
            ph = []
            for i, c in enumerate(cols_sql):
                n = i + 1
                if c == "created_at":
                    ph.append(f"COALESCE(${n}::timestamptz, NOW())")
                elif c == "metadata":
                    ph.append(f"${n}::jsonb")
                else:
                    ph.append(f"${n}")

            sql = f"INSERT INTO knowledge_nodes ({', '.join(cols_sql)}) VALUES ({', '.join(ph)})"
            await conn.execute(sql, *vals)
            inserted += 1
            if inserted % 1000 == 0:
                print(f"   ... {inserted}")
        except Exception as e:
            skipped += 1
            if "duplicate" not in str(e).lower() and "violates" not in str(e).lower():
                print(f"   ‚ö†Ô∏è {e}")
    
    total = await conn.fetchval("SELECT COUNT(*) FROM knowledge_nodes")
    await conn.close()
    print(f"–ò–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ: {inserted}, –ø—Ä–æ–ø—É—â–µ–Ω–æ: {skipped}, –≤—Å–µ–≥–æ –≤ –ë–î: {total}")
    return inserted


async def main():
    dump = find_best_dump()
    if not dump:
        print("‚ùå –î–∞–º–ø –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:", ATRA_BACKUPS)
        return 1
    
    print(f"üìÇ –î–∞–º–ø: {dump} ({dump.stat().st_size / 1e6:.1f} MB)")
    print("üì• –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —É–∑–ª–æ–≤...")
    nodes = extract_knowledge_nodes_from_dump(dump)
    print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(nodes)} —É–∑–ª–æ–≤")
    
    if not nodes:
        print("‚ùå –£–∑–ª—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return 1
    
    print("üíæ –ò–º–ø–æ—Ä—Ç –≤ –ë–î...")
    await import_nodes(nodes)
    return 0


if __name__ == "__main__":
    sys.exit(asyncio.run(main()))
