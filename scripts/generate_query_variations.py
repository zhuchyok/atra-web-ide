#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –≤–∞—Ä–∏–∞—Ü–∏–π –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è validation set.
–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ QA: —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –ø–æ–∫—Ä—ã—Ç–∏—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏; Technical Writer: —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫.
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python3 scripts/generate_query_variations.py --max-per-query 2
"""
import argparse
import json
import re
import sys
from pathlib import Path
from typing import Any, Dict, List

REPO_ROOT = Path(__file__).resolve().parent.parent

# –°–∏–Ω–æ–Ω–∏–º—ã –∏ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫–∏ (Technical Writer: –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ)
REPHRASE_PAIRS = [
    (r"\b–∫–∞–∫\b", "–∫–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º"),
    (r"\b–ø–æ–¥—Å–∫–∞–∂–∏\b", "—Ä–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ"),
    (r"\b–ø–æ–∫–∞–∂–∏\b", "–ø–æ–∫–∞–∂–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ"),
    (r"\b—á—Ç–æ —Ç–∞–∫–æ–µ\b", "–æ–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ"),
    (r"\b—Å–∫–æ–ª—å–∫–æ\b", "–∫–∞–∫–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å"),
    (r"\b–≥–¥–µ\b", "–≥–¥–µ –Ω–∞–π—Ç–∏"),
    (r"\b–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã\b", "—Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã"),
    (r"\b–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\b", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ"),
    (r"\b–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å\b", "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é"),
    (r"\b–º–µ—Ç—Ä–∏–∫–∏\b", "–º–µ—Ç—Ä–∏–∫–∏ –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥"),
    (r"\b–∫–∞–∫ —Å–æ–∑–¥–∞—Ç—å\b", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é"),
    (r"\b–∫–∞–∫ –æ—Ç–º–µ–Ω–∏—Ç—å\b", "–ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –æ—Ç–º–µ–Ω—ã"),
    (r"\b–∫–∞–∫ —Å–±—Ä–æ—Å–∏—Ç—å\b", "—Å–±—Ä–æ—Å –ø–∞—Ä–æ–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"),
    (r"\b—á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ\b", "FAQ —á–∞—Å—Ç–æ –∑–∞–¥–∞–≤–∞–µ–º—ã–µ"),
    (r"\b—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ\b", "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é"),
    (r"\b–∑–∞–ø—É—Å—Ç–∏—Ç—å\b", "–ª–æ–∫–∞–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞"),
    (r"\b–ø–æ—Ä—Ç—ã\b", "–ø–æ—Ä—Ç—ã —Å–µ—Ä–≤–∏—Å–æ–≤"),
    (r"\b—Ç–∞—Ä–∏—Ñ—ã\b", "—Ç–∞—Ä–∏—Ñ—ã –∏ —Ü–µ–Ω—ã"),
    (r"\b—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç"),
    (r"\b–æ—Ç–º–µ–Ω–∞\b", "–æ—Ç–º–µ–Ω–∞ –ø–æ–¥–ø–∏—Å–∫–∏"),
]
SYNONYMS = {
    "–ø–æ–¥–ø–∏—Å–∫–∞": ["—Ç–∞—Ä–∏—Ñ", "–ø–ª–∞–Ω"],
    "–ø–æ–¥–¥–µ—Ä–∂–∫–∞": ["—Å–∞–ø–ø–æ—Ä—Ç", "–ø–æ–º–æ—â—å"],
    "—Å–ø—Ä–∞–≤–∫–∞": ["—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", "help"],
    "–Ω–∞—Å—Ç—Ä–æ–∏—Ç—å": ["–Ω–∞—Å—Ç—Ä–æ–π–∫–∞", "–∫–æ–Ω—Ñ–∏–≥"],
    "API": ["—ç–Ω–¥–ø–æ–∏–Ω—Ç—ã", "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å"],
    "–∞–∫–∫–∞—É–Ω—Ç": ["—É—á—ë—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å", "account"],
    "–ø–∞—Ä–æ–ª—å": ["password", "–≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ"],
    "–∫–æ–Ω—Ç–∞–∫—Ç—ã": ["–∫–æ–Ω—Ç–∞–∫—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è", "—Å–≤—è–∑–∞—Ç—å—Å—è"],
    "—Ü–µ–Ω": ["—Å—Ç–æ–∏–º–æ—Å—Ç—å", "–ø—Ä–∞–π—Å"],
    "–≤–æ–ø—Ä–æ—Å": ["FAQ", "–≤–æ–ø—Ä–æ—Å—ã"],
    "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω": ["–ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ", "–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è"],
}


def normalize_for_dedup(text: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏ (Data Engineer)."""
    return " ".join(text.lower().split()).strip()


def generate_variations(query_text: str, max_variations: int = 2) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–æ max_variations –≤–∞—Ä–∏–∞—Ü–∏–π –∑–∞–ø—Ä–æ—Å–∞ (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏)."""
    variations = []
    seen = {normalize_for_dedup(query_text)}

    # –í–∞—Ä–∏–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∫—É
    for pattern, repl in REPHRASE_PAIRS:
        if len(variations) >= max_variations:
            break
        new_q = re.sub(pattern, repl, query_text, count=1, flags=re.IGNORECASE)
        if new_q != query_text:
            norm = normalize_for_dedup(new_q)
            if norm not in seen:
                seen.add(norm)
                variations.append(new_q.strip())

    # –í–∞—Ä–∏–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ —Å–∏–Ω–æ–Ω–∏–º—ã (–æ–¥–Ω–∞ –∑–∞–º–µ–Ω–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å)
    for word, syns in SYNONYMS.items():
        if len(variations) >= max_variations:
            break
        if word.lower() in query_text.lower():
            for syn in syns:
                new_q = re.sub(rf"\b{re.escape(word)}\b", syn, query_text, count=1, flags=re.IGNORECASE)
                if new_q != query_text:
                    norm = normalize_for_dedup(new_q)
                    if norm not in seen:
                        seen.add(norm)
                        variations.append(new_q.strip())
                        break

    return variations[:max_variations]


def main() -> int:
    parser = argparse.ArgumentParser(
        description="Generate synthetic query variations for validation set (QA: coverage)"
    )
    parser.add_argument(
        "--dataset",
        default="data/validation_queries.json",
        help="Path to validation_queries.json",
    )
    parser.add_argument(
        "--output",
        default="data/synthetic_query_variations.json",
        help="Output path for variations",
    )
    parser.add_argument(
        "--max-per-query",
        type=int,
        default=2,
        help="Max variations per source query",
    )
    args = parser.parse_args()

    path = REPO_ROOT / args.dataset
    if not path.exists():
        print(f"‚ùå Dataset not found: {path}", file=sys.stderr)
        return 1

    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    queries = data.get("queries", data) if isinstance(data, dict) else data
    if not queries:
        print("No queries in dataset.", file=sys.stderr)
        return 1

    out_queries: List[Dict[str, Any]] = []
    for item in queries:
        q = item.get("query")
        if not q or len(q) < 3:
            continue
        ref = item.get("reference")
        ctx = item.get("context_expected", [])
        base_id = item.get("id", "v")
        for i, var_text in enumerate(generate_variations(q, args.max_per_query)):
            out_queries.append({
                "id": f"{base_id}_var{i+1}",
                "query": var_text,
                "reference": ref,
                "context_expected": ctx,
                "source_id": base_id,
                "source": "synthetic_variation",
            })

    out_path = REPO_ROOT / args.output
    out_path.parent.mkdir(parents=True, exist_ok=True)
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(
            {"queries": out_queries, "version": "1.0", "source_dataset": str(args.dataset)},
            f,
            indent=2,
            ensure_ascii=False,
        )
    print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(out_queries)} –≤–∞—Ä–∏–∞—Ü–∏–π ‚Üí {out_path}")
    print("üí° –û–±—ä–µ–¥–∏–Ω–∏—Ç—å —Å validation set: python3 scripts/merge_validation_sources.py")
    return 0


if __name__ == "__main__":
    sys.exit(main())
