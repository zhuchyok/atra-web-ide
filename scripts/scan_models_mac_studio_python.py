#!/usr/bin/env python3
"""
–ü–æ–ª–Ω–æ–µ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ Mac Studio M4 Max
–ó–∞–ø—É—Å–∫–∞–π—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –Ω–∞ Mac Studio –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
"""

import os
import subprocess
from pathlib import Path
from typing import Dict, List, Tuple

def get_ollama_models() -> Tuple[List[Dict], float]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ Ollama –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö –æ–±—â–∏–π —Ä–∞–∑–º–µ—Ä"""
    models = []
    total_size = 0
    
    try:
        result = subprocess.run(['ollama', 'list'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')[1:]  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
            for line in lines:
                if line.strip():
                    parts = line.split()
                    if len(parts) >= 2:
                        name = parts[0]
                        size_str = parts[2] if len(parts) > 2 else "0"
                        # –ü–∞—Ä—Å–∏–º —Ä–∞–∑–º–µ—Ä
                        size_gb = 0
                        if 'GB' in size_str:
                            size_gb = float(size_str.replace('GB', '').strip())
                        elif 'MB' in size_str:
                            size_gb = float(size_str.replace('MB', '').strip()) / 1024
                        models.append({
                            'name': name,
                            'size': size_str,
                            'size_gb': size_gb
                        })
                        total_size += size_gb
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è Ollama –º–æ–¥–µ–ª–µ–π: {e}")
    
    return models, total_size


def get_mlx_models_hf_cache() -> Tuple[List[Dict], float]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ MLX –º–æ–¥–µ–ª–µ–π –∏–∑ HuggingFace –∫–µ—à–∞"""
    models = []
    total_size = 0
    
    hf_cache = Path.home() / ".cache" / "huggingface" / "hub"
    if not hf_cache.exists():
        return models, total_size
    
    mlx_dirs = [d for d in hf_cache.iterdir() if d.is_dir() and "mlx-community" in d.name]
    
    for mlx_dir in mlx_dirs:
        model_name = mlx_dir.name.replace("models--", "").replace("--", "/")
        try:
            size = sum(f.stat().st_size for f in mlx_dir.rglob('*') if f.is_file())
            size_gb = size / (1024**3)
            models.append({
                'name': model_name,
                'path': str(mlx_dir),
                'size_gb': size_gb
            })
            total_size += size_gb
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {model_name}: {e}")
    
    return models, total_size


def get_mlx_models_dir() -> Tuple[List[Dict], float]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ MLX –º–æ–¥–µ–ª–µ–π –∏–∑ ~/.mlx_models"""
    models = []
    total_size = 0
    
    mlx_dir = Path.home() / ".mlx_models"
    if not mlx_dir.exists():
        return models, total_size
    
    model_dirs = [d for d in mlx_dir.iterdir() if d.is_dir()]
    
    for model_dir in model_dirs:
        try:
            size = sum(f.stat().st_size for f in model_dir.rglob('*') if f.is_file())
            size_gb = size / (1024**3)
            models.append({
                'name': model_dir.name,
                'path': str(model_dir),
                'size_gb': size_gb
            })
            total_size += size_gb
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {model_dir.name}: {e}")
    
    return models, total_size


def check_ollama_api() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama API"""
    try:
        import httpx
        response = httpx.get("http://localhost:11434/api/tags", timeout=2)
        return response.status_code == 200
    except:
        try:
            import urllib.request
            response = urllib.request.urlopen("http://localhost:11434/api/tags", timeout=2)
            return response.status == 200
        except:
            return False


def check_mlx_available() -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å MLX"""
    try:
        import mlx.core as mx
        return True
    except ImportError:
        return False


def main():
    print("=" * 70)
    print("üîç –ü–û–õ–ù–û–ï –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ù–ê MAC STUDIO M4 MAX")
    print("=" * 70)
    print()
    
    # 1. Ollama –º–æ–¥–µ–ª–∏
    print("üì¶ OLLAMA –ú–û–î–ï–õ–ò:")
    print("-" * 70)
    ollama_models, ollama_total = get_ollama_models()
    if ollama_models:
        for model in ollama_models:
            print(f"  ‚úÖ {model['name']:40} {model['size']:>10}")
        print(f"\n  –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä Ollama: {ollama_total:.2f} GB")
    else:
        print("  ‚ö†Ô∏è  Ollama –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    print()
    print()
    
    # 2. MLX –º–æ–¥–µ–ª–∏ –≤ HuggingFace –∫–µ—à–µ
    print("üçé MLX –ú–û–î–ï–õ–ò (HuggingFace –∫–µ—à):")
    print("-" * 70)
    mlx_hf_models, mlx_hf_total = get_mlx_models_hf_cache()
    if mlx_hf_models:
        for model in mlx_hf_models:
            print(f"  ‚úÖ {model['name']:50} {model['size_gb']:>6.2f} GB")
        print(f"\n  –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä MLX (HF cache): {mlx_hf_total:.2f} GB")
    else:
        print("  ‚ö†Ô∏è  MLX –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ HuggingFace –∫–µ—à–µ")
    
    print()
    print()
    
    # 3. MLX –º–æ–¥–µ–ª–∏ –≤ ~/.mlx_models
    print("üìÅ MLX –ú–û–î–ï–õ–ò (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è ~/.mlx_models):")
    print("-" * 70)
    mlx_dir_models, mlx_dir_total = get_mlx_models_dir()
    if mlx_dir_models:
        for model in mlx_dir_models:
            print(f"  ‚úÖ {model['name']:50} {model['size_gb']:>6.2f} GB")
        print(f"\n  –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä MLX (~/.mlx_models): {mlx_dir_total:.2f} GB")
    else:
        print("  ‚ö†Ô∏è  –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è ~/.mlx_models –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏–ª–∏ –ø—É—Å—Ç–∞")
    
    print()
    print()
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ä–≤–∏—Å–æ–≤
    print("üåê –ü–†–û–í–ï–†–ö–ê –°–ï–†–í–ò–°–û–í:")
    print("-" * 70)
    ollama_api = check_ollama_api()
    print(f"  Ollama API (localhost:11434): {'‚úÖ –î–æ—Å—Ç—É–ø–µ–Ω' if ollama_api else '‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}")
    mlx_available = check_mlx_available()
    print(f"  MLX –±–∏–±–ª–∏–æ—Ç–µ–∫–∞: {'‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞' if mlx_available else '‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞'}")
    
    print()
    print()
    
    # 5. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("üíæ –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("-" * 70)
    total_size = ollama_total + mlx_hf_total + mlx_dir_total
    print(f"  Ollama –º–æ–¥–µ–ª–∏:        {ollama_total:>8.2f} GB")
    print(f"  MLX (HF cache):       {mlx_hf_total:>8.2f} GB")
    print(f"  MLX (~/.mlx_models):  {mlx_dir_total:>8.2f} GB")
    print(f"  {'-' * 30}")
    print(f"  –ò–¢–û–ì–û:                {total_size:>8.2f} GB")
    
    print()
    print("=" * 70)
    print("‚úÖ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print()
    print("üìã –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ —ç—Ç–æ—Ç –æ—Ç—á–µ—Ç –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏")
    print("=" * 70)


if __name__ == "__main__":
    main()

