#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è mlx_api_server.py –Ω–∞ Mac Studio
–ó–∞–ø—É—Å–∫–∞—Ç—å –Ω–∞ Mac Studio: python3 scripts/copy_mlx_server_to_macstudio.py
"""

import os
import shutil

# –ù–∞—Ö–æ–¥–∏–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (atra-web-ide –∏–ª–∏ atra)
root = None
for d in [
    os.path.expanduser("~/Documents/atra-web-ide"),
    os.path.expanduser("~/Documents/dev/atra"),
    os.path.expanduser("~/atra"),
    os.path.expanduser("~/Documents/GITHUB/atra/atra"),
]:
    p = os.path.join(d, "knowledge_os", "docker-compose.yml") if "atra" in d else os.path.join(d, "docker-compose.yml")
    if os.path.exists(p) or os.path.exists(os.path.join(d, "knowledge_os", "app", "mlx_api_server.py")):
        root = d
        break

if not root:
    print("‚ùå –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω")
    exit(1)

api_file = os.path.join(root, "knowledge_os", "app", "mlx_api_server.py")
os.makedirs(os.path.dirname(api_file), exist_ok=True)

# –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ (–ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è)
content = '''"""
MLX API Server –¥–ª—è Mac Studio M4 Max
FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ MLX –º–æ–¥–µ–ª–∏
"""

import asyncio
import logging
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional
import json
import os
from mlx_lm import load, generate
import sys

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ mlx_router –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(__file__))

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="MLX Model Server", version="1.0.0")

# –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
_models_cache = {}

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (–ø—É—Ç–∏ –∫ MLX –º–æ–¥–µ–ª—è–º)
MODEL_PATHS = {
    "default": os.path.expanduser("~/.mlx_models/Qwen2.5-Coder-32B-Instruct-Q8"),
}

# –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
MLX_MODELS_DIR = os.getenv("MLX_MODELS_DIR", os.path.expanduser("~/.mlx_models"))


class GenerateRequest(BaseModel):
    prompt: str
    model: Optional[str] = None
    max_tokens: int = 512
    temperature: float = 0.7
    stream: bool = False


def get_model(model_key: str):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å"""
    if model_key in _models_cache:
        return _models_cache[model_key]
    
    model_path = MODEL_PATHS.get(model_key)
    if not model_path:
        model_path = os.path.join(MLX_MODELS_DIR, model_key)
    
    if not model_path or not os.path.exists(model_path):
        raise ValueError(f"Model {model_key} not found at {model_path}")
    
    logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_key} –∏–∑ {model_path}")
    model, tokenizer = load(model_path)
    
    _models_cache[model_key] = {"model": model, "tokenizer": tokenizer}
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_key}")
    return _models_cache[model_key]


@app.get("/")
async def root():
    """Health check"""
    return {
        "status": "online",
        "server": "MLX Model Server",
        "device": "Mac Studio M4 Max",
        "models_loaded": len(_models_cache),
        "available_models": list(MODEL_PATHS.keys())
    }


@app.get("/api/tags")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Ollama API)"""
    return {
        "models": [
            {"name": name, "model": name, "size": 0, "format": "mlx", "exists": os.path.exists(MODEL_PATHS.get(name, ""))}
            for name in MODEL_PATHS.keys()
        ]
    }


@app.post("/api/generate")
async def generate_text(request: GenerateRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Ollama API)"""
    try:
        model_key = request.model or "default"
        model_data = get_model(model_key)
        model, tokenizer = model_data["model"], model_data["tokenizer"]
        if request.stream:
            return StreamingResponse(
                generate_stream(model, tokenizer, request.prompt, request.max_tokens),
                media_type="application/json"
            )
        loop = asyncio.get_event_loop()
        response_text = await loop.run_in_executor(
            None,
            lambda: generate(model, tokenizer, prompt=request.prompt, max_tokens=request.max_tokens)
        )
        return {"model": model_key, "response": response_text, "done": True}
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def generate_stream(model, tokenizer, prompt: str, max_tokens: int):
    loop = asyncio.get_event_loop()
    response = await loop.run_in_executor(None, lambda: generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens))
    for char in response:
        yield json.dumps({"response": char, "done": False}) + "\\n"
    yield json.dumps({"response": "", "done": True}) + "\\n"


@app.get("/api/models/{model_name}")
async def get_model_info(model_name: str):
    if model_name not in MODEL_PATHS:
        raise HTTPException(status_code=404, detail="Model not found")
    p = MODEL_PATHS[model_name]
    return {"name": model_name, "path": p, "exists": os.path.exists(p), "loaded": model_name in _models_cache}


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=11434)
'''

if os.path.exists(api_file):
    os.remove(api_file)
    print("üóëÔ∏è  –°—Ç–∞—Ä—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω")

with open(api_file, 'w', encoding='utf-8') as f:
    f.write(content)

import py_compile
try:
    py_compile.compile(api_file, doraise=True)
    print(f"‚úÖ –§–∞–π–ª —Å–æ–∑–¥–∞–Ω –∏ –∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω: {api_file}")
except py_compile.PyCompileError as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞: {e}")
    exit(1)

print("\nüöÄ –ó–∞–ø—É—Å–∫:")
print(f"   cd {root}")
print("   export PYTHONPATH=\"$(pwd):$PYTHONPATH\"")
print("   python3 -m uvicorn knowledge_os.app.mlx_api_server:app --host 0.0.0.0 --port 11434")
