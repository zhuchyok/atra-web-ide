"""
Ollama Client (–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π
Retry logic, timeout handling, connection pooling
"""
import httpx
from typing import AsyncGenerator, Optional, List
import logging
import json
import asyncio

from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class OllamaClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è Ollama API —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    
    # –ú–æ–¥–µ–ª–∏ (70b/104b —É–¥–∞–ª–µ–Ω—ã)
    MODELS = {
        "complex": "qwen2.5-coder:32b",
        "enterprise": "qwen2.5-coder:32b",
        "reasoning": "qwq:32b",
        "complex_alt": "qwen2.5-coder:32b",
        "coding": "qwen2.5-coder:32b",                 # ~20GB - –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥
        "fast": "phi3.5:3.8b",                         # ~2.5GB - –ë—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏
        "fast_light": "phi3:mini-4k",                  # ~2GB - –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
        "default": "qwen2.5:3b",                       # ~2GB - –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        "tiny": "tinyllama:1.1b-chat"                  # ~700MB - –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–µ
    }
    
    # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    FAST_MODEL = "qwen2.5:3b"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def __init__(self, base_url: Optional[str] = None):
        self.base_url = base_url or settings.ollama_url
        self.timeout = httpx.Timeout(
            settings.ollama_timeout,
            connect=10.0
        )
        self.max_retries = 2
        self.retry_delay = 2.0
    
    async def _retry_request(self, func, *args, **kwargs):
        """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                return await func(*args, **kwargs)
            except (httpx.HTTPError, httpx.TimeoutException) as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt)
                    logger.warning(
                        f"Ollama request failed (attempt {attempt + 1}/{self.max_retries}), "
                        f"retrying in {delay}s: {e}"
                    )
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Ollama request failed after {self.max_retries} attempts: {e}")
        
        raise last_error
    
    async def list_models(self) -> List[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        async def _make_request():
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                return response.json()
        
        try:
            data = await self._retry_request(_make_request)
            return data.get("models", [])
        except httpx.HTTPError as e:
            logger.error(f"Ollama list_models error: {e}")
            return []
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None,
        stream: bool = False
    ) -> dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            system: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            stream: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∏–º–∏–Ω–≥
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        model = model or settings.default_model
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        if system:
            payload["system"] = system
        
        async def _make_request():
            logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Ollama: {self.base_url}/api/generate")
            logger.info(f"üì¶ Payload: model={payload.get('model')}, prompt_length={len(payload.get('prompt', ''))}")
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=payload
                )
                logger.info(f"üì• –û—Ç–≤–µ—Ç Ollama: HTTP {response.status_code}")
                if response.status_code != 200:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ Ollama: {response.status_code} - {response.text[:200]}")
                response.raise_for_status()
                return response.json()
        
        try:
            return await self._retry_request(_make_request)
        except httpx.HTTPError as e:
            logger.error(f"Ollama generate error: {e}")
            return {"error": str(e)}
    
    async def generate_stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        –°—Ç—Ä–∏–º–∏–Ω–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Yields:
            JSON —Å—Ç—Ä–æ–∫–∏ —Å —á–∞—Å—Ç—è–º–∏ –æ—Ç–≤–µ—Ç–∞
        """
        model = model or settings.default_model
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": True
        }
        if system:
            payload["system"] = system
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                async with client.stream(
                    "POST",
                    f"{self.base_url}/api/generate",
                    json=payload
                ) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if line:
                            yield line
            except httpx.HTTPError as e:
                logger.error(f"Ollama stream error: {e}")
                yield json.dumps({"error": str(e)})
    
    async def health(self) -> dict:
        """Health check Ollama"""
        async def _make_request():
            async with httpx.AsyncClient(timeout=httpx.Timeout(5.0)) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                return {"status": "healthy"}
        
        try:
            return await self._retry_request(_make_request)
        except httpx.HTTPError as e:
            return {"status": "unhealthy", "error": str(e)}


# Singleton instance
ollama_client = OllamaClient()


async def get_ollama_client() -> OllamaClient:
    """Dependency –¥–ª—è FastAPI"""
    return ollama_client
