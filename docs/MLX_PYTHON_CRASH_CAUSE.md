# Почему падает Python при работе MLX API Server

Разбор крашей и принятые меры (решение на основе анализа отчётов macOS и практик ограничения памяти/кэша; команда экспертов из Docker на момент внедрения была недоступна).

## Причина (по краш-репорту macOS)

Разбор отчёта `~/Library/Logs/DiagnosticReports/Python-*.ips` (например, Python-2026-02-09-180251.ips):

- **Сигнал:** `SIGABRT` (Abort trap: 6), в отчёте: `"asi" : {"libsystem_c.dylib":["abort() called"]}`.
- **Цепочка вызовов на упавшем потоке** (`com.Metal.CompletionQueueDispatch`):
  1. `abort` → `__abort_message` → `demangling_terminate_handler` → `_objc_terminate` → `std::__terminate`
  2. `__cxxabiv1::failed_throw` → `__cxa_throw`
  3. **`mlx::core::gpu::check_error(MTL::CommandBuffer*)`** — библиотека MLX проверяет результат выполнения Metal Command Buffer и обнаруживает ошибку GPU
  4. Далее по стеку: `MTL::CommandBuffer::addCompletedHandler`, `IOGPUMetalCommandBuffer`, `_MTLCommandQueue` — то есть падение происходит в **callback завершения GPU-команды**.

**Вывод:** процесс падает из‑за **ошибки Metal/GPU** при выполнении команд MLX. MLX в колбэке завершения Command Buffer вызывает `check_error`, получает ошибку от драйвера, бросает C++ исключение; оно доходит до `std::terminate` и вызывается `abort()` — Python завершается без возможности поймать это в коде приложения.

## Типичные причины такой ошибки GPU

1. **Нехватка памяти GPU (Metal)** — загрузка или инференс большой модели (например 70B), несколько моделей в кэше, или тяжёлый запрос при уже занятой памяти.
2. **Слишком большая нагрузка на GPU** — много одновременных запросов или частые переключения моделей (load/unload).
3. **Ошибка драйвера Metal / прошивки** — реже, но возможно.

У нас падения совпадают с замером модели **deepseek-r1-distill-llama:70b** и последующими запросами — то есть сценарий «загрузка/инференс 70B → ошибка Metal → abort» наиболее вероятен.

---

## Причина в Python или в MLX? Что говорят эксперты и мировые практики

### Коротко: не Python, а лимиты Metal/памяти под большие модели

- **Python** здесь только хост процесса: в нём крутится uvicorn и код MLX API Server. Падение инициирует **нативный код MLX (C++)**: в колбэке Metal вызывается `mlx::core::gpu::check_error`, драйвер сообщает об ошибке GPU → MLX бросает C++ исключение → `std::terminate` → `abort()`. В Python это не перехватить, процесс целиком завершается.
- **MLX** большие модели (70B, 104B) «вытягивает» только при достаточной **памяти и соблюдении лимитов Metal**. Иначе драйвер возвращает ошибку (например, OOM или превышение лимита буфера), MLX на это реагирует падением процесса. То есть причина не в том, что «MLX не умеет большие модели», а в том, что **железо/драйвер не даёт выделить нужный объём** под данную конфигурацию (объём RAM, лимит Metal, размер одного буфера).

### Эксперты из .cursor/rules (Дмитрий — ML Engineer)

В правилах явно заложено (02_dmitriy.md):

- **Metal OOM, падения MLX API** — в зоне ответственности ML: мониторинг, перезапуск (monitor_mlx_api_server, system_auto_recovery).
- **Один слот, очередь:** `MLX_MAX_CONCURRENT=1` по умолчанию из‑за **Metal OOM**; лишние запросы ждут в очереди.
- Таймауты и лимиты параллелизма задаются **с учётом Metal OOM** и ошибок при таймауте.

То есть эксперты трактуют падения как **Metal OOM / ограничения GPU-памяти**, а не как баг Python или «MLX не тянет большие модели вообще».

### Мировые практики (Metal, Apple Silicon, MLX)

- **Лимит Metal** на Apple Silicon — порядка **75% физической RAM** (`recommendedMaxWorkingSetSize`); плюс **лимит на один буфер** (на M4 Pro — порядка 27 GB). Если модель или один большой тензор запрашивают больше — выделение не удаётся, драйвер возвращает ошибку.
- **70B/104B** при полном или даже квантованном виде могут требовать десятки гигабайт; при длинном префилле или больших матрицах внимания один буфер может превысить лимит и вызвать падение даже при свободной общей памяти.
- Рекомендации в сообществе: **квантование (4-bit)**, **меньше `prefill_step_size`** (чтобы не аллоцировать гигантские буферы), **очистка кэша** (`mx.clear_cache()`), **один запрос/одна модель в кэше** для снижения пика памяти.

**Итог:** причина падений — **ограничения Metal/памяти при загрузке или инференсе больших моделей (70B, 104B)**; MLX корректно доносит ошибку драйвера до процесса, но через C++ exception → abort, поэтому «падает Python». Винить стоит не Python и не MLX «в принципе», а нехватку ресурса или превышение лимитов под данную машину и конфигурацию.

**Принятое решение:** из конфигурации MLX API Server убраны модели **command-r-plus:104b**, **deepseek-r1-distill-llama:70b**, **llama3.3:70b** (и категория **reasoning** → 70B). Позже из MLX убрана и **qwen2.5-coder:32b** (≈35 ГБ процесс) — в приоритетах и маппинге только лёгкие (phi3.5:3.8b, qwen2.5:3b, phi3:mini-4k, tinyllama). Все категории (reasoning, default, coding) перенаправляются на **fast**. Сервер тяжёлые модели не предлагает и не загружает.

**Стратегия «только лёгкие и жизнедеятельность»:** при **MLX_ONLY_LIGHT=true** (по умолчанию) все категории (default, coding, reasoning) перенаправляются на **fast** (лёгкая модель); тяжёлые задачи — в Ollama. См. [MLX_STRATEGY_LIGHT_AND_VITALITY.md](MLX_STRATEGY_LIGHT_AND_VITALITY.md).

---

## Что делать

### 1. Снизить нагрузку на GPU и память (уже частично есть)

- **MLX_MAX_CONCURRENT=1** — один запрос к MLX одновременно (в `start_mlx_api_server.sh` уже выставляется).
- **MLX_MAX_CACHED_MODELS** — не держать в кэше много больших моделей (по умолчанию 2); при нехватке памяти можно уменьшить до 1.
- Не загружать 70B и 104B одновременно с другими тяжёлыми моделями; при ограниченной памяти — отключить предзагрузку больших моделей или не держать их в списке.

### 2. Не вызывать тяжёлые модели под память

- На машинах с ограниченной GPU-памятью не использовать модели 70B/104B через MLX или запускать их отдельным процессом.
- В конфиге MLX (MODEL_PATHS / категории) можно временно убрать или не выставлять по умолчанию модели 70B/104B, чтобы их не загружали по первому запросу под нагрузкой.

### 3. Перезапуск MLX при падении

- Использовать скрипт с автоперезапуском (например `start_mlx_server.sh` с циклом или внешний монитор), чтобы после краша Python процесс MLX снова поднимался.
- Так мы не устраняем причину (ошибка Metal), но восстанавливаем доступ к MLX после падения.
- **После прогона куратора или длинных запросов** MLX может вылететь — это типично при нагрузке. Запускай MLX через **`scripts/start_mlx_server.sh`** (автоперезапуск), либо полагайся на Ollama: Victoria в Docker теперь сначала обращается к Ollama, затем к MLX, так что при вылете MLX ответы идут через Ollama (см. FINDINGS_2026-02-09 в docs/curator_reports/).
- **Автозапуск при логине:** один раз выполнить **`bash scripts/setup_mlx_autostart.sh`** — launchd будет держать MLX через `start_mlx_server.sh` (при падении — перезапуск до 10 раз). Команды: `launchctl start com.atra.mlx-api-server`, `launchctl stop com.atra.mlx-api-server`; статус: `launchctl list | grep mlx`. Логи: `logs/mlx_server_wrapper.log`, `~/Library/Logs/atra-mlx-api-server.log`.

### 4. Обновления

- Обновлять **mlx**, **mlx-lm** и **Python** (в т.ч. 3.14): в новых версиях иногда чинят обработку ошибок Metal и реже доводят до `abort`.
- Обновлять macOS и Xcode Command Line Tools (влияют на Metal/driver).

### 5. Логи и повторение

- При следующем падении смотреть свежий отчёт в `~/Library/Logs/DiagnosticReports/Python-*.ips` и проверять, что падающий поток снова в `mlx::core::gpu::check_error` и Metal completion — тогда диагноз тот же (ошибка GPU при работе MLX).
- Логи приложения: `~/Library/Logs/atra/mlx_api_server.log` — по времени последнего запроса перед крашем можно понять, какая модель или запрос спровоцировали падение.

## Принятое решение (внедрено в коде)

По результатам разбора краш-репортов и рекомендациям (в т.ч. ограничение кэша и отказ от предзагрузки тяжёлых моделей):

1. **MLX_MAX_CONCURRENT=1** — оставлен (в `scripts/start_mlx_api_server.sh` и в wrapper).
2. **MLX_MAX_CACHED_MODELS=1** — по умолчанию в `knowledge_os/app/mlx_api_server.py` и в скриптах запуска; в коде комментарий про снижение Metal OOM.
3. **Предзагрузка без 70B/104B** — в `mlx_api_server.py` список предзагрузки фильтруется: ключи `reasoning`, `command-r-plus:104b`, `deepseek-r1-distill-llama:70b`, `llama3.3:70b` никогда не попадают в предзагрузку (даже если указаны в `MLX_PRELOAD_MODELS`). По умолчанию `MLX_PRELOAD_MODELS=fast`.
4. **Автоперезапуск при падении** — скрипт **`scripts/start_mlx_server.sh`** запускает MLX через uvicorn с теми же env (кэш 1, preload=fast); при выходе процесса (в т.ч. краш) делает паузу и перезапуск (до `MLX_WRAPPER_MAX_RESTARTS`, по умолчанию 10). Для постоянной работы MLX **рекомендуется запускать именно его**, чтобы после краша сервис поднимался сам:  
   `nohup bash scripts/start_mlx_server.sh &`  
   Лог: `logs/mlx_server_wrapper.log`.
5. **Замер моделей без краша** — в **`scripts/measure_cold_start_all_models.py`** по умолчанию для MLX не замеряются тяжёлые модели (70B/104B, reasoning): `MEASURE_MLX_SKIP_HEAVY=1`. Так тест не вызывает загрузку 70B и не роняет Python. Чтобы замерить и их (на свой риск): `MEASURE_MLX_SKIP_HEAVY=0`.
6. **Обновления** — в документации оставлена рекомендация обновлять mlx, mlx-lm и Python; при повторных крашах — проверять новые версии.

Мировые практики учтены: один запрос одновременно, один модель в кэше, отказ от предзагрузки тяжёлых моделей, перезапуск при падении (resilience).

---

## Почему падает даже лёгкая модель (fast)

При **MLX_ONLY_LIGHT=true** в работе только **fast** → phi3.5-mini-4k (~3.8B), без 70B/104B. Тем не менее краши возможны:

1. **Общая нагрузка на Metal/память** — Ollama, Docker, браузер и др. тоже используют unified memory. Даже 3–4 GB под phi3.5-mini в момент загрузки или инференса могут столкнуться с нехваткой или фрагментацией, и драйвер Metal вернёт ошибку → MLX делает abort.
2. **Особенности драйвера Metal / MLX** — отдельные операции (буферы, очереди команд) при определённых условиях могут приводить к ошибке GPU даже на маленькой модели; в коде Python это не перехватить.
3. **Параллельные запросы или переключение моделей** — при одном слоте (MLX_MAX_CONCURRENT=1) и только fast такого быть не должно, но если запрос пришёл во время предзагрузки или перезапуска, возможны гонки.
4. **Проверка при краше:** в логах MLX (`~/Library/Logs/atra/mlx_api_server.log` или `logs/mlx_server_wrapper.log`) смотреть, какая модель и какой запрос были перед падением. Если везде только `fast` / phi3.5-mini — значит, причина в п.1–2.

**Скачок памяти перед крашем:** на графике нагрузки (например «Нагрузка на память») часто виден **резкий пик** перед падением Python — в этот момент MLX или другие процессы (Ollama, Docker) кратковременно поднимают использование RAM/GPU; при уже высоком базовом потреблении (например 90+ ГБ из 128) пик может привести к нехватке памяти для Metal или к свопу и нестабильности. Типичная картина: скачок → краш. Решение то же: не запускать MLX или отключить его в Victoria (§ «Как решить»).

**Итог:** да, падает именно лёгкая модель; причина не в размере модели как таковом, а в Metal/памяти или в драйвере при работе с GPU; скачок памяти перед крашем — типичный признак.

---

## Как решить

Варианты по возрастанию «жёсткости»:

### 1. Не запускать MLX (рекомендуется при постоянных крашах)

Victoria в Docker уже опрашивает **сначала Ollama, потом MLX**. Если MLX не запущен, запросы идут только в Ollama — крашей Python от MLX не будет.

- **Выключить автозапуск MLX:**  
  `launchctl unload ~/Library/LaunchAgents/com.atra.mlx-api-server.plist`  
  (при следующем логине MLX не поднимется)
- **Остановить сейчас:**  
  `launchctl stop com.atra.mlx-api-server`  
  или просто не запускать `start_mlx_server.sh`.

Итог: один источник моделей (Ollama), без падений MLX.

### 2. Отключить MLX в Victoria (только Ollama)

Чтобы Victoria вообще не опрашивала MLX (нет таймаутов и попыток подключения к :11435):

- В **knowledge_os/docker-compose.yml** для сервиса `victoria-agent` в `environment:` задать:  
  **`MLX_API_URL: "disabled"`**  
  (или `MLX_API_URL: ""`, или `MLX_API_URL: "none"`).  
  В коде сканера при таком значении MLX не опрашивается, список MLX-моделей пустой — Victoria использует только Ollama.

- Перезапустить контейнер:  
  `docker compose -f knowledge_os/docker-compose.yml up -d victoria-agent --force-recreate`

### 3. Снизить конкуренцию за память/GPU

- Закрыть тяжёлые приложения (браузер с десятками вкладок, другие модели).
- Не держать одновременно под нагрузкой Ollama и MLX: например, гонять куратора/длинные задачи в часы, когда MLX не используется, или наоборот.
- Обновить macOS, Xcode Command Line Tools, пакеты `mlx` и `mlx-lm` — иногда в новых версиях меньше падений Metal.

### 4. Оставить MLX, но только через wrapper

Если MLX всё же нужен (например, для лёгкой классификации):

- Запускать **только** через `scripts/start_mlx_server.sh` (автоперезапуск при падении).
- Не запускать «голый» `start_mlx_api_server.sh` в фоне без wrapper.
- Проверить, что в plist launchd указан именно `start_mlx_server.sh`:  
  `bash scripts/setup_mlx_autostart.sh`  
  Тогда после краша процесс поднимется снова; Victoria при недоступности MLX продолжит через Ollama.

**Итог:** самый простой и надёжный вариант — **1 (не запускать MLX)**. При необходимости «только Ollama» из Victoria — **2**. Варианты 3–4 уменьшают частоту крашей или их последствия, но не убирают причину (Metal/драйвер).

---

## Повторные краши (2026-02-10)

Зафиксированы новые падения Python: отчёты в `~/Library/Logs/DiagnosticReports/Python-2026-02-10-*.ips` (00:01, 00:13, 01:30, 02:27 и др.). Причина та же: **mlx::core::gpu::check_error** в колбэке Metal (ошибка GPU). MLX после краша поднимается за счёт wrapper (`start_mlx_server.sh`) или вручную; Victoria при этом отвечает через Ollama (сначала опрашивает Ollama, потом MLX). В **start_mlx_server.sh** явно выставлен **MLX_ONLY_LIGHT=true**, чтобы не грузить тяжёлые модели. Если краши продолжаются даже с fast — возможны драйвер Metal или нагрузка; вариант: временно не запускать MLX и полагаться только на Ollama.

**Снова вылетел (позже 2026-02-10):** та же картина — Metal/GPU; быстрый перезапуск: см. «Что делать» ниже; при постоянных крашах — не запускать MLX или отключить в Victoria (`MLX_API_URL: disabled`).

---

## Что отслеживать в Мониторинге системы (Память)

- **Python (35+ ГБ)** — процесс MLX API Server. В MLX **только лёгкие модели** (70B/104B и 32B убраны из приоритетов и маппинга). Если видишь ~35 ГБ — возможно накопление/KV-cache или старая сессия; после правок 32B в MLX не загружается.
- **ollama (25+5+2 ГБ и т.д.)** — это **Ollama**, не MLX; может держать 32B (qwq:32b, qwen2.5-coder:32b) — мы их из Ollama не убирали.
- **Используемая память 108–111 ГБ из 128 ГБ** — при скачке возможен своп и нестабильность; при повторных крашах MLX — смотреть график и при необходимости отключать MLX (оставить только Ollama).

---

## Кратко

**Почему часто вылетает:** при работе MLX с Metal происходит ошибка GPU (чаще всего нехватка памяти или перегрузка при больших моделях). MLX в колбэке Metal бросает C++ исключение → срабатывает `std::terminate` → `abort()` → падение процесса Python.  
**Что делать:** ограничивать одновременные запросы и число кэшированных моделей, не нагружать GPU большими моделями (70B/104B) при нехватке памяти, использовать автоперезапуск MLX (`scripts/start_mlx_server.sh`) и при повторениях проверять краш-репорты и логи.
