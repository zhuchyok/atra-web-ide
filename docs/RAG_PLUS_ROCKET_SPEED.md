# RAG+ с ракетной скоростью: архитектура агента

**Дата:** 2026-01-29

## Цель

Выстроить систему агента с RAG так, чтобы **латентность ответа была минимальной** при сохранении качества: релевантный контекст из базы знаний + быстрая генерация.

---

## Текущее состояние

| Компонент | Сейчас | Узкое место |
|-----------|--------|-------------|
| **Victoria: контекст** | `_get_knowledge_context()` — только ILIKE по тексту | Нет семантики, нет кэша по запросу |
| **ai_core: RAG** | `get_embedding()` → pgvector, LIMIT 5 | 1 запрос на эмбеддинг + 1 запрос к БД последовательно |
| **Эмбеддинги** | Ollama nomic-embed-text, по одному запросу | Сеть на каждый запрос; есть EmbeddingOptimizer (кэш в БД + память) |
| **Кэш ответов** | SemanticAICache по сходству запроса | Уже даёт «ракетную» отдачу при попадании в кэш |

---

## Принципы «RAG+ на ракетной скорости»

### 1. Ни одного лишнего round-trip

- **Один эмбеддинг на запрос** — считать один раз, использовать и для RAG, и для семантического кэша.
- **Один запрос к БД** — один SQL с векторным поиском (pgvector), без второго запроса для «реранкинга» в горячем пути.
- **Параллель** — эмбеддинг и выбор эксперта / план можно считать параллельно; RAG и подготовка промпта — параллельно с тем, что не зависит от контекста.

### 2. Кэш на всех уровнях

- **Эмбеддинги**: уже есть EmbeddingOptimizer (память + БД). Держать горячие запросы в памяти, остальное — в `embedding_cache`.
- **Контекст RAG**: кэш «query_hash → список (content, score)» с TTL 1–5 минут для одинаковых/похожих запросов.
- **Ответы**: SemanticAICache по сходству эмбеддингов — при высоком similarity возвращать ответ без вызова LLM.

### 3. Минимум данных в промпте

- **Мало чанков, высокий порог**: топ-3–5 по similarity, порог 0.6–0.65.
- **Короткие фрагменты**: обрезать каждый чанк до 150–200 символов для контекста, полный текст — только при необходимости.
- **Без реранкинга в горячем пути**: реранкинг только для «важных» или сложных запросов (например, по флагу или по категории).

### 4. Инфраструктура поиска

- **pgvector**: индекс HNSW или IVFFlat на `knowledge_nodes.embedding` (уже есть колонка).
- **Гибрид (опционально)**: если нужна максимальная релевантность — (keyword ILIKE + векторный поиск) с объединением по score; для скорости по умолчанию — только векторный поиск по одному индексу.

### 5. RAG+ = RAG + быстрый путь

- **Быстрый путь**: семантический кэш → попадание → ответ без LLM.
- **Средний путь**: RAG (векторный поиск по одному запросу) → один вызов LLM с коротким контекстом.
- **Медленный путь** (редко): реранкинг, несколько моделей, расширенный контекст — только для сложных/критичных задач.

---

## Схема потока (ракетный вариант)

```
Запрос пользователя
       │
       ▼
┌──────────────────┐
│ Semantic cache   │── hit (sim ≥ 0.92) ──► Ответ (без LLM)
│ (по эмбеддингу)  │
└────────┬─────────┘
         │ miss
         ▼
┌──────────────────┐     ┌─────────────────────┐
│ get_embedding(q)  │     │ select_expert(...) │  ← параллельно
│ (кэш → Ollama)    │     └─────────┬─────────┘
└────────┬─────────┘               │
         │                          │
         ▼                          │
┌──────────────────┐                │
│ RAG: 1 SQL       │                │
│ pgvector, LIMIT 3│                │
│ (кэш контекста   │                │
│  по query_hash)  │                │
└────────┬─────────┘                │
         │                          │
         └──────────┬───────────────┘
                    ▼
         Формирование промпта (эксперт + knowledge context + цель)
                    │
                    ▼
         Один вызов LLM (streaming)
                    │
                    ▼
         Ответ + сохранение в semantic cache
```

---

## Что внедрить по шагам

### Уровень 1 (быстро, без смены стека)

1. **Victoria: векторный RAG вместо только ILIKE**  
   В `_get_knowledge_context()`: один вызов `get_embedding(goal)` (или из общего слоя с кэшем), один запрос к pgvector по `embedding`, LIMIT 3–5, порог similarity ≥ 0.6. Текст контекста — короткие фрагменты (например до 200 символов).

2. **Параллель: контекст и эксперт**  
   В месте, где сейчас последовательно вызываются `select_expert_for_task` и `_get_knowledge_context`, запускать их параллельно (`asyncio.gather`), затем собирать промпт.

3. **Кэш контекста RAG**  
   В памяти или в Redis: ключ `rag_ctx:{query_hash}`, значение — список `(content, score)`, TTL 60–300 сек. Перед запросом к БД проверять кэш; при попадании не вызывать эмбеддинг и не ходить в БД.

### Уровень 2 (скорость + качество)

4. **Индекс pgvector**  
   Убедиться, что на `knowledge_nodes(embedding)` есть индекс (HNSW или IVFFlat). При росте таблицы — настроить параметры индекса под размер данных.

5. **Один эмбеддинг на запрос**  
   В одном месте для запроса пользователя: вычислить эмбеддинг один раз, передать его и в RAG, и в SemanticAICache (проверка кэша и сохранение ответа). Избегать повторных вызовов Ollama для того же текста.

6. **Реранкинг только по флагу**  
   Для «сложных» или «критичных» задач вызывать реранкинг (например EnhancedRAGEngine); в обычном потоке — только один векторный запрос.

### Уровень 3 (максимум скорости)

7. **Батч эмбеддингов**  
   Если за один запрос нужны эмбеддинги для нескольких целей (например, подзапросы) — собирать в батч и один вызов к Ollama/embedding API (если поддерживается).

8. **Предзагрузка типовых запросов**  
   Для частых интентов (например «статус», «список файлов») — предзаполнять кэш контекста или ответов при старте/фоне.

9. **Метрики**  
   Логировать время: эмбеддинг, RAG, LLM. Цель — p99 RAG+путь < 200–300 ms до первого токена LLM (без учёта самой генерации).

---

## Переменные окружения (рекомендуемые)

| Переменная | Значение | Описание |
|------------|----------|----------|
| `RAG_CONTEXT_LIMIT` | 3 | Число чанков в контексте (меньше = быстрее) |
| `RAG_SIMILARITY_THRESHOLD` | 0.6 | Минимальный similarity для чанка |
| `RAG_CACHE_TTL_SEC` | 120 | TTL кэша контекста RAG (0 = отключить) |
| `SEMANTIC_CACHE_THRESHOLD` | 0.92 | Порог для возврата ответа из кэша без LLM |
| `OLLAMA_EMBED_URL` | http://localhost:11434/api/embeddings | Сервис эмбеддингов |
| `OLLAMA_MODEL` | nomic-embed-text | Модель эмбеддингов |

---

## Реализовано (уровень 1)

- **Victoria** (`src/agents/bridge/victoria_server.py`):
  - **Векторный RAG**: `_get_embedding_for_rag()` → один запрос к Ollama embeddings; `_get_knowledge_context()` при наличии эмбеддинга выполняет pgvector-поиск (LIMIT из `RAG_CONTEXT_LIMIT`, порог `RAG_SIMILARITY_THRESHOLD`), иначе fallback на ILIKE.
  - **Параллель**: `select_expert_for_task(goal)` и `_get_knowledge_context(goal)` вызываются через `asyncio.gather`, затем собирается промпт.

- Переменные: `OLLAMA_EMBED_URL`, `OLLAMA_EMBED_MODEL`, `RAG_CONTEXT_LIMIT`, `RAG_SIMILARITY_THRESHOLD` (см. таблицу выше).

## Итог

- **RAG+** = один эмбеддинг, один векторный запрос, кэш эмбеддингов и контекста, семантический кэш ответов.
- **Ракетная скорость** = параллель (эксперт + RAG), минимум чанков, без реранкинга в горячем пути, индексы pgvector.
- Дальше: кэш контекста RAG по query_hash (уровень 1), индексы pgvector и один эмбеддинг на запрос в одном месте (уровни 2–3).
