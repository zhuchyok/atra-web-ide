# RAG+ с ракетной скоростью: архитектура агента

**Дата:** 2026-01-29

## Цель

Выстроить систему агента с RAG так, чтобы **латентность ответа была минимальной** при сохранении качества: релевантный контекст из базы знаний + быстрая генерация.

---

## Текущее состояние

| Компонент | Сейчас | Узкое место |
|-----------|--------|-------------|
| **Victoria: контекст** | `_get_knowledge_context()` — только ILIKE по тексту | Нет семантики, нет кэша по запросу |
| **ai_core: RAG** | `get_embedding()` → pgvector, LIMIT 5 | 1 запрос на эмбеддинг + 1 запрос к БД последовательно |
| **Эмбеддинги** | Ollama nomic-embed-text, по одному запросу | Сеть на каждый запрос; есть EmbeddingOptimizer (кэш в БД + память) |
| **Кэш ответов** | SemanticAICache по сходству запроса | Уже даёт «ракетную» отдачу при попадании в кэш |

---

## Принципы «RAG+ на ракетной скорости»

### 1. Ни одного лишнего round-trip

- **Один эмбеддинг на запрос** — считать один раз, использовать и для RAG, и для семантического кэша.
- **Один запрос к БД** — один SQL с векторным поиском (pgvector), без второго запроса для «реранкинга» в горячем пути.
- **Параллель** — эмбеддинг и выбор эксперта / план можно считать параллельно; RAG и подготовка промпта — параллельно с тем, что не зависит от контекста.

### 2. Кэш на всех уровнях

- **Эмбеддинги**: уже есть EmbeddingOptimizer (память + БД). Держать горячие запросы в памяти, остальное — в `embedding_cache`.
- **Контекст RAG**: кэш «query_hash → список (content, score)» с TTL 1–5 минут для одинаковых/похожих запросов.
- **Ответы**: SemanticAICache по сходству эмбеддингов — при высоком similarity возвращать ответ без вызова LLM.

### 3. Минимум данных в промпте

- **Мало чанков, высокий порог**: топ-3–5 по similarity, порог 0.6–0.65.
- **Короткие фрагменты**: обрезать каждый чанк до 150–200 символов для контекста, полный текст — только при необходимости.
- **Без реранкинга в горячем пути**: реранкинг только для «важных» или сложных запросов (например, по флагу или по категории).

### 4. Инфраструктура поиска

- **pgvector**: индекс HNSW или IVFFlat на `knowledge_nodes.embedding` (уже есть колонка).
- **Гибрид (опционально)**: если нужна максимальная релевантность — (keyword ILIKE + векторный поиск) с объединением по score; для скорости по умолчанию — только векторный поиск по одному индексу.

### 5. RAG+ = RAG + быстрый путь

- **Быстрый путь**: семантический кэш → попадание → ответ без LLM.
- **Средний путь**: RAG (векторный поиск по одному запросу) → один вызов LLM с коротким контекстом.
- **Медленный путь** (редко): реранкинг, несколько моделей, расширенный контекст — только для сложных/критичных задач.

---

## Схема потока (ракетный вариант)

```
Запрос пользователя
       │
       ▼
┌──────────────────┐
│ Semantic cache   │── hit (sim ≥ 0.92) ──► Ответ (без LLM)
│ (по эмбеддингу)  │
└────────┬─────────┘
         │ miss
         ▼
┌──────────────────┐     ┌─────────────────────┐
│ get_embedding(q)  │     │ select_expert(...) │  ← параллельно
│ (кэш → Ollama)    │     └─────────┬─────────┘
└────────┬─────────┘               │
         │                          │
         ▼                          │
┌──────────────────┐                │
│ RAG: 1 SQL       │                │
│ pgvector, LIMIT 3│                │
│ (кэш контекста   │                │
│  по query_hash)  │                │
└────────┬─────────┘                │
         │                          │
         └──────────┬───────────────┘
                    ▼
         Формирование промпта (эксперт + knowledge context + цель)
                    │
                    ▼
         Один вызов LLM (streaming)
                    │
                    ▼
         Ответ + сохранение в semantic cache
```

---

## Что внедрить по шагам

### Уровень 1 (быстро, без смены стека)

1. **Victoria: векторный RAG вместо только ILIKE**  
   В `_get_knowledge_context()`: один вызов `get_embedding(goal)` (или из общего слоя с кэшем), один запрос к pgvector по `embedding`, LIMIT 3–5, порог similarity ≥ 0.6. Текст контекста — короткие фрагменты (например до 200 символов).

2. **Параллель: контекст и эксперт**  
   В месте, где сейчас последовательно вызываются `select_expert_for_task` и `_get_knowledge_context`, запускать их параллельно (`asyncio.gather`), затем собирать промпт.

3. **Кэш контекста RAG**  
   В памяти или в Redis: ключ `rag_ctx:{query_hash}`, значение — список `(content, score)`, TTL 60–300 сек. Перед запросом к БД проверять кэш; при попадании не вызывать эмбеддинг и не ходить в БД.

### Уровень 2 (скорость + качество)

4. **Индекс pgvector**  
   Убедиться, что на `knowledge_nodes(embedding)` есть индекс (HNSW или IVFFlat). При росте таблицы — настроить параметры индекса под размер данных.

5. **Один эмбеддинг на запрос**  
   В одном месте для запроса пользователя: вычислить эмбеддинг один раз, передать его и в RAG, и в SemanticAICache (проверка кэша и сохранение ответа). Избегать повторных вызовов Ollama для того же текста.

6. **Реранкинг только по флагу**  
   Для «сложных» или «критичных» задач вызывать реранкинг (например EnhancedRAGEngine); в обычном потоке — только один векторный запрос.

### Уровень 3 (максимум скорости)

7. **Батч эмбеддингов**  
   Если за один запрос нужны эмбеддинги для нескольких целей (например, подзапросы) — собирать в батч и один вызов к Ollama/embedding API (если поддерживается).

8. **Предзагрузка типовых запросов**  
   Для частых интентов (например «статус», «список файлов») — предзаполнять кэш контекста или ответов при старте/фоне.

9. **Метрики**  
   Логировать время: эмбеддинг, RAG, LLM. Цель — p99 RAG+путь < 200–300 ms до первого токена LLM (без учёта самой генерации).

---

## Переменные окружения (рекомендуемые)

| Переменная | Значение | Описание |
|------------|----------|----------|
| `RAG_CONTEXT_LIMIT` | 3 | Число чанков в контексте (меньше = быстрее) |
| `RAG_SIMILARITY_THRESHOLD` | 0.6 | Минимальный similarity для чанка |
| `RAG_CACHE_TTL_SEC` | 120 | TTL кэша контекста RAG (0 = отключить) |
| `RAG_LATENCY_LOG` | false | Логировать каждый замер [RAG+_latency] (info) |
| `RAG_LATENCY_EMBED_MS_MAX` | 300 | Порог «тормозит» для embed_ms (WARNING + slow_count) |
| `RAG_LATENCY_PREPARE_MS_MAX` | 300 | Порог «тормозит» для prepare_ms |
| `RAG_LATENCY_LLM_PLAN_MS_MAX` | 2000 | Порог «тормозит» для llm_plan_ms |
| `RAG_RERANK_ENABLED` | false | Реранкинг: брать limit×2 кандидатов, score = similarity × бонус за длину, топ limit |
| `RAG_PRELOAD_TYPICAL_QUERIES` | true | При старте в фоне заполнять кэш RAG типовыми запросами (при RAG_CACHE_TTL_SEC>0) |
| `SEMANTIC_CACHE_THRESHOLD` | 0.92 | Порог для возврата ответа из кэша без LLM |
| `OLLAMA_EMBED_URL` | http://localhost:11434/api/embeddings | Сервис эмбеддингов |
| `OLLAMA_MODEL` | nomic-embed-text | Модель эмбеддингов |

---

## Реализовано (уровень 1 и частично 2)

- **Victoria** (`src/agents/bridge/victoria_server.py`):
  - **Векторный RAG**: `_get_embedding_for_rag()` → один запрос к Ollama embeddings; `_get_knowledge_context()` при наличии эмбеддинга выполняет pgvector-поиск (LIMIT из `RAG_CONTEXT_LIMIT`, порог `RAG_SIMILARITY_THRESHOLD`), иначе fallback на ILIKE.
  - **Параллель**: `select_expert_for_task(goal)` и `_get_knowledge_context(goal)` вызываются через `asyncio.gather`, затем собирается промпт.
  - **Сниппеты и топ-1**: `RAG_SNIPPET_CHARS` (500), `RAG_TOP1_FULL_MAX_CHARS` (2000) — для топ-1 по similarity в контекст передаётся полный фрагмент до лимита (MASTER_REFERENCE § узлы знаний).

- **Индекс pgvector**: в `knowledge_os/db/init.sql` — IVFFlat на `knowledge_nodes(embedding)`; миграция `add_hnsw_index_knowledge_nodes.sql` — HNSW для ускорения (применить при росте данных: `psql $DATABASE_URL -f knowledge_os/db/migrations/add_hnsw_index_knowledge_nodes.sql`).

- **Backend**: есть `unified_embedding_provider.py` — один эмбеддинг на запрос для RAG + semantic cache (в контуре чата/задач при использовании провайдера).

- **Кэш контекста RAG (уровень 1):** в `_get_knowledge_context()` перед эмбеддингом и БД проверяется in-memory кэш по ключу `md5(goal.strip().lower())`. При попадании возвращается сохранённый контекст без вызова Ollama и БД. TTL: `RAG_CACHE_TTL_SEC` (по умолчанию 120 с, 0 = отключить). Макс. размер кэша 500 записей, вытеснение по самой старой записи. Реализовано 2026-02-08 (CHANGES §0.4o).

- **Один эмбеддинг на запрос (уровень 2):** в точке входа (формирование промпта для эксперта) эмбеддинг вычисляется один раз (`_get_embedding_for_rag(goal)`), затем параллельно запускаются выбор эксперта и `_get_knowledge_context(goal, precomputed_embedding=...)`. В `_get_knowledge_context` добавлен опциональный параметр `precomputed_embedding`; при передаче векторный поиск выполняется без повторного вызова Ollama. Реализовано 2026-02-08 (CHANGES §0.4p).

- **Метрики латентности (уровень 3):** в `plan()` замеряются и при `RAG_LATENCY_LOG=true` или `VICTORIA_DEBUG=true` логируются: **embed_ms** (время эмбеддинга), **prepare_ms** (параллель эксперт + RAG), **llm_plan_ms** (вызов planner.ask). Префикс лога `[RAG+_latency]`. Цель — анализ p99, целевой порог &lt; 200–300 ms до первого токена (при стриминге — при необходимости инструментировать executor). Реализовано 2026-02-08 (CHANGES §0.4q).

- **Отслеживание и проверка «тормозит»:** последние значения **embed_ms**, **prepare_ms**, **llm_plan_ms** всегда обновляются в памяти; **GET /status** возвращает блок **rag_latency**: `last`, `slow_count`, `last_slow_at`, `thresholds_ms`. При превышении порогов (по умолчанию embed≤300, prepare≤300, llm_plan≤2000 ms) пишется **WARNING** `[RAG+_latency] SLOW ...` и увеличивается `slow_count`. Мониторинг может опрашивать `/status` и строить алерты по `slow_count` или по логам. Пороги задаются **RAG_LATENCY_EMBED_MS_MAX**, **RAG_LATENCY_PREPARE_MS_MAX**, **RAG_LATENCY_LLM_PLAN_MS_MAX**. **GET /metrics** отдаёт Prometheus-метрики (victoria_rag_embed_seconds, victoria_rag_prepare_seconds, victoria_rag_llm_plan_seconds, victoria_rag_slow_requests_total). Дашборд: **grafana/dashboards/victoria-rag-latency.json**. Victoria уже в Prometheus: job victoria-agent в **prometheus/prometheus.yml** и **infrastructure/monitoring/prometheus.yml** (metrics_path: /metrics). Реализовано 2026-02-08 (CHANGES §0.4r, §0.4s).

- Переменные: `OLLAMA_EMBED_URL`, `OLLAMA_EMBED_MODEL`, `RAG_CONTEXT_LIMIT`, `RAG_SIMILARITY_THRESHOLD`, `RAG_SNIPPET_CHARS`, `RAG_TOP1_FULL_MAX_CHARS`, `RAG_CACHE_TTL_SEC`, `RAG_LATENCY_LOG` (см. таблицу и MASTER_REFERENCE).

---

## Что осталось (чтобы выйти на «ракетную скорость» полностью)

| Уровень | Пункт | Что сделать | Приоритет |
|---------|--------|-------------|-----------|
| **1** | ~~Кэш контекста RAG~~ | **Сделано (2026-02-08):** в Victoria `_get_knowledge_context()` — in-memory кэш по `query_hash` (md5 goal.strip().lower()), TTL из `RAG_CACHE_TTL_SEC` (120 с, 0 = выкл), макс. 500 записей. При попадании не вызываем эмбеддинг и не ходим в БД. См. victoria_server.py, CHANGES §0.4o. | — |
| **2** | HNSW индекс | Миграция в `apply_migrations.py`. Проверка: `knowledge_os/scripts/verify_hnsw_index.py` (выход 0 — индекс есть). При отсутствии — применить миграции. | Средний |
| **2** | ~~Один эмбеддинг на запрос (Victoria)~~ | **Сделано (2026-02-08):** в точке входа (формирование промпта) эмбеддинг вычисляется один раз (`_get_embedding_for_rag(goal)`), передаётся в `_get_knowledge_context(goal, precomputed_embedding=...)`. Параллель: эксперт + RAG с этим эмбеддингом. Параметр `precomputed_embedding` в _get_knowledge_context опционален. См. victoria_server.py, CHANGES §0.4p. | — |
| **2** | ~~Реранкинг по флагу~~ | **Сделано:** при **RAG_RERANK_ENABLED=true** Victoria берёт limit×2 кандидатов по вектору, реранжирует (similarity × бонус за длину 100–1000 символов), возвращает топ limit. По умолчанию выключено. | — |
| **3** | ~~Батч эмбеддингов~~ | **Сделано:** Victoria _get_embeddings_batch(texts) — один POST с input: [t1, t2, ...] при поддержке API; иначе fallback на одиночные вызовы. Используется в предзагрузке типовых запросов. | — |
| **3** | ~~Предзагрузка типовых запросов~~ | **Сделано:** при старте Victoria в фоне вызывается _preload_rag_cache() — заполнение кэша RAG для запросов «статус», «список файлов», «покажи файлы в текущей директории», «что ты умеешь». Включено при RAG_CACHE_TTL_SEC>0 и RAG_PRELOAD_TYPICAL_QUERIES=true (по умолчанию). | — |
| **3** | ~~Метрики латентности~~ | **Сделано (2026-02-08):** в Victoria в `plan()` логируются embed_ms, prepare_ms (эксперт+RAG), llm_plan_ms. Включение: `RAG_LATENCY_LOG=true` или `VICTORIA_DEBUG=true`. Цель — p99 &lt; 200–300 ms (анализ по логам). См. victoria_server.py, CHANGES §0.4q. | — |

## Итог

- **RAG+** = один эмбеддинг, один векторный запрос, кэш эмбеддингов и контекста, семантический кэш ответов.
- **Ракетная скорость** = параллель (эксперт + RAG), минимум чанков, без реранкинга в горячем пути, индексы pgvector.
- **Уже есть:** векторный RAG в Victoria, параллель эксперт+RAG, сниппеты и топ-1 полный, IVFFlat, unified_embedding_provider в backend, **кэш контекста RAG** (RAG_CACHE_TTL_SEC), **один эмбеддинг на запрос** (precomputed_embedding), **метрики латентности** (RAG_LATENCY_LOG: embed_ms, prepare_ms, llm_plan_ms). HNSW применяется через apply_migrations. **Осталось:** опционально батч/предзагрузка; для p99 до первого токена — при необходимости инструментировать стриминг в executor.
