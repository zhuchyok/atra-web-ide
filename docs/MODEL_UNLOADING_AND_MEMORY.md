# Выгрузка неиспользуемых моделей и память

**Назначение:** как у нас организована выгрузка моделей (MLX и Ollama), висят ли они в памяти бесконечно.

---

## MLX API Server

**Организовано:** модели **не висят бесконечно**. Есть лимит кэша и выгрузка по LRU.

| Что | Значение / поведение |
|-----|----------------------|
| **Максимум моделей в кэше** | **MLX_MAX_CACHED_MODELS** (по умолчанию **1**). Одновременно в памяти держится не больше этого числа моделей. |
| **Выгрузка по LRU** | Перед загрузкой новой модели вызывается **evict_lru_to_limit(keep_max)** — самая давно неиспользуемая модель выгружается из `_models_cache`. Не трогаются: активные запросы к модели и модели, использованные в последние 30 с. |
| **Периодическая очистка** | Фоновая задача **periodic_cache_cleanup()** каждые **MLX_CACHE_CLEANUP_INTERVAL_SEC** (по умолчанию **600** с = 10 мин) вызывает **evict_lru_to_limit(_max_cached_models)**. Лишние модели выгружаются по LRU. Отключить: `MLX_CACHE_CLEANUP_INTERVAL_SEC=0`. |
| **Критическая нехватка памяти** | При использовании памяти выше порога (**MLX_MEMORY_CRITICAL_PERCENT**, по умолчанию 95%) вызывается **cleanup_unused_models()** — выгрузка неиспользуемых по LRU, с защитой активных и недавно использованных (30 с). |
| **При завершении процесса** | По SIGINT/SIGTERM выполняется **\_models_cache.clear()**. |

**Итог по MLX:** в кэше по умолчанию одна модель; при запросе другой — предыдущая выгружается по LRU; раз в 10 мин фоново проверяется лимит; при нехватке памяти — экстренная очистка. Модели не остаются в памяти «навсегда».

---

## Ollama

**Организовано:** выгрузкой управляет **Ollama** по параметру **keep_alive**, который мы передаём в каждом запросе. Если не передавать, Ollama может использовать свой серверный дефолт (в т.ч. OLLAMA_KEEP_ALIVE=-1 при запуске) — тогда модели не выгружаются.

| Что | Поведение |
|-----|-----------|
| **Дефолт Ollama** | После последнего использования модель остаётся в памяти примерно **5 минут** (зависит от версии/конфига), затем Ollama выгружает её сам. |
| **Параметр keep_alive** | В запросах к `/api/generate` или `/api/chat` можно передать **keep_alive**: `0` — выгрузить сразу после ответа; положительное число — держать в памяти столько секунд; `-1` — не выгружать. |
| **В нашем коде** | В вызовах Ollama из **executor** и **LocalAIRouter** параметр **keep_alive** теперь рассчитывается адаптивно (Singularity 10.0: Smart Keep-Alive). Если env **VICTORIA_OLLAMA_KEEP_ALIVE** или **OLLAMA_KEEP_ALIVE** не заданы, используется следующая логика на основе веса модели: |
| **Smart Keep-Alive** | **Light (< 5GB):** 3600с (1ч) — дешево держать, быстро отвечать. <br> **Medium (5-15GB):** 600с (10м). <br> **Heavy (15-30GB):** 300с (5м). <br> **Monster (> 30GB):** 60с (1м) — агрессивная выгрузка для освобождения VRAM. |
| **Эмбеддинги** | В запросах к `/api/embeddings` (Victoria RAG) передаётся **keep_alive=0** — модель nomic-embed-text выгружается сразу после ответа, не висит в памяти. |
| **OLLAMA_KEEP_ALIVE на сервере** | Если **Ollama** запущен с переменной **OLLAMA_KEEP_ALIVE=-1** (например в launchd/systemd), он держит модели бесконечно. Наш код **переопределяет** это, передавая keep_alive в теле каждого запроса (300 по умолчанию или значение из .env). Проверить: как запущен процесс `ollama serve` (env в launchd, терминале). |

**Итог по Ollama:** мы **всегда** передаём keep_alive в запросах: по умолчанию 300 с (5 мин), при необходимости задать в .env `OLLAMA_KEEP_ALIVE=0` (выгрузить сразу) или `OLLAMA_KEEP_ALIVE=-1` (держать постоянно).

---

## Сводка

| Источник | Держат в памяти? | Как выгружаются |
|----------|------------------|------------------|
| **MLX** | Не более **MLX_MAX_CACHED_MODELS** (по умолчанию 1) | LRU при загрузке новой модели, раз в 10 мин фоновая очистка, при нехватке памяти — экстренная очистка. |
| **Ollama** | До ~5 мин после последнего использования (дефолт) | Автовыгрузка по таймауту Ollama; при необходимости можно передавать keep_alive=0 в запросах. |

При желании **жёстче освобождать память Ollama**: задать в .env **OLLAMA_KEEP_ALIVE=0** (или **VICTORIA_OLLAMA_KEEP_ALIVE=0**) — executor и LocalAIRouter подставят это в запросы (плюс: меньше пиков памяти; минус: холодный старт на следующий запрос).

---

## Рекомендации экспертов (команда Atra Core)

- **Дмитрий (ML):** явно задавать **keep_alive** по сценарию: для интерактивного чата с повторными запросами — 300–600 с или `5m`/`10m`, чтобы не платить холодным стартом; для пакетных/редких вызовов — 0 или короткий интервал. Сверяться с VERIFICATION_CHECKLIST §3 при скачках памяти и Metal OOM.
- **Елена (SRE):** при скачках памяти и OOM на Mac Studio — уменьшить keep_alive (например 60 или 0) или отключить MLX (MLX_API_URL=disabled), мониторить метрики и логи; при стабильной работе — не менять дефолт без необходимости.
- **Игорь (Backend):** единый источник значения — env (**VICTORIA_OLLAMA_KEEP_ALIVE** / **OLLAMA_KEEP_ALIVE**), не хардкод в коде; передача в теле запроса к Ollama уже реализована в executor и local_router.

**Политика по умолчанию:** переменная **не задана** — в запрос передаётся **keep_alive=300** (5 мин), чтобы модели гарантированно выгружались. При необходимости задать в .env: `OLLAMA_KEEP_ALIVE=0` (сразу после ответа) или `OLLAMA_KEEP_ALIVE=-1` (не выгружать). См. .env.example.

**При нескольких подряд задачах** (типичный сценарий чата/воркера) выгружать сразу (**keep_alive=0**) не нужно — иначе каждая следующая задача будет с холодным стартом. Оставлять дефолт (не задавать переменную) или явно задать 300/5m.

---

## Мировые практики и гиганты

Как делают крупные провайдеры и открытые стеки:

### Облачные API (OpenAI, Anthropic, Google и т.д.)

- **Выгрузки «на сторону клиента» нет** — инференс идёт в их кластерах; клиент вызывает API, модель уже загружена и масштабируется по их инфраструктуре. Управление памятью и выгрузкой — внутренняя тема провайдера (пулы реплик, автоскейлинг, очереди).
- **Практика:** разделение prefill (вычислительно ограничен) и decode (ограничен пропускной способностью памяти), батчинг запросов, лимиты по контексту и длине ответа.

### vLLM и PagedAttention (открытый стек, инференс на своих GPU)

- **PagedAttention** — управление памятью KV-кэша по аналогии с виртуальной памятью ОС: кэш разбит на блоки фиксированного размера, выделение неконтигуозное → меньше фрагментации, выше утилизация памяти и batch size.
- **Параметры:** `gpu_memory_utilization` (доля GPU под инференс, обычно 0.9–0.95), `max_model_len` (ограничение контекста — главный рычаг экономии памяти), `max_num_seqs` (число одновременных последовательностей). Сначала загружаются веса, затем резервируется место под KV-кэш и оценивается допустимая конкуренция.
- **Итог:** модель чаще всего одна (или несколько при мульти-GPU); «выгрузка» — не по таймеру, а за счёт жёсткого лимита памяти и планирования запросов; лишние запросы ждут в очереди или получают отказ.

### NVIDIA Triton Inference Server

- **Явные load/unload API:** `POST v2/repository/models/${MODEL_NAME}/load` и `POST v2/repository/models/${MODEL_NAME}/unload` — модель можно явно выгрузить из памяти. Есть индекс репозитория и состояние моделей.
- **Практика:** в продакшене часто держат ограниченный набор моделей загруженными; выгрузка — по политике (реже по запросу) или при деплое новой версии.

### Ollama (локальный, как у нас)

- **keep_alive** в запросах (`/api/generate`, `/api/chat`) и **OLLAMA_KEEP_ALIVE** в окружении — стандартный способ задать, сколько держать модель в памяти после последнего запроса: `0` — выгрузить сразу, положительное число — секунды, `-1` — не выгружать.
- **Дефолт:** обычно ~5 минут неактивности, затем автовыгрузка. В продакшене часто явно задают `keep_alive` (например 24h или -1 для стабильной задержки, или 0/короткое значение для экономии памяти).
- **Проблемы:** в разных версиях были баги с применением keep_alive; при нескольких процессах/клиентах одна и та же модель может выгружаться и снова загружаться («model swapping») — для стабильности рекомендуют явно задавать keep_alive и проверять, что настройка реально действует.

### Вывод для нашего стека

- **MLX:** наш лимит кэша (1 модель) и LRU ближе к подходу «жёсткий лимит памяти + выгрузка лишнего», как в vLLM/Triton, но без PagedAttention для KV-кэша.
- **Ollama:** имеет смысл явно задавать **keep_alive** в зависимости от сценария: для экономии памяти — `0` или короткий интервал; для стабильной латентности при повторных запросах — большее значение или `OLLAMA_KEEP_ALIVE` в окружении.
- **Гиганты** не дают клиенту «выгрузить модель» — они управляют ресурсами на своей стороне; у нас при локальном Ollama/MLX управление выгрузкой и лимитами — наша ответственность, по тем же идеям: лимит кэша, явный keep_alive, при нехватке памяти — агрессивная выгрузка.

---

**Связь:** MLX_PYTHON_CRASH_CAUSE.md (скачки памяти, отключение MLX), MAC_STUDIO_LOAD_AND_VICTORIA.md (лимиты памяти).
