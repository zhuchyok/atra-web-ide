# Стратегия MLX: только лёгкие модели и роль «поддержание жизнедеятельности»

**Назначение:** зафиксировать, как правильно использовать MLX, чтобы он перестал постоянно вылетать: ограничить лёгкими моделями и отвести ему роль «жизнедеятельность», а не решение тяжёлых задач. Учтены правила команды (.cursor/rules), узлы знания (MASTER_REFERENCE, MLX_PYTHON_CRASH_CAUSE, MAC_STUDIO) и мировые практики.

**Связанные документы:** [MLX_PYTHON_CRASH_CAUSE.md](MLX_PYTHON_CRASH_CAUSE.md), [MAC_STUDIO_LOAD_AND_VICTORIA.md](MAC_STUDIO_LOAD_AND_VICTORIA.md), [VICTORIA_RESTARTS_CAUSE.md](VICTORIA_RESTARTS_CAUSE.md), правила: Дмитрий (02), Елена (07), Игорь (03).

---

## 1. Проблема

MLX API Server постоянно вылетает (Metal OOM, SIGABRT в `mlx::core::gpu::check_error`). Причины по документации и экспертам:

- **Дмитрий (ML Engineer):** Metal OOM, падения MLX API — в зоне ML; один слот `MLX_MAX_CONCURRENT=1`, таймауты с учётом Metal OOM; большие модели (70B/104B) уже убраны, но **32B тоже дают пики памяти** и при длинных запросах или смене моделей процесс падает.
- **Мировые практики (Metal, Apple Silicon):** лимит на один буфер (~27 GB на M4 Pro), 75% RAM под GPU; квантование, один запрос/одна модель в кэше, не аллоцировать гигантские буферы.
- **Елена (SRE):** мониторинг MLX, перезапуск при падении (start_mlx_server.sh) — это смягчает последствия, но не устраняет причину.

**Итог:** пока MLX используется для «любых» задач (включая default/coding на 32B), пики памяти и переключения моделей приводят к крашам. Нужна явная стратегия: что MLX делает, а что — нет.

---

## 2. Предложения команды и мировые практики

### 2.1 Только лёгкие модели в MLX

- **Дмитрий:** «На машинах с ограниченной GPU-памятью не использовать модели 70B/104B через MLX; в конфиге убрать или не выставлять по умолчанию тяжёлые модели.» Расширяем: **и 32B в MLX дают риск** — загрузка ~60 с, инференс 40 с/1k токенов, пик памяти. Предложение: в MLX API Server по умолчанию **все категории (default, coding, reasoning) перенаправлять на лёгкую модель (fast: phi3.5-mini-4k / phi3.5:3.8b)**. Тяжёлые задачи (код, рассуждение, длинный контекст) пусть идут в **Ollama** (11434), где память и стабильность под контролем.
- **Мировые практики:** разделение «light path» и «heavy path»: быстрые ответы и проверки — лёгкая модель; сложный инференс — отдельный сервис с большей памятью и таймаутами (у нас это Ollama).

### 2.2 Роль MLX: «поддержание жизнедеятельности», а не решение задач

Использовать MLX **не для решения пользовательских задач** (анализ, код, рассуждение), а для:

- **Быстрых ответов «жизнедеятельности»:** приветствия, «что умеешь», короткие факты, health/readiness проверки с лёгким запросом.
- **Лёгкой классификации и маршрутизации:** категория запроса (fast vs coding vs reasoning) — короткий промпт, мало токенов.
- **Опционально:** эмбеддинги (если в MLX появится лёгкая модель для эмбеддингов и стабильность будет подтверждена).

**Решение пользовательских задач** (написать код, проанализировать проект, длинный ответ) — **только через Ollama** (Victoria в Docker уже сначала обращается к Ollama, затем к MLX; при падении MLX всё равно отвечает Ollama).

### 2.3 Игорь (Backend) и роутинг

- Роутинг уже настроен: Victoria Enhanced в Docker пробует **Ollama первым**, потом MLX (см. FINDINGS_2026-02-09, CHANGES §0.4bl). Так что при падении MLX пользователь не видит «не могу подключиться».
- Дополнительно: при вызове MLX передавать **только категорию fast** (или явный список «лёгких» категорий), чтобы MLX никогда не загружал 32B по умолчанию.

### 2.4 Елена (SRE): мониторинг и перезапуск

- Оставить **start_mlx_server.sh** (автоперезапуск при падении) и **system_auto_recovery** — MLX остаётся «best effort»: если жив — используем для лёгких ответов; упал — все запросы идут в Ollama.
- Не считать MLX критичным для «решения задач» — критичен Ollama; MLX — ускоритель для простых запросов и жизнедеятельности.

---

## 3. Принятая стратегия (как правильно использовать MLX)

| Принцип | Реализация |
|--------|-------------|
| **Только лёгкие модели в MLX** | В MLX API Server: default, coding, reasoning → **fast** (phi3.5-mini-4k / phi3.5:3.8b). Переменная **MLX_ONLY_LIGHT=true** (по умолчанию) включает этот режим; при false — старое поведение (default/coding → 32B) для тех, кто явно рискует. |
| **Роль: жизнедеятельность, не решение задач** | Использовать MLX для: приветствия, «что умеешь», короткие ответы, лёгкая классификация. Тяжёлые задачи (код, анализ, длинный контекст) — только Ollama. В коде Victoria/воркера при обращении к MLX передавать category=fast для простых запросов; default/coding не слать в MLX или слать как fast. |
| **Ollama — основной для задач** | Victoria в Docker: Ollama первым (уже сделано). Тяжёлые модели (32B, 70B) — только в Ollama. |
| **MLX падает — не блокируем** | Автоперезапуск (start_mlx_server.sh), fallback на Ollama; не увеличивать зависимость от MLX для критичных сценариев. |
| **Один модель в кэше, один запрос** | MLX_MAX_CACHED_MODELS=1, MLX_MAX_CONCURRENT=1 (уже есть); при MLX_ONLY_LIGHT предзагрузка только fast. |

---

## 4. Что сделать в коде и конфиге

1. **knowledge_os/app/mlx_api_server.py:** при **MLX_ONLY_LIGHT=true** (по умолчанию) маппинг категорий: default, coding, reasoning → **fast**. Тогда все запросы к MLX идут в лёгкую модель, 32B не загружаются.
2. **Предзагрузка:** при MLX_ONLY_LIGHT предзагружать только **fast** (MLX_PRELOAD_MODELS=fast уже по умолчанию).
3. **Документация:** в MLX_PYTHON_CRASH_CAUSE и в MASTER_REFERENCE добавить ссылку на этот документ; в .cursor/rules (Дмитрий, Елена) — упоминание стратегии «MLX только лёгкие и жизнедеятельность».
4. **HOW_TO_INDEX:** строка «MLX постоянно вылетает / как правильно использовать MLX» → этот документ.

---

## 5. Гипотезы практической пользы лёгких моделей (~10)

Лёгкие модели (phi3.5-mini-4k, qwen2.5:3b, tinyllama) — короткий контекст, мало токенов, стабильно на MLX. Ниже **гипотезы**, где они могут дать измеримую пользу без тяжёлого инференса.

| № | Гипотеза | Что делаем | Польза | Как проверить |
|---|----------|------------|--------|----------------|
| 1 | **Классификация намерения** | Один промпт: «Тип запроса: greeting / data_question / coding / reasoning / general» → один токен/слово | Маршрутизация до вызова тяжёлой модели (quick_data vs enhanced vs Ollama 32B) | A/B: время до первого байта, % правильной маршрутизации |
| 2 | **Извлечение сущностей** | Из одного предложения: имена экспертов, проект, дата → JSON или список | Роутинг в отдел, подстановка в контекст без полного понимания | Точность по тестовому набору 50–100 фраз |
| 3 | **Нужно ли уточнение?** | «Нужен ли уточняющий вопрос пользователю? Да/Нет. Если да — один вопрос.» | Уменьшение числа лишних вызовов 32B для needs_clarification | Доля «да» и качество сгенерированного вопроса (ручная выборочная проверка) |
| 4 | **Краткое резюме (1–2 предложения)** | Превью задачи/ответа для списка или лога | Меньше трафика и когнитивной нагрузки в UI/логах | Длина и читаемость на 20–50 примерах |
| 5 | **Похож ли на дубликат?** | «Похож ли запрос на один из [3 примера]? Да/Нет/Частично.» | Сведение дублей, предложение существующего ответа | Precision/recall на наборе пар запросов |
| 6 | **Нормализация в формат** | «Приведи к формату: один пункт на строку, макс. 5 пунктов» (короткий ввод) | Единообразие ответов для парсинга и отображения | Доля корректно отформатированных на выборке |
| 7 | **Оценка сложности 1–5** | «Сложность запроса от 1 (простой) до 5 (сложный): одна цифра.» | Приоритет в очереди воркера, выбор fast vs Ollama 32B | Корреляция с ручной разметкой и с временем решения |
| 8 | **Один факт из предложения** | «Извлеки: дату / число / имя / проект.» (один тип за раз) | Ускорение корпоративных ответов без полного Text-to-SQL | Точность на тестовых предложениях с известным ответом |
| 9 | **Тон/безопасность** | «Содержит ли запрос оскорбления или запрос на вредоносное действие? Нет/Да.» | Ранний отвод нежелательных запросов | False positive rate на нормальных запросах |
| 10 | **Генерация заголовка/темы** | По 1–3 предложениям — одна короткая тема (до 10 слов) | Заголовки задач, топики в логах, группировка | Читаемость и релевантность на выборке |

**Общие принципы проверки:** (1) промпт короткий, ответ — один токен/число/короткая фраза; (2) метрика — точность/доля успехов на фиксированной выборке; (3) при провале гипотезы — fallback на правило или Ollama без блокировки сценария.

**Порядок внедрения:** начать с 1 (классификация) и 7 (сложность) — уже влияют на маршрутизацию и воркер; затем 3 (уточнение) и 4 (резюме) для чата и задач.

---

## 5.1 Что желательно внедрить (приоритет и точки входа)

| Приоритет | Гипотеза | Что внедрить | Где в коде / потоке |
|-----------|----------|--------------|----------------------|
| **1** | **Классификация намерения (№1)** | Опциональный вызов лёгкой MLX, когда правило не уверено: один промпт «Тип: greeting / data_question / coding / reasoning / general» → подставить категорию. | **Victoria:** после `_categorize_task(goal)` в `victoria_enhanced.py`: если категория получилась `general` и длина запроса 5–25 слов — один запрос к MLX (category=fast), разобрать ответ, подменить category; иначе оставить правило. Fallback: при таймауте/ошибке MLX оставить `general`. |
| **2** | **Оценка сложности 1–5 (№7)** | Один промпт «Сложность запроса 1–5: одна цифра.» → при 4–5 направлять в Ollama 32B, при 1–2 — разрешать fast/MLX. | **Воркер:** в `smart_worker_autonomous` при формировании блока или выборе модели: для задачи без явного preferred_model опционально вызвать MLX (fast), получить цифру; 4–5 → предпочитать тяжёлую модель (Ollama 32B), 1–2 → fast. **Или** в Victoria: перед выбором модели по категории — вызов MLX «сложность 1–5», при 1–2 принудительно category=fast. |
| **3** | **Нужно ли уточнение (№3)** | «Нужен ли уточняющий вопрос? Да/Нет. Если да — один вопрос.» | **Victoria:** в `_understand_goal_with_clarification` или перед ним: если текущая логика не предложила уточнение — один короткий вызов MLX; при «Да» + текст вопроса — вернуть needs_clarification без вызова тяжёлой модели. |
| **4** | **Краткое резюме (№4)** | Превью 1–2 предложения для задачи/ответа. | **Задачи:** при записи в БД или отображении в списке — для поля preview/summary опционально вызывать MLX по телу задачи или по первому фрагменту ответа. **Чат:** превью последнего ответа в истории (опционально). |
| — | Остальные (2, 5, 6, 8, 9, 10) | По мере потребности: сущности, дубликаты, формат, один факт, тон, заголовок. | Точечно: корпоративные данные (8), модерация (9), заголовки задач (10), дедупликация (5), нормализация (6). |

**Условия внедрения:** (1) вызов MLX — только с таймаутом 5–10 с и fallback на правило/текущее поведение при ошибке; (2) не блокировать основной путь: если MLX недоступен — всё работает как сейчас; (3) метрики: логировать категорию/сложность и при A/B сравнивать время до ответа и качество маршрутизации.

**Минимальный первый шаг:** внедрить только **гипотезу 1 (классификация)** в Victoria для запросов с category=general и длиной 5–25 слов — один вызов к MLX fast, подстановка категории; замерить долю смены категории и влияние на латентность.

**Внедрено (гипотеза 1):** в `knowledge_os/app/victoria_enhanced.py` добавлены `_try_mlx_light_classify(goal)` и вызов после `_categorize_task`: при **VICTORIA_MLX_LIGHT_CLASSIFY=true**, category=general и 5–25 словах — один POST к MLX `/api/generate` (category=fast, max_tokens=10, таймаут 8 с); ответ парсится в одно слово (greeting/data_question/coding/reasoning/general), greeting→fast; при успехе категория подменяется, в логах `[MLX_LIGHT_CLASSIFY] general -> X goal_len=N duration_ms=M`. По умолчанию флаг **false** — поведение без изменений; включить для проверки «нужно ли реально».

**Замер:** скрипт `scripts/measure_mlx_light_classify.py` — шлёт тестовые запросы (5–25 слов, без явных ключевых слов) к Victoria и выводит время на запрос; в логах Victoria при включённом флаге появятся строки с duration_ms. Разбор логов: `docker logs victoria-agent 2>&1 | tail -500 | python3 scripts/measure_mlx_light_classify.py --parse-logs` — выведет число срабатываний и мин/макс/среднее duration_ms.

---

## 6. Резюме от команды

- **Дмитрий (ML):** «Используем MLX только для лёгких моделей и коротких запросов; тяжёлое — в Ollama. Таймауты и один слот оставляем.»
- **Елена (SRE):** «MLX мониторим и перезапускаем; не считаем критичным — при падении всё идёт в Ollama.»
- **Игорь (Backend):** «Роутинг Ollama-first уже есть; при вызове MLX можно явно слать category=fast для простых сценариев.»

**Мировые практики:** separation of concerns (light vs heavy path), fail-safe fallback, не нагружать один процесс и тяжёлыми, и лёгкими задачами — у нас light path = MLX, heavy path = Ollama.
