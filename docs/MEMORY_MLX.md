# Память MLX API Server (почему Python занимает десятки ГБ)

**Дата:** 2026-01-29

## Почему процесс Python (MLX API Server) занимает ~90 ГБ

MLX API Server загружает модели **внутри своего процесса** (через `mlx_lm.load()`). Все загруженные модели хранятся в словаре `_models_cache` в памяти процесса. В отличие от Ollama, здесь нет отдельного демона: один процесс Python держит и API, и веса моделей.

- **Одна большая модель** (например `deepseek-r1-distill-llama:70b`) — порядка **40–50 ГБ** RAM.
- **qwen2.5-coder:32b** — порядка **20–25 ГБ**.
- **command-r-plus:104b** — порядка **65 ГБ**.
- **phi3.5:3.8b** (fast) — порядка **2.5–3 ГБ**.

Если при старте предзагружались `default` и `fast` (32b + 3.8b ≈ 23 ГБ), а затем по запросам подгружались ещё reasoning (70b) или другие большие модели и **никогда не выгружались**, суммарно процесс легко доходит до **60–90 ГБ** и выше.

Раньше выгрузка по нехватке памяти срабатывала только при **>95% использования RAM**. На машине с большим объёмом памяти (например 128 ГБ) загрузка 2–3 больших моделей не достигала этого порога, поэтому они оставались в кэше постоянно.

## Что сделано для снижения потребления

1. **Лимит числа моделей в кэше**  
   Переменная окружения **`MLX_MAX_CACHED_MODELS`** (по умолчанию `2`). При запросе новой модели лишние (наименее недавно использованные) выгружаются до загрузки новой, так что в кэше одновременно хранится не больше указанного числа моделей.

2. **Периодическая выгрузка по LRU**  
   **`MLX_CACHE_CLEANUP_INTERVAL_SEC`** (по умолчанию `600` сек = 10 мин). Фоновая задача периодически оставляет в кэше не более `MLX_MAX_CACHED_MODELS` моделей, остальные выгружает по LRU. Это уменьшает «залипание» больших моделей в памяти при редких запросах.

3. **Меньшая предзагрузка по умолчанию**  
   **`MLX_PRELOAD_MODELS`** по умолчанию изменён с `default,fast` на **`fast`**. При старте загружается только лёгкая модель (порядка 2.5 ГБ), а не ещё и 32b (~20 ГБ). При необходимости `default` (32b) подгрузится по первому запросу.

## Рекомендации

- Чтобы **сразу снизить пиковое потребление**: оставить `MLX_MAX_CACHED_MODELS=2` (или 1) и по необходимости задать `MLX_PRELOAD_MODELS=fast` или пусто.
- Если нужна **минимальная задержка на первом запросе** к тяжёлой модели и RAM не критичен: можно увеличить `MLX_MAX_CACHED_MODELS` (например до 3) и при необходимости вернуть предзагрузку `default,fast`.
- **Полностью отключить предзагрузку**: `MLX_PRELOAD_MODELS=` (пусто).
- **Отключить периодическую очистку**: `MLX_CACHE_CLEANUP_INTERVAL_SEC=0`.

После изменений перезапустите MLX API Server (например `scripts/start_mlx_api_server.sh` или соответствующий сервис).
