#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ü§ñ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï LIGHTGBM –° –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò FEATURES
"""

import json
import os
import sys
import logging
from pathlib import Path
from datetime import datetime
import numpy as np
import pandas as pd

sys.path.append(str(Path(__file__).parent.parent))

from src.shared.utils.datetime_utils import get_utc_now

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

try:
    import lightgbm as lgb
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import (
        roc_auc_score, accuracy_score, precision_score, recall_score, f1_score,
        mean_absolute_error, mean_squared_error, r2_score
    )
    LIGHTGBM_AVAILABLE = True
except ImportError:
    LIGHTGBM_AVAILABLE = False
    logger.error("‚ùå LightGBM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install lightgbm scikit-learn")
    sys.exit(1)

print("=" * 80)
print("ü§ñ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï LIGHTGBM –° –ü–†–ê–í–ò–õ–¨–ù–´–ú–ò FEATURES")
print("=" * 80)

PATTERNS_FILE = Path(__file__).parent.parent / "ai_learning_data" / "trading_patterns.json"
MODEL_DIR = Path(__file__).parent.parent / "ai_learning_data" / "lightgbm_models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)

# ‚ö° LEARNING SESSION #4 (–î–º–∏—Ç—Ä–∏–π): –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ lag features
# Features –∫–æ—Ç–æ—Ä—ã–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
# Original 15 + —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ lag features = 28 total
FEATURE_NAMES = [
    # Base features (15)
    'rsi', 'macd', 'volume_ratio', 'volatility',
    'ema_distance', 'bb_position', 'atr_pct',
    'signal_is_long', 'risk_pct', 'leverage',
    'tp1_distance_pct', 'tp2_distance_pct',
    'hour_of_day', 'day_of_week', 'is_weekend',
    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ lag features (13) - –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    'rsi_lag_1', 'rsi_lag_2', 'rsi_lag_3', 'rsi_change',
    'macd_lag_1', 'macd_lag_2', 'macd_lag_3', 'macd_change',
    'volume_ratio_lag_1', 'volume_trend', 'volume_change_1',
    'volatility_lag_1', 'volatility_change',
    'price_change_1', 'price_change_3'
]

def load_patterns():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ —Ñ–∞–π–ª–∞"""
    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ {PATTERNS_FILE}")
    
    if not PATTERNS_FILE.exists():
        logger.error(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {PATTERNS_FILE}")
        sys.exit(1)
    
    with open(PATTERNS_FILE, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    wins = sum(1 for p in data if p.get('result') == 'WIN')
    losses = len(data) - wins
    logger.info(f"   WIN: {wins} ({wins/len(data)*100:.1f}%)")
    logger.info(f"   LOSS: {losses} ({losses/len(data)*100:.1f}%)")
    
    return data

def extract_features_from_pattern(pattern):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç features –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–∞
    
    –ï—Å–ª–∏ indicators –ø—É—Å—Ç—ã–µ - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞–∑—É–º–Ω—ã–µ –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    """
    try:
        # –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        entry_price = float(pattern.get('entry_price', 0))
        tp1 = float(pattern.get('tp1', 0))
        tp2 = float(pattern.get('tp2', 0))
        
        if entry_price == 0:
            return None
        
        # –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (—Å –¥–µ—Ñ–æ–ª—Ç–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏)
        indicators = pattern.get('indicators', {})
        
        # RSI: –µ—Å–ª–∏ –Ω–µ—Ç - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–∞–∑—É–º–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        # –î–ª—è WIN –æ–±—ã—á–Ω–æ RSI –±–ª–∏–∂–µ –∫ —ç–∫—Å—Ç—Ä–µ–º—É–º–∞–º (–ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–æ/–ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–æ)
        if 'rsi' in indicators:
            rsi = float(indicators['rsi'])
        else:
            # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            if pattern.get('signal_type') == 'LONG':
                rsi = 35.0  # oversold –¥–ª—è LONG
            else:
                rsi = 65.0  # overbought –¥–ª—è SHORT
        
        # MACD
        macd = float(indicators.get('macd', 0.001))
        
        # Volume ratio
        volume_ratio = float(indicators.get('volume_ratio', 1.5))
        
        # Volatility (ATR %)
        volatility = float(indicators.get('volatility', 0.02))
        
        # EMA distance
        ema_fast = float(indicators.get('ema_fast', entry_price * 1.01))
        ema_slow = float(indicators.get('ema_slow', entry_price * 0.99))
        ema_distance = abs(ema_fast - ema_slow) / entry_price
        
        # BB position
        bb_upper = float(indicators.get('bb_upper', entry_price * 1.02))
        bb_lower = float(indicators.get('bb_lower', entry_price * 0.98))
        if bb_upper > bb_lower:
            bb_position = (entry_price - bb_lower) / (bb_upper - bb_lower)
        else:
            bb_position = 0.5
        
        # ATR %
        atr = float(indicators.get('atr', entry_price * 0.015))
        atr_pct = atr / entry_price
        
        # Signal type
        signal_is_long = 1.0 if pattern.get('signal_type') == 'LONG' else 0.0
        
        # Risk params
        risk_pct = float(pattern.get('risk_pct', 2.0))
        leverage = float(pattern.get('leverage', 1.0))
        
        # TP distances
        tp1_distance_pct = abs(tp1 - entry_price) / entry_price * 100
        tp2_distance_pct = abs(tp2 - entry_price) / entry_price * 100
        
        # Time features
        try:
            timestamp = datetime.fromisoformat(pattern['timestamp'].replace('Z', '+00:00'))
            hour_of_day = timestamp.hour
            day_of_week = timestamp.weekday()
            is_weekend = 1.0 if day_of_week >= 5 else 0.0
        except:
            hour_of_day = 12
            day_of_week = 2
            is_weekend = 0.0
        
        # –°–æ–±–∏—Ä–∞–µ–º features
        features = {
            'rsi': rsi,
            'macd': macd,
            'volume_ratio': volume_ratio,
            'volatility': volatility,
            'ema_distance': ema_distance,
            'bb_position': bb_position,
            'atr_pct': atr_pct,
            'signal_is_long': signal_is_long,
            'risk_pct': risk_pct,
            'leverage': leverage,
            'tp1_distance_pct': tp1_distance_pct,
            'tp2_distance_pct': tp2_distance_pct,
            'hour_of_day': hour_of_day,
            'day_of_week': day_of_week,
            'is_weekend': is_weekend
        }
        
        # ‚ö° LEARNING SESSION #4 (–î–º–∏—Ç—Ä–∏–π): –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ lag features
        # Lag features (–±—É–¥—É—Ç –≤—ã—á–∏—Å–ª–µ–Ω—ã –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞)
        # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ fallback
        features['rsi_lag_1'] = rsi  # –ë—É–¥–µ—Ç –ø–µ—Ä–µ–∑–∞–ø–∏—Å–∞–Ω–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è
        features['rsi_change'] = 0.0
        features['macd_lag_1'] = macd
        features['macd_change'] = 0.0
        features['volume_ratio_lag_1'] = volume_ratio
        features['volume_trend'] = 0.0
        features['volatility_lag_1'] = volatility
        features['volatility_change'] = 0.0
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ lag features –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        # (–±—É–¥—É—Ç –≤—ã—á–∏—Å–ª–µ–Ω—ã –∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏)
        features['rsi_lag_2'] = rsi
        features['rsi_lag_3'] = rsi
        features['macd_lag_2'] = macd
        features['macd_lag_3'] = macd
        features['price_change_1'] = 0.0  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 1 –ø–µ—Ä–∏–æ–¥
        features['price_change_3'] = 0.0  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ —Ü–µ–Ω—ã –∑–∞ 3 –ø–µ—Ä–∏–æ–¥–∞
        features['volume_change_1'] = 0.0  # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ–±—ä—ë–º–∞
        
        return features
        
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è features: {e}")
        return None

def prepare_dataset(patterns):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    logger.info("\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è lag features
    try:
        def get_ts(p):
            ts = p.get('timestamp', '')
            if isinstance(ts, datetime):
                return ts.isoformat()
            return str(ts)
        patterns_sorted = sorted(patterns, key=get_ts)
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {e}")
        patterns_sorted = patterns
    
    X_list = []
    y_class = []  # 1 = WIN, 0 = LOSS
    y_reg = []    # profit_pct
    
    # ‚ö° LEARNING SESSION #4 (–î–º–∏—Ç—Ä–∏–π): –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ lag features
    # –•—Ä–∞–Ω–∏–º –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è lag features (lag_1, lag_2, lag_3)
    history_rsi = []
    history_macd = []
    history_volume = []
    history_volatility = []
    history_price = []
    
    for i, pattern in enumerate(patterns_sorted):
        features = extract_features_from_pattern(pattern)
        if features is None:
            continue
        
        # –ü–æ–ª—É—á–∞–µ–º —Ü–µ–Ω—É –≤—Ö–æ–¥–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è price_change
        entry_price = float(pattern.get('entry_price', pattern.get('entry', 0)))
        
        # –í—ã—á–∏—Å–ª—è–µ–º lag features –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        # Lag 1
        if len(history_rsi) >= 1:
            features['rsi_lag_1'] = history_rsi[-1]
            features['rsi_change'] = features['rsi'] - history_rsi[-1]
        else:
            features['rsi_lag_1'] = features['rsi']
            features['rsi_change'] = 0.0
        
        # Lag 2
        if len(history_rsi) >= 2:
            features['rsi_lag_2'] = history_rsi[-2]
        else:
            features['rsi_lag_2'] = features['rsi']
        
        # Lag 3
        if len(history_rsi) >= 3:
            features['rsi_lag_3'] = history_rsi[-3]
        else:
            features['rsi_lag_3'] = features['rsi']
        
        # MACD lags
        if len(history_macd) >= 1:
            features['macd_lag_1'] = history_macd[-1]
            features['macd_change'] = features['macd'] - history_macd[-1]
        else:
            features['macd_lag_1'] = features['macd']
            features['macd_change'] = 0.0
        
        if len(history_macd) >= 2:
            features['macd_lag_2'] = history_macd[-2]
        else:
            features['macd_lag_2'] = features['macd']
        
        if len(history_macd) >= 3:
            features['macd_lag_3'] = history_macd[-3]
        else:
            features['macd_lag_3'] = features['macd']
        
        # Volume lags
        if len(history_volume) >= 1:
            features['volume_ratio_lag_1'] = history_volume[-1]
            features['volume_trend'] = features['volume_ratio'] - history_volume[-1]
            features['volume_change_1'] = features['volume_ratio'] - history_volume[-1]
        else:
            features['volume_ratio_lag_1'] = features['volume_ratio']
            features['volume_trend'] = 0.0
            features['volume_change_1'] = 0.0
        
        # Volatility lags
        if len(history_volatility) >= 1:
            features['volatility_lag_1'] = history_volatility[-1]
            features['volatility_change'] = features['volatility'] - history_volatility[-1]
        else:
            features['volatility_lag_1'] = features['volatility']
            features['volatility_change'] = 0.0
        
        # Price changes
        if len(history_price) >= 1 and entry_price > 0 and history_price[-1] > 0:
            features['price_change_1'] = (entry_price - history_price[-1]) / history_price[-1]
        else:
            features['price_change_1'] = 0.0
        
        if len(history_price) >= 3 and entry_price > 0 and history_price[-3] > 0:
            features['price_change_3'] = (entry_price - history_price[-3]) / history_price[-3]
        else:
            features['price_change_3'] = 0.0
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏—é (—Ö—Ä–∞–Ω–∏–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3)
        history_rsi.append(features['rsi'])
        history_macd.append(features['macd'])
        history_volume.append(features['volume_ratio'])
        history_volatility.append(features['volatility'])
        history_price.append(entry_price)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(history_rsi) > 3:
            history_rsi.pop(0)
            history_macd.pop(0)
            history_volume.pop(0)
            history_volatility.pop(0)
            history_price.pop(0)
        
        X_list.append(features)
        
        # ‚ö° –≠–ö–°–ü–ï–†–¢–ù–ê–Ø –†–ê–ó–ú–ï–¢–ö–ê (–î–º–∏—Ç—Ä–∏–π): –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
        # Target: classification
        if 'bin' in pattern:
            # –ï—Å–ª–∏ –µ—Å—Ç—å Triple Barrier Labeling
            y_class.append(1 if pattern['bin'] > 0 else 0)
        else:
            # Fallback –Ω–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–¥–µ–ª–∫–∏
            result = pattern.get('result', 'LOSS')
            y_class.append(1 if result == 'WIN' else 0)
        
        # Target: regression
        profit_pct = pattern.get('profit_pct', 0.0)
        if profit_pct is None:
            profit_pct = 0.0
        y_reg.append(float(profit_pct))

    # –°–æ–∑–¥–∞–µ–º DataFrame
    X = pd.DataFrame(X_list)[FEATURE_NAMES]
    y_class = np.array(y_class)
    y_reg = np.array(y_reg)
    
    logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} samples —Å {len(FEATURE_NAMES)} features")
    return X, y_class, y_reg
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    X = pd.DataFrame(X_list)[FEATURE_NAMES]
    y_class = np.array(y_class)
    y_reg = np.array(y_reg)
    
    logger.info(f"‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} samples —Å {len(FEATURE_NAMES)} features")
    logger.info(f"   WIN: {y_class.sum()} ({y_class.mean()*100:.1f}%)")
    logger.info(f"   LOSS: {len(y_class) - y_class.sum()} ({(1-y_class.mean())*100:.1f}%)")
    
    return X, y_class, y_reg

def train_models(X, y_class, y_reg, use_purged_cv=True):
    """
    –û–±—É—á–∞–µ—Ç classifier –∏ regressor
    
    Args:
        X: Features DataFrame
        y_class: Classification targets
        y_reg: Regression targets
        use_purged_cv: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Purged K-Fold CV (–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage)
    """
    logger.info("\nü§ñ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
    
    # üìä PURGED K-FOLD CV (–î–º–∏—Ç—Ä–∏–π - –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è 30% –ø—Ä–æ–≥—Ä–∞–º–º—ã)
    # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç data leakage –≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö
    if use_purged_cv:
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å timestamps –∏–∑ –∏–Ω–¥–µ–∫—Å–∞ –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            # –î–ª—è retrain_lightgbm.py timestamps –º–æ–≥—É—Ç –±—ã—Ç—å –≤ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö
            from purged_k_fold import purged_train_test_split
            
            # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å timestamps (–µ—Å–ª–∏ –µ—Å—Ç—å –≤ –¥–∞–Ω–Ω—ã—Ö)
            timestamps = None
            if hasattr(X, 'index') and isinstance(X.index, pd.DatetimeIndex):
                timestamps = pd.Series(X.index)
            elif 'timestamp' in X.columns:
                # –ï—Å–ª–∏ timestamp –≤ features, –∏–∑–≤–ª–µ–∫–∞–µ–º
                timestamps = pd.to_datetime(X['timestamp'], errors='coerce')
            
            logger.info("üìä –ò—Å–ø–æ–ª—å–∑—É–µ–º Purged K-Fold CV –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è data leakage...")
            X_train, X_test, y_class_train, y_class_test = purged_train_test_split(
                X, y_class,
                test_size=0.2,
                purge_gap=1,  # –£–¥–∞–ª—è–µ–º 1 –ø–µ—Ä–∏–æ–¥ –º–µ–∂–¥—É train/test
                embargo_pct=0.01,  # 1% embargo
                timestamps=timestamps
            )
            
            # –î–ª—è regression –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ –∏–Ω–¥–µ–∫—Å—ã
            if isinstance(X_train, pd.DataFrame):
                train_idx = X_train.index
                test_idx = X_test.index
            else:
                # –ï—Å–ª–∏ –Ω–µ DataFrame, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã
                train_idx = np.arange(len(X_train))
                test_idx = np.arange(len(X_train), len(X_train) + len(X_test))
            
            y_reg_train = y_reg[train_idx]
            y_reg_test = y_reg[test_idx]
            
            logger.info(f"   ‚úÖ Purged CV: train={len(X_train)}, test={len(X_test)}")
            logger.info(f"   üìä Purged samples –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞—é—Ç data leakage")
            
        except ImportError:
            logger.warning("‚ö†Ô∏è purged_k_fold –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π split")
            use_purged_cv = False
    
    # Fallback: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π split
    if not use_purged_cv:
        X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(
            X, y_class, y_reg, test_size=0.2, random_state=42, stratify=y_class
        )
    
    logger.info(f"   Train: {len(X_train)} samples")
    logger.info(f"   Test: {len(X_test)} samples")
    
    # ==================== SAMPLE WEIGHTS ====================
    # –î–æ–±–∞–≤–ª–µ–Ω–æ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è 5% –ø—Ä–æ–≥—Ä–∞–º–º—ã (Ernest Chan)
    from sklearn.utils.class_weight import compute_sample_weight
    
    sample_weights_train = compute_sample_weight(
        class_weight='balanced',
        y=y_class_train
    )
    logger.info(f"   Sample weights computed for class imbalance")
    logger.info(f"   Min weight: {sample_weights_train.min():.3f}, Max weight: {sample_weights_train.max():.3f}")
    
    # ==================== CLASSIFIER ====================
    logger.info("\nüìä –û–±—É—á–µ–Ω–∏–µ Classifier...")
    
    # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –∫–ª–∞—Å—Å–æ–≤
    scale_pos_weight = (len(y_class_train) - y_class_train.sum()) / y_class_train.sum()
    logger.info(f"   Scale pos weight: {scale_pos_weight:.2f}")
    
    train_data_class = lgb.Dataset(X_train, label=y_class_train, weight=sample_weights_train)
    test_data_class = lgb.Dataset(X_test, label=y_class_test, reference=train_data_class)
    
    params_class = {
        'objective': 'binary',
        'metric': 'auc',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'scale_pos_weight': scale_pos_weight,  # –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞
        'min_child_samples': 20,
        'max_depth': 7,
    }
    
    classifier = lgb.train(
        params_class,
        train_data_class,
        num_boost_round=500,
        valid_sets=[test_data_class],
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]
    )
    
    # –ú–µ—Ç—Ä–∏–∫–∏ classifier
    y_pred_class = (classifier.predict(X_test) > 0.5).astype(int)
    y_pred_proba = classifier.predict(X_test)
    
    class_metrics = {
        'roc_auc': roc_auc_score(y_class_test, y_pred_proba),
        'accuracy': accuracy_score(y_class_test, y_pred_class),
        'precision': precision_score(y_class_test, y_pred_class, zero_division=0),
        'recall': recall_score(y_class_test, y_pred_class, zero_division=0),
        'f1_score': f1_score(y_class_test, y_pred_class, zero_division=0)
    }
    
    logger.info("\n‚úÖ Classifier –º–µ—Ç—Ä–∏–∫–∏:")
    for key, value in class_metrics.items():
        logger.info(f"   {key}: {value:.4f}")
    
    # ==================== REGRESSOR ====================
    logger.info("\nüìä –û–±—É—á–µ–Ω–∏–µ Regressor...")
    
    # –≠–ö–°–ü–ï–†–¢–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø (–î–º–∏—Ç—Ä–∏–π): –í–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–±—Å–æ–ª—é—Ç–Ω–æ–≥–æ –ø—Ä–æ—Ñ–∏—Ç–∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–∞
    # –°–¥–µ–ª–∫–∏ —Å –±–æ–ª—å—à–∏–º –¥–≤–∏–∂–µ–Ω–∏–µ–º —Ü–µ–Ω—ã –±–æ–ª–µ–µ –≤–∞–∂–Ω—ã –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    sample_weights_reg = np.abs(y_reg_train) + 1.0
    sample_weights_reg = sample_weights_reg / sample_weights_reg.mean()
    
    train_data_reg = lgb.Dataset(X_train, label=y_reg_train, weight=sample_weights_reg)
    test_data_reg = lgb.Dataset(X_test, label=y_reg_test, reference=train_data_reg)
    
    params_reg = {
        'objective': 'regression',
        'metric': 'rmse',
        'boosting_type': 'gbdt',
        'num_leaves': 31,
        'learning_rate': 0.05,
        'feature_fraction': 0.9,
        'bagging_fraction': 0.8,
        'bagging_freq': 5,
        'verbose': -1,
        'min_child_samples': 20,
        'max_depth': 7,
    }
    
    regressor = lgb.train(
        params_reg,
        train_data_reg,
        num_boost_round=500,
        valid_sets=[test_data_reg],
        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]
    )
    
    # –ú–µ—Ç—Ä–∏–∫–∏ regressor
    y_pred_reg = regressor.predict(X_test)
    
    reg_metrics = {
        'mae': mean_absolute_error(y_reg_test, y_pred_reg),
        'rmse': np.sqrt(mean_squared_error(y_reg_test, y_pred_reg)),
        'r2': r2_score(y_reg_test, y_pred_reg)
    }
    
    logger.info("\n‚úÖ Regressor –º–µ—Ç—Ä–∏–∫–∏:")
    for key, value in reg_metrics.items():
        logger.info(f"   {key}: {value:.4f}")
    
    # Feature importance
    logger.info("\nüìä Feature Importance (Top 10):")
    importance = classifier.feature_importance(importance_type='gain')
    feature_importance = sorted(zip(FEATURE_NAMES, importance), key=lambda x: x[1], reverse=True)
    for feature, imp in feature_importance[:10]:
        logger.info(f"   {feature}: {imp:.0f}")
    
    return classifier, regressor, class_metrics, reg_metrics

def save_models(classifier, regressor, class_metrics, reg_metrics):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ"""
    logger.info(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ {MODEL_DIR}...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
    classifier.save_model(str(MODEL_DIR / "classifier.txt"))
    regressor.save_model(str(MODEL_DIR / "regressor.txt"))
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    metadata = {
        'feature_names': FEATURE_NAMES,
        'training_metrics': {
            'classifier': class_metrics,
            'regressor': reg_metrics
        },
        'trained_at': get_utc_now().isoformat()
    }
    
    with open(MODEL_DIR / "metadata.json", 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)
    
    logger.info("‚úÖ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!")
    logger.info(f"   - {MODEL_DIR / 'classifier.txt'}")
    logger.info(f"   - {MODEL_DIR / 'regressor.txt'}")
    logger.info(f"   - {MODEL_DIR / 'metadata.json'}")

def main():
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    patterns = load_patterns()
    
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
    X, y_class, y_reg = prepare_dataset(patterns)
    
    if len(X) < 100:
        logger.error("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        sys.exit(1)
    
    # 3. –û–±—É—á–µ–Ω–∏–µ
    classifier, regressor, class_metrics, reg_metrics = train_models(X, y_class, y_reg)
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    save_models(classifier, regressor, class_metrics, reg_metrics)
    
    logger.info("=" * 80)
    logger.info("‚úÖ –ü–ï–†–ï–û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    logger.info("=" * 80)
    logger.info(f"\nüìä –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    logger.info(f"   ROC AUC: {class_metrics['roc_auc']:.4f}")
    logger.info(f"   Accuracy: {class_metrics['accuracy']:.4f}")
    logger.info(f"   Precision: {class_metrics['precision']:.4f}")
    logger.info(f"   Recall: {class_metrics['recall']:.4f}")
    logger.info(f"   F1 Score: {class_metrics['f1_score']:.4f}")
    logger.info(f"\n   Regressor MAE: {reg_metrics['mae']:.4f}")
    logger.info(f"   Regressor RMSE: {reg_metrics['rmse']:.4f}")
    logger.info(f"   Regressor R¬≤: {reg_metrics['r2']:.4f}")
    logger.info("\nüéØ –ú–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    logger.info("=" * 80)

if __name__ == "__main__":
    main()

