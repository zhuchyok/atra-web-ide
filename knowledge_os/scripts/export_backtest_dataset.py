#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–≠–∫—Å–ø–æ—Ä—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è backtest replay (baseline vs adaptive).

–°–æ–±–∏—Ä–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è `position_sizing_events`, –ø–æ–¥–±–∏—Ä–∞–µ—Ç –∫ –Ω–∏–º –±–ª–∏–∂–∞–π—à–∏–µ –∑–∞–ø–∏—Å–∏
–∏–∑ `signals_log` –∏ `trades`, —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç
—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ CSV –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è baseline/adaptive.
"""

from __future__ import annotations

import argparse
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Optional

import pandas as pd

import sys

ROOT = Path(__file__).resolve().parent.parent
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from src.database.db import Database  # noqa: E402

# =============================================================================
# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# =============================================================================


def _load_events(conn: sqlite3.Connection, hours: int) -> pd.DataFrame:
    query = """
        SELECT
            id AS event_id,
            created_at AS event_created_at,
            entry_time AS event_entry_time,
            symbol,
            direction,
            trade_mode AS event_trade_mode,
            baseline_amount_usd,
            ai_amount_usd,
            final_amount_usd,
            base_risk_pct,
            ai_risk_pct,
            leverage,
            regime,
            regime_confidence,
            correlation_multiplier,
            adaptive_multiplier,
            risk_adjustment_multiplier
        FROM position_sizing_events
        WHERE datetime(created_at) >= datetime('now', ?)
        ORDER BY symbol ASC, event_entry_time ASC
    """
    df = pd.read_sql_query(query, conn, params=[f"-{hours} hours"])
    if df.empty:
        return df
    df["event_entry_dt"] = pd.to_datetime(df["event_entry_time"], errors="coerce", utc=True).dt.tz_convert(None)
    df["event_created_dt"] = pd.to_datetime(df["event_created_at"], errors="coerce", utc=True).dt.tz_convert(None)
    df = df.dropna(subset=["event_entry_dt"])
    return df.sort_values(["symbol", "event_entry_dt"]).reset_index(drop=True)


def _load_signals(conn: sqlite3.Connection, hours: int, tolerance_minutes: int) -> pd.DataFrame:
    # –ë–µ—Ä—ë–º —á—É—Ç—å –±–æ–ª—å—à–µ –æ–∫–Ω–∞, —á—Ç–æ–±—ã –æ—Ö–≤–∞—Ç–∏—Ç—å —Å–∏–≥–Ω–∞–ª—ã, –ø–æ–ø–∞–¥–∞—é—â–∏–µ –≤ –¥–æ–ø—É—Å–∫
    extra_hours = hours + max(1, tolerance_minutes // 60 + 1)
    query = """
        SELECT
            id AS signal_id,
            symbol,
            entry_time AS signal_entry_time,
            entry AS signal_entry_price,
            stop AS signal_stop_price,
            tp1 AS signal_tp1_price,
            tp2 AS signal_tp2_price,
            result AS signal_result,
            net_profit AS signal_net_profit,
            entry_amount_usd AS signal_entry_amount_usd,
            risk_pct_used AS signal_risk_pct,
            leverage_used AS signal_leverage,
            trade_mode AS signal_trade_mode,
            created_at AS signal_created_at
        FROM signals_log
        WHERE datetime(entry_time) >= datetime('now', ?)
        ORDER BY symbol ASC, signal_entry_time ASC
    """
    df = pd.read_sql_query(query, conn, params=[f"-{extra_hours} hours"])
    if df.empty:
        return df
    df["signal_entry_dt"] = pd.to_datetime(df["signal_entry_time"], errors="coerce", utc=True).dt.tz_convert(None)
    df["signal_created_dt"] = pd.to_datetime(df["signal_created_at"], errors="coerce", utc=True).dt.tz_convert(None)
    df = df.dropna(subset=["signal_entry_dt"])
    return df.sort_values(["symbol", "signal_entry_dt"]).reset_index(drop=True)


def _load_trades(conn: sqlite3.Connection, hours: int, tolerance_minutes: int) -> pd.DataFrame:
    extra_hours = hours + max(1, tolerance_minutes // 60 + 1)
    query = """
        SELECT
            id AS trade_id,
            symbol,
            direction AS trade_direction,
            entry_time AS trade_entry_time,
            exit_time AS trade_exit_time,
            entry_price AS trade_entry_price,
            exit_price AS trade_exit_price,
            net_pnl_usd,
            pnl_percent,
            position_size_usdt,
            risk_percent AS trade_risk_pct,
            leverage,
            trade_mode AS trade_trade_mode
        FROM trades
        WHERE datetime(entry_time) >= datetime('now', ?)
        ORDER BY symbol ASC, trade_entry_time ASC
    """
    df = pd.read_sql_query(query, conn, params=[f"-{extra_hours} hours"])
    if df.empty:
        return df
    df["trade_entry_dt"] = pd.to_datetime(df["trade_entry_time"], errors="coerce", utc=True).dt.tz_convert(None)
    df["trade_exit_dt"] = pd.to_datetime(df["trade_exit_time"], errors="coerce", utc=True).dt.tz_convert(None)
    df = df.dropna(subset=["trade_entry_dt"])
    return df.sort_values(["symbol", "trade_entry_dt"]).reset_index(drop=True)


def _merge_nearest(
    left: pd.DataFrame,
    right: pd.DataFrame,
    left_on: str,
    right_on: str,
    tolerance: pd.Timedelta,
    suffix: str,
) -> pd.DataFrame:
    if right.empty:
        for col in right.columns:
            if col == right_on or col == "symbol":
                continue
            left[f"{col}{suffix}"] = pd.NA
        return left

    merged = pd.merge_asof(
        left.sort_values([left_on]),
        right.sort_values([right_on]),
        left_on=left_on,
        right_on=right_on,
        by="symbol",
        direction="nearest",
        tolerance=tolerance,
        suffixes=("", suffix),
    )
    return merged


def _compute_metrics(df: pd.DataFrame) -> pd.DataFrame:
    if df.empty:
        return df

    df["final_vs_baseline"] = df["final_amount_usd"] / df["baseline_amount_usd"].replace({0.0: pd.NA})
    df["ai_vs_baseline"] = df["ai_amount_usd"] / df["baseline_amount_usd"].replace({0.0: pd.NA})

    df["trade_return_pct"] = pd.NA
    mask_trade = df["net_pnl_usd"].notna() & df["final_amount_usd"].notna() & (df["final_amount_usd"] != 0)
    df.loc[mask_trade, "trade_return_pct"] = df.loc[mask_trade, "net_pnl_usd"] / df.loc[mask_trade, "final_amount_usd"] * 100.0

    df["baseline_pnl_usd"] = pd.NA
    df.loc[mask_trade, "baseline_pnl_usd"] = (
        df.loc[mask_trade, "trade_return_pct"] / 100.0 * df.loc[mask_trade, "baseline_amount_usd"]
    )

    return df


def export_dataset(hours: int, tolerance_minutes: int, output_dir: Path) -> Optional[Path]:
    tolerance = pd.Timedelta(minutes=tolerance_minutes)

    db = Database()
    conn = db.conn

    events = _load_events(conn, hours)
    if events.empty:
        print(f"‚ö†Ô∏è –ù–µ—Ç —Å–æ–±—ã—Ç–∏–π position_sizing –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {hours} —á.")
        return None

    signals = _load_signals(conn, hours, tolerance_minutes)
    trades = _load_trades(conn, hours, tolerance_minutes)

    merged = _merge_nearest(events, signals, "event_entry_dt", "signal_entry_dt", tolerance, "_sig")
    merged = _merge_nearest(merged, trades, "event_entry_dt", "trade_entry_dt", tolerance, "_trade")

    merged["signal_time_diff_sec"] = (
        (merged["event_entry_dt"] - merged["signal_entry_dt"]).abs().dt.total_seconds()
        if "signal_entry_dt" in merged
        else pd.NA
    )
    merged["trade_time_diff_sec"] = (
        (merged["event_entry_dt"] - merged["trade_entry_dt"]).abs().dt.total_seconds()
        if "trade_entry_dt" in merged
        else pd.NA
    )

    merged = _compute_metrics(merged)

    output_dir.mkdir(parents=True, exist_ok=True)
    ts = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    out_path = output_dir / f"backtest_dataset_{ts}.csv"
    merged.to_csv(out_path, index=False)

    print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(merged)} (—Å–∏–º–≤–æ–ª–æ–≤: {merged['symbol'].nunique()})")
    print(f"üìÅ –§–∞–π–ª: {out_path}")
    if "event_entry_dt" in merged:
        print(
            f"üïí –û—Ö–≤–∞—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {merged['event_entry_dt'].min()} ‚Üí {merged['event_entry_dt'].max()}"
        )
    matches = merged["signal_id"].notna().sum() if "signal_id" in merged else 0
    trade_matches = merged["trade_id"].notna().sum() if "trade_id" in merged else 0
    print(f"üîó –°–æ–≤–ø–∞–¥–µ–Ω–∏–π —Å signals_log: {matches} | —Å trades: {trade_matches}")
    return out_path


def main() -> None:
    parser = argparse.ArgumentParser(description="–≠–∫—Å–ø–æ—Ä—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è backtest replay baseline/adaptive")
    parser.add_argument("--hours", type=int, default=168, help="–ü–µ—Ä–∏–æ–¥ –≤—ã–±–æ—Ä–∫–∏ –≤ —á–∞—Å–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 168)")
    parser.add_argument(
        "--tolerance-minutes",
        type=int,
        default=10,
        help="–ú–∞–∫—Å. —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ –º–µ–∂–¥—É —Å–æ–±—ã—Ç–∏—è–º–∏ –∏ —Å–∏–≥–Ω–∞–ª–∞–º–∏ –≤ –º–∏–Ω—É—Ç–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10)",
    )
    parser.add_argument(
        "--output-dir",
        default="data/backtest",
        help="–ö–∞—Ç–∞–ª–æ–≥ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è CSV (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é data/backtest)",
    )
    args = parser.parse_args()

    output = export_dataset(int(args.hours), int(args.tolerance_minutes), Path(args.output_dir))
    if output is None:
        raise SystemExit(1)


if __name__ == "__main__":
    main()

