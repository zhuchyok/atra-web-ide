import streamlit as st
import os
import logging
import psycopg2
from psycopg2.extras import RealDictCursor
from psycopg2 import pool
import contextlib
from datetime import datetime, timezone
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

# Connection pool –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è —É—Ç–µ—á–µ–∫
@st.cache_resource
def _get_connection_pool():
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ db_session()."""
    db_urls = [
        os.getenv("DATABASE_URL"),
        "postgresql://admin:secret@localhost:5432/knowledge_os",
        "postgresql://admin:secret@127.0.0.1:5432/knowledge_os",
        "postgresql://admin:secret@knowledge_postgres:5432/knowledge_os"
    ]
    for db_url in db_urls:
        if db_url:
            try:
                # –ü—É–ª –æ—Ç 1 –¥–æ 20 —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –¥–ª—è Mac Studio
                p = pool.ThreadedConnectionPool(1, 20, db_url, cursor_factory=RealDictCursor, connect_timeout=5)
                logger.info(f"‚úÖ –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π (psycopg2) –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è {db_url}")
                return p
            except (psycopg2.Error, psycopg2.OperationalError) as e:
                logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø—É–ª –¥–ª—è {db_url}: {e}")
                continue
    return None

def _set_query_timeout(conn, seconds=15):
    """–û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤."""
    try:
        with conn.cursor() as cur:
            cur.execute(f"SET statement_timeout = '{int(seconds) * 1000}'")
    except Exception:
        pass

@contextlib.contextmanager
def db_session():
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å–µ—Å—Å–∏—è —Ä–∞–±–æ—Ç—ã —Å –ë–î —á–µ—Ä–µ–∑ –ø—É–ª."""
    p = _get_connection_pool()
    if not p:
        st.error("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ PostgreSQL.")
        yield None
        return
    
    conn = None
    try:
        conn = p.getconn()
        _set_query_timeout(conn)
        yield conn
    except Exception as e:
        if conn:
            conn.rollback()
        logger.error(f"üö® –û—à–∏–±–∫–∞ –≤ db_session: {e}")
        raise
    finally:
        if conn:
            p.putconn(conn)

def get_db_connection():
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å."""
    p = _get_connection_pool()
    if p:
        return p.getconn()
    return None

@st.cache_data(ttl=60, max_entries=100)
def fetch_data(query, params=None, cache_key=None):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø—É–ª —Å–µ—Å—Å–∏–π."""
    for attempt in range(3):
        try:
            with db_session() as conn:
                if not conn:
                    return []
                with conn.cursor() as cur:
                    cur.execute(query, params or ())
                    return cur.fetchall()
        except (psycopg2.Error, psycopg2.OperationalError, psycopg2.DatabaseError) as e:
            if "deadlock detected" in str(e).lower() and attempt < 2:
                import time
                time.sleep(0.15 * (attempt + 1))
                continue
            logger.error(f"–û—à–∏–±–∫–∞ –ë–î –≤ fetch_data: {e}")
            return []
        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –≤ fetch_data: {e}")
            return []
    return []

@st.cache_data(ttl=15, max_entries=50)
def fetch_data_tasks(query, params=None, _cache_bust=None):
    """–î–∞–Ω–Ω—ã–µ –¥–ª—è –≤–∫–ª–∞–¥–∫–∏ –ó–∞–¥–∞—á–∏ (–æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —á–∞—â–µ)."""
    return fetch_data(query, params)

def run_query(query, params=None):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç SQL –∑–∞–ø—Ä–æ—Å –Ω–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ –ø—É–ª."""
    try:
        with db_session() as conn:
            if not conn:
                return False
            with conn.cursor() as cur:
                cur.execute(query, params or ())
                conn.commit()
            return True
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ë–î –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}")
        return False

def fetch_parallel(queries_dict):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤."""
    results = {}
    with ThreadPoolExecutor(max_workers=min(len(queries_dict), 5)) as executor:
        futures = {executor.submit(fetch_data, val[0], val[1] if len(val) > 1 else ()): key 
                   for key, val in queries_dict.items()}
        for future in as_completed(futures):
            key = futures[future]
            try:
                results[key] = future.result()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ fetch_parallel –¥–ª—è {key}: {e}")
                results[key] = []
    return results

def check_services():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–µ—Ä–≤–∏—Å–æ–≤ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    services = {"PostgreSQL": "‚úÖ", "Victoria Agent": "‚úÖ"}
    is_container = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER') == 'true'
    host_url = "http://host.docker.internal" if is_container else "http://localhost"
    
    # MLX API
    try:
        import httpx
        mlx_response = httpx.get(f"{host_url}:11435/health", timeout=8, follow_redirects=True)
        if mlx_response.status_code in [200, 429, 503]:
            try:
                data = mlx_response.json()
                if data.get('status') in ['healthy', 'overloaded'] or 'service' in data:
                    services["MLX API"] = "‚úÖ"
                elif 'error' in data:
                    error_msg = str(data.get('error', '')).lower()
                    services["MLX API"] = "‚úÖ" if any(kw in error_msg for kw in ['rate limit', '429', 'overload', '503', 'concurrent']) else "‚ö†Ô∏è"
                else:
                    services["MLX API"] = "‚úÖ"
            except (ValueError, KeyError, TypeError):
                services["MLX API"] = "‚úÖ"
        else:
            services["MLX API"] = "‚ö†Ô∏è"
    except Exception:
        try:
            import httpx
            r = httpx.get("http://localhost:11435/health", timeout=8)
            services["MLX API"] = "‚úÖ" if r.status_code in [200, 429, 503] else "‚ö†Ô∏è"
        except Exception:
            services["MLX API"] = "‚ö†Ô∏è"
            
    # Ollama
    try:
        import httpx
        ollama_response = httpx.get(f"{host_url}:11434/api/tags", timeout=2)
        services["Ollama"] = "‚úÖ" if ollama_response.status_code == 200 else "‚ö†Ô∏è"
    except Exception:
        try:
            import httpx
            r = httpx.get("http://localhost:11434/api/tags", timeout=2)
            services["Ollama"] = "‚úÖ" if r.status_code == 200 else "‚ö†Ô∏è"
        except Exception:
            services["Ollama"] = "‚ö†Ô∏è"
            
    return services

@st.cache_data(ttl=60, max_entries=5)
def fetch_intellectual_capital():
    """–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ö–∞–ø–∏—Ç–∞–ª: –ø–æ–ª–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ fallback –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ usage_count/is_verified."""
    conn = None
    try:
        p = _get_connection_pool()
        if not p: return []
        conn = p.getconn()
        with conn.cursor() as cur:
            cur.execute("""
                SELECT 
                    COUNT(*) as total_nodes,
                    COALESCE(SUM(usage_count), 0) as total_usage,
                    AVG(confidence_score) as avg_confidence,
                    COUNT(*) FILTER (WHERE is_verified = true) as verified_nodes
                FROM knowledge_nodes
            """)
            res = cur.fetchall()
            p.putconn(conn)
            return res
    except Exception as e:
        if conn:
            try: conn.rollback()
            except: pass
            p.putconn(conn)
        logger.error(f"–û—à–∏–±–∫–∞ –≤ fetch_intellectual_capital: {e}")
        return []

def quick_db_check():
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å –ë–î."""
    try:
        with db_session() as conn:
            if not conn: return False, "No connection"
            with conn.cursor() as cur:
                cur.execute("SELECT 1")
                return True, None
    except Exception as e:
        return False, str(e)

def _normalize_metadata(metadata):
    """–ü—Ä–∏–≤–æ–¥–∏—Ç metadata –∫ dict (–∏–∑ –ë–î –º–æ–∂–µ—Ç –ø—Ä–∏–π—Ç–∏ JSON-—Å—Ç—Ä–æ–∫–∞)."""
    if metadata is None: return {}
    if isinstance(metadata, dict): return metadata
    if isinstance(metadata, str):
        try:
            import json
            return json.loads(metadata)
        except (json.JSONDecodeError, TypeError):
            return {}
    return {}

def get_project_slugs():
    """–°–ø–∏—Å–æ–∫ slug –ø—Ä–æ–µ–∫—Ç–æ–≤."""
    try:
        r = fetch_data("SELECT slug FROM projects WHERE is_active = true ORDER BY slug")
        return [x["slug"] for x in r] if r else []
    except Exception:
        return []

def fetch_latest_directive():
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∏—Ä–µ–∫—Ç–∏–≤—É —Å–æ–≤–µ—Ç–∞."""
    return fetch_data("""
        SELECT content, created_at FROM knowledge_nodes 
        WHERE metadata->>'type' IN ('board_directive', 'board_consult')
        ORDER BY created_at DESC LIMIT 1
    """)

def fetch_sidebar_metrics():
    """–ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏."""
    return fetch_parallel({
        "tasks": ("SELECT COUNT(*) as total, COUNT(*) FILTER (WHERE status = 'completed') as completed, COUNT(*) FILTER (WHERE status = 'in_progress') as in_progress, COUNT(*) FILTER (WHERE status = 'pending') as pending FROM tasks", ()),
        "experts": ("SELECT COUNT(*) as count FROM experts", ()),
        "finance": ("SELECT COALESCE(SUM(token_usage), 0) as total_tokens, COALESCE(SUM(cost_usd), 0) as total_cost FROM interaction_logs WHERE created_at > NOW() - INTERVAL '24 hours'", ()),
        "failed_tasks": ("SELECT id, title, metadata->>'source' as source FROM tasks WHERE status = 'failed' ORDER BY updated_at DESC LIMIT 5", ()),
        "changes": ("SELECT COUNT(*) FILTER (WHERE created_at > NOW() - INTERVAL '1 minute') as last_minute, COUNT(*) FILTER (WHERE created_at > NOW() - INTERVAL '1 hour') as last_hour FROM tasks", ())
    })

def search_knowledge_base(embedding):
    """–ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥—É."""
    return fetch_data("""
        SELECT LEFT(k.content, 300) as content, d.name as domain, (1 - (k.embedding <=> %s::vector)) as similarity
        FROM knowledge_nodes k JOIN domains d ON k.domain_id = d.id
        WHERE k.embedding IS NOT NULL
        ORDER BY similarity DESC LIMIT 5
    """, (str(embedding),))
