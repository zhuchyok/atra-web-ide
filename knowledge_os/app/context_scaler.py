"""
Context Scaler –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
"""

import asyncio
import logging
import asyncpg
from typing import Dict, Optional, Tuple
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class ContextScaler:
    """
    –ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –ø–æ–¥–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ max_tokens –∏ context_window.
    """
    
    def __init__(self, db_url: str = None):
        import os
        self.db_url = db_url or os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
        self.task_type_stats: Dict[str, Dict] = {}
        
    async def analyze_request_history(self, task_type: str, days: int = 7) -> Dict:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏"""
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å—ã –∏–∑ semantic_ai_cache
                rows = await conn.fetch("""
                    SELECT 
                        LENGTH(query_text) as prompt_length,
                        LENGTH(response_text) as response_length,
                        performance_score,
                        created_at
                    FROM semantic_ai_cache
                    WHERE created_at > NOW() - INTERVAL '%s days'
                    AND query_text ILIKE '%' || $1 || '%'
                    ORDER BY created_at DESC
                    LIMIT 100
                """, days, task_type)
                
                if not rows:
                    return {"avg_prompt_length": 500, "avg_response_length": 1000, "count": 0}
                
                total_prompt = sum(row['prompt_length'] or 0 for row in rows)
                total_response = sum(row['response_length'] or 0 for row in rows)
                count = len(rows)
                
                return {
                    "avg_prompt_length": total_prompt // count if count > 0 else 500,
                    "avg_response_length": total_response // count if count > 0 else 1000,
                    "max_prompt_length": max((row['prompt_length'] or 0 for row in rows), default=500),
                    "max_response_length": max((row['response_length'] or 0 for row in rows), default=1000),
                    "count": count
                }
            finally:
                await conn.close()
        except Exception as e:
            logger.debug(f"Error analyzing request history: {e}")
            return {"avg_prompt_length": 500, "avg_response_length": 1000, "count": 0}
    
    def calculate_optimal_params(self, stats: Dict, task_type: str) -> Tuple[int, int]:
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.
        
        Returns:
            (max_tokens, context_window)
        """
        avg_response = stats.get("avg_response_length", 1000)
        max_response = stats.get("max_response_length", 2000)
        avg_prompt = stats.get("avg_prompt_length", 500)
        
        # –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        base_max_tokens = 2000
        base_context = 4000
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –∑–∞–¥–∞—á–∏
        if task_type in ["coding", "refactoring"]:
            # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            max_tokens = int(max_response * 1.5) + 500
            context_window = int((avg_prompt + max_tokens) * 1.2)
        elif task_type in ["analysis", "planning"]:
            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–±—É–µ—Ç —Å—Ä–µ–¥–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            max_tokens = int(avg_response * 1.3) + 300
            context_window = int((avg_prompt + max_tokens) * 1.1)
        else:
            # –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏
            max_tokens = int(avg_response * 1.2) + 200
            context_window = int((avg_prompt + max_tokens) * 1.05)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        max_tokens = min(max_tokens, 8000)  # –ú–∞–∫—Å–∏–º—É–º 8k —Ç–æ–∫–µ–Ω–æ–≤
        context_window = min(context_window, 16000)  # –ú–∞–∫—Å–∏–º—É–º 16k –∫–æ–Ω—Ç–µ–∫—Å—Ç
        
        return max_tokens, context_window
    
    async def get_optimal_params(self, task_type: str, prompt_length: int) -> Tuple[int, int]:
        """–ü–æ–ª—É—á–∏—Ç—å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –∑–∞–¥–∞—á–∏"""
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
        stats = await self.analyze_request_history(task_type)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        max_tokens, context_window = self.calculate_optimal_params(stats, task_type)
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        if prompt_length > context_window * 0.8:
            context_window = int(prompt_length * 1.3)
        
        logger.debug(f"üìè [CONTEXT SCALER] {task_type}: max_tokens={max_tokens}, context={context_window}")
        
        return max_tokens, context_window

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
_scaler: Optional[ContextScaler] = None

def get_context_scaler(db_url: str = None) -> ContextScaler:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä ContextScaler"""
    global _scaler
    if _scaler is None:
        _scaler = ContextScaler(db_url)
    return _scaler

