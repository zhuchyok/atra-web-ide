"""
–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤, —É–ª—É—á—à–µ–Ω–∏—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
Singularity 5.0: Advanced Optimizations
"""

import asyncio
import logging
from typing import List, Dict, Optional, Any, Tuple
import hashlib
import json

logger = logging.getLogger(__name__)

class PromptOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
    """
    
    @staticmethod
    def compress_prompt(prompt: str, max_length: int = 2000) -> str:
        """–°–∂–∞—Ç–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã"""
        if len(prompt) <= max_length:
            return prompt
        
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        prompt = " ".join(prompt.split())
        
        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –¥–ª–∏–Ω–Ω—ã–π, –æ–±—Ä–µ–∑–∞–µ–º
        if len(prompt) > max_length:
            # –û–±—Ä–µ–∑–∞–µ–º —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            prompt = prompt[:max_length-50] + "...\n[–¢–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤]"
        
        return prompt
    
    @staticmethod
    def remove_redundancy(prompt: str) -> str:
        """–£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ –∏–∑ –ø—Ä–æ–º–ø—Ç–∞"""
        lines = prompt.split('\n')
        seen = set()
        result = []
        
        for line in lines:
            line_stripped = line.strip()
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏ –∏ –¥—É–±–ª–∏–∫–∞—Ç—ã
            if line_stripped and line_stripped not in seen:
                seen.add(line_stripped)
                result.append(line)
        
        return '\n'.join(result)

class BatchProcessor:
    """
    Batch processing –¥–ª—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (—ç–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤)
    """
    
    def __init__(self):
        self.batch_queue = []
        self.batch_size = 5
        self.batch_timeout = 2.0  # —Å–µ–∫—É–Ω–¥—ã
    
    async def add_to_batch(self, prompt: str, category: str) -> str:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –≤ batch –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç"""
        # –î–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å
        if len(self.batch_queue) < self.batch_size:
            self.batch_queue.append((prompt, category))
            # –ñ–¥–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è batch –∏–ª–∏ timeout
            await asyncio.sleep(self.batch_timeout)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch
        if self.batch_queue:
            return await self._process_batch()
        return None
    
    async def _process_batch(self) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ batch –∑–∞–ø—Ä–æ—Å–æ–≤"""
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å—ã –≤ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç
        combined_prompt = "\n\n".join([f"–ó–∞–ø—Ä–æ—Å {i+1}: {p[0]}" for i, p in enumerate(self.batch_queue)])
        
        # –û—á–∏—â–∞–µ–º –æ—á–µ—Ä–µ–¥—å
        self.batch_queue = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        # (–∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏)
        return combined_prompt

class PredictiveCache:
    """
    –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ - –ø—Ä–µ–¥-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã.
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–∞–ª–µ–Ω–¥–∞—Ä—å/–∏—Å—Ç–æ—Ä–∏—é –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤.
    """
    
    def __init__(self, cache_manager, db_url: Optional[str] = None):
        self.cache = cache_manager
        self.prediction_queue = []
        self.db_url = db_url or os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
        self.query_patterns = {}  # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self._background_task_running = False  # –§–ª–∞–≥ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏
        
        # –ò–º–ø–æ—Ä—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ë–î
        try:
            import asyncpg
            self.asyncpg = asyncpg
        except ImportError:
            self.asyncpg = None
        
    
    async def analyze_query_history(self, hours: int = 24) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        if not self.asyncpg or not self.db_url:
            return {}
        
        try:
            conn = await self.asyncpg.connect(self.db_url)
            try:
                # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏–∑ semantic_ai_cache —Å –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–æ–π –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é/—Å–µ—Å—Å–∏–∏
                rows = await conn.fetch("""
                    SELECT query_text, created_at, expert_name
                    FROM semantic_ai_cache
                    WHERE created_at > NOW() - INTERVAL '1 hour' * $1
                    ORDER BY created_at ASC
                    LIMIT 200
                """, hours)
                
                if not rows:
                    return {}
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                patterns = {
                    "keywords": {},  # –ß–∞—Å—Ç–æ—Ç–∞ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                    "sequences": {},  # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
                    "contexts": {},  # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (—á—Ç–æ –∏–¥–µ—Ç –≤–º–µ—Å—Ç–µ)
                    "temporal": {},  # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                    "categories": {}  # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
                }
                
                queries = []
                for row in rows:
                    query = row['query_text'].lower()
                    queries.append({
                        'text': query,
                        'timestamp': row['created_at'],
                        'expert': row.get('expert_name', 'unknown')
                    })
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                    keywords = self._extract_keywords(query)
                    for keyword in keywords:
                        patterns["keywords"][keyword] = patterns["keywords"].get(keyword, 0) + 1
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é
                    category = self._categorize_query(query)
                    patterns["categories"][category] = patterns["categories"].get(category, 0) + 1
                
                # –ê–Ω–∞–ª–∏–∑ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π (—á—Ç–æ —Å–ª–µ–¥—É–µ—Ç –ø–æ—Å–ª–µ —á–µ–≥–æ)
                for i in range(len(queries) - 1):
                    current = queries[i]
                    next_query = queries[i + 1]
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ø—Ä–æ—Å—ã –±–ª–∏–∑–∫–∏ –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–≤ –ø—Ä–µ–¥–µ–ª–∞—Ö 10 –º–∏–Ω—É—Ç)
                    time_diff = (next_query['timestamp'] - current['timestamp']).total_seconds()
                    if time_diff < 600:  # 10 –º–∏–Ω—É—Ç
                        current_keywords = self._extract_keywords(current['text'])
                        next_keywords = self._extract_keywords(next_query['text'])
                        
                        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                        if current_keywords and next_keywords:
                            pattern_key = f"{current_keywords[0]} -> {next_keywords[0]}"
                            patterns["sequences"][pattern_key] = patterns["sequences"].get(pattern_key, 0) + 1
                
                # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ (–∑–∞–ø—Ä–æ—Å—ã –≤ –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏)
                expert_queries = {}
                for query in queries:
                    expert = query['expert']
                    if expert not in expert_queries:
                        expert_queries[expert] = []
                    expert_queries[expert].append(query)
                
                for expert, expert_qs in expert_queries.items():
                    if len(expert_qs) >= 2:
                        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º, –∫–∞–∫–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ —á–∞—Å—Ç–æ –∏–¥—É—Ç –≤–º–µ—Å—Ç–µ
                        for i in range(len(expert_qs) - 1):
                            for j in range(i + 1, min(i + 3, len(expert_qs))):  # –°–ª–µ–¥—É—é—â–∏–µ 2 –∑–∞–ø—Ä–æ—Å–∞
                                keywords_i = set(self._extract_keywords(expert_qs[i]['text']))
                                keywords_j = set(self._extract_keywords(expert_qs[j]['text']))
                                common = keywords_i & keywords_j
                                if common:
                                    context_key = " & ".join(sorted(common))
                                    patterns["contexts"][context_key] = patterns["contexts"].get(context_key, 0) + 1
                
                # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, —á–∞—Å –¥–Ω—è)
                for query in queries:
                    timestamp = query['timestamp']
                    day_of_week = timestamp.strftime('%A')
                    hour = timestamp.hour
                    
                    time_key = f"{day_of_week}_{hour}"
                    patterns["temporal"][time_key] = patterns["temporal"].get(time_key, 0) + 1
                
                return patterns
            finally:
                await conn.close()
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤: {e}")
            return {}
    
    def _categorize_query(self, query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–∞"""
        query_lower = query.lower()
        
        if any(kw in query_lower for kw in ["–∫–æ–¥", "—Ñ—É–Ω–∫—Ü–∏—è", "–∫–ª–∞—Å—Å", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–π"]):
            return "coding"
        elif any(kw in query_lower for kw in ["–æ—à–∏–±–∫–∞", "–±–∞–≥", "–∏—Å–ø—Ä–∞–≤–∏—Ç—å", "–ø—Ä–æ–±–ª–µ–º–∞"]):
            return "error"
        elif any(kw in query_lower for kw in ["—Ç–µ—Å—Ç", "–ø—Ä–æ–≤–µ—Ä–∫–∞", "–≤–∞–ª–∏–¥–∞—Ü–∏—è"]):
            return "testing"
        elif any(kw in query_lower for kw in ["–æ–±—ä—è—Å–Ω–∏", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç"]):
            return "explanation"
        elif any(kw in query_lower for kw in ["–æ–ø—Ç–∏–º–∏–∑–∞—Ü", "—É–ª—É—á—à", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥"]):
            return "optimization"
        else:
            return "general"
    
    def _extract_keywords(self, query: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –∏—â–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã
        tech_keywords = [
            "–∫–æ–¥", "—Ñ—É–Ω–∫—Ü–∏—è", "–∫–ª–∞—Å—Å", "—Ç–µ—Å—Ç", "–æ—à–∏–±–∫–∞", "–±–∞–≥", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥",
            "–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è", "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "–¥–∏–∑–∞–π–Ω",
            "–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö", "api", "endpoint", "–º–∏–≥—Ä–∞—Ü–∏—è", "–¥–µ–ø–ª–æ–π"
        ]
        
        found_keywords = []
        for keyword in tech_keywords:
            if keyword in query:
                found_keywords.append(keyword)
        
        return found_keywords
    
    async def predict_next_queries(self, current_query: str) -> List[str]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –∏ –∏—Å—Ç–æ—Ä–∏–∏ —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        predictions = []
        query_lower = current_query.lower()
        
        # –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        history_patterns = await self.analyze_query_history(hours=24)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        current_keywords = self._extract_keywords(current_query)
        current_category = self._categorize_query(current_query)
        
        # 1. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        if history_patterns.get("sequences"):
            sequences = history_patterns["sequences"]
            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—á–∏–Ω–∞—é—â–∏–µ—Å—è —Å —Ç–µ–∫—É—â–∏—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            for seq_key, count in sorted(sequences.items(), key=lambda x: x[1], reverse=True)[:5]:
                if " -> " in seq_key:
                    start_keyword, next_keyword = seq_key.split(" -> ", 1)
                    if any(kw in current_query.lower() for kw in [start_keyword]):
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                        predictions.append(f"–ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è {next_keyword}")
        
        # 2. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if history_patterns.get("contexts"):
            contexts = history_patterns["contexts"]
            # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—Å—Ç–æ –∏–¥—É—Ç –≤–º–µ—Å—Ç–µ —Å —Ç–µ–∫—É—â–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            for context_key, count in sorted(contexts.items(), key=lambda x: x[1], reverse=True)[:3]:
                context_keywords = context_key.split(" & ")
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å —Ç–µ–∫—É—â–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
                if any(kw in current_keywords for kw in context_keywords):
                    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã —Å –¥—Ä—É–≥–∏–º–∏ –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    for kw in context_keywords:
                        if kw not in current_keywords:
                            predictions.append(f"–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {kw}")
        
        # 3. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (—á—Ç–æ –æ–±—ã—á–Ω–æ —Å–ª–µ–¥—É–µ—Ç –ø–æ—Å–ª–µ —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)
        category_transitions = {
            "coding": ["—Ç–µ—Å—Ç", "–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è", "–¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥"],
            "error": ["–∏—Å–ø—Ä–∞–≤–∏—Ç—å", "–ø—Ä–∏—á–∏–Ω–∞", "–ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å", "–ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ"],
            "testing": ["–∑–∞–ø—É—Å—Ç–∏—Ç—å", "–ø–æ–∫—Ä—ã—Ç–∏–µ", "–µ—â–µ —Ç–µ—Å—Ç—ã", "–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è"],
            "explanation": ["–ø—Ä–∏–º–µ—Ä", "–¥–µ—Ç–∞–ª–∏", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ", "–ª—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏"],
            "optimization": ["–∏–∑–º–µ—Ä–∏—Ç—å", "–ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ", "–±–µ–Ω—á–º–∞—Ä–∫–∏", "–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã"]
        }
        
        if current_category in category_transitions:
            for next_action in category_transitions[current_category]:
                predictions.append(f"{next_action} –¥–ª—è —ç—Ç–æ–≥–æ")
        
        # 4. –ë–∞–∑–æ–≤—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (fallback)
        if "–∫–æ–¥" in query_lower or "—Ñ—É–Ω–∫—Ü–∏—è" in query_lower:
            predictions.extend([
                "–Ω–∞–ø–∏—à–∏ —Ç–µ—Å—Ç –¥–ª—è —ç—Ç–æ–≥–æ –∫–æ–¥–∞",
                "–∫–∞–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –∫–æ–¥",
                "–∫–∞–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ —ç—Ç–æ—Ç –∫–æ–¥",
                "–∫–∞–∫ —É–ª—É—á—à–∏—Ç—å —á–∏—Ç–∞–µ–º–æ—Å—Ç—å —ç—Ç–æ–≥–æ –∫–æ–¥–∞"
            ])
        
        if "–æ—à–∏–±–∫–∞" in query_lower or "–±–∞–≥" in query_lower:
            predictions.extend([
                "–∫–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å —ç—Ç—É –æ—à–∏–±–∫—É",
                "–≤ —á–µ–º –ø—Ä–∏—á–∏–Ω–∞ —ç—Ç–æ–π –æ—à–∏–±–∫–∏",
                "–∫–∞–∫ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å —ç—Ç—É –æ—à–∏–±–∫—É –≤ –±—É–¥—É—â–µ–º"
            ])
        
        if "—Ç–µ—Å—Ç" in query_lower:
            predictions.extend([
                "–∫–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å —ç—Ç–æ—Ç —Ç–µ—Å—Ç",
                "–∫–∞–∫ —É–ª—É—á—à–∏—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏",
                "–∫–∞–∫–∏–µ –µ—â–µ —Ç–µ—Å—Ç—ã –Ω—É–∂–Ω—ã"
            ])
        
        # 5. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–æ–ø –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
        if history_patterns.get("keywords"):
            top_keywords = sorted(history_patterns["keywords"].items(), key=lambda x: x[1], reverse=True)[:3]
            for keyword, count in top_keywords:
                if keyword not in current_keywords and count >= 3:  # –ú–∏–Ω–∏–º—É–º 3 –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
                    predictions.append(f"–ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è {keyword}")
        
        # 6. –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –¥–Ω—è)
        from datetime import datetime
        current_time = datetime.now()
        current_hour = current_time.hour
        day_of_week = current_time.strftime('%A')
        
        if history_patterns.get("temporal"):
            temporal = history_patterns["temporal"]
            # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏
            time_key = f"{day_of_week}_{current_hour}"
            similar_time_keys = [k for k in temporal.keys() if k.startswith(day_of_week)]
            
            if similar_time_keys:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —ç—Ç–æ–≥–æ –¥–Ω—è –Ω–µ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö
                pass  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        
        if 6 <= current_hour < 12:  # –£—Ç—Ä–æ
            predictions.extend([
                "–∫–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –Ω–∞ —Å–µ–≥–æ–¥–Ω—è",
                "–∫–∞–∫–æ–π –ø–ª–∞–Ω —Ä–∞–±–æ—Ç—ã",
                "–∫–∞–∫–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã"
            ])
        elif 18 <= current_hour < 22:  # –í–µ—á–µ—Ä
            predictions.extend([
                "–∫–∞–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞ –¥–µ–Ω—å",
                "—á—Ç–æ —Å–¥–µ–ª–∞–Ω–æ —Å–µ–≥–æ–¥–Ω—è",
                "–∫–∞–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã –≤–æ–∑–Ω–∏–∫–ª–∏"
            ])
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
        unique_predictions = []
        seen = set()
        for pred in predictions:
            pred_lower = pred.lower()
            if pred_lower not in seen:
                seen.add(pred_lower)
                unique_predictions.append(pred)
        
        return unique_predictions[:5]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 5 –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    
    async def predict_and_cache(self, current_query: str, expert_name: str):
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –∏ –ø—Ä–µ–¥-–∫—ç—à–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã"""
        predictions = await self.predict_next_queries(current_query)
        
        # –ü—Ä–µ–¥-–∫—ç—à–∏—Ä—É–µ–º –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
        for pred_query in predictions:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ —É–∂–µ –≤ –∫—ç—à–µ
            cached = await self.cache.get_cached_response(pred_query, expert_name)
            if not cached:
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å –¥–ª—è –ø—Ä–µ–¥-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ —Ñ–æ–Ω–µ)
                self.prediction_queue.append((pred_query, expert_name))
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ñ–æ–Ω–æ–≤—É—é –∑–∞–¥–∞—á—É –¥–ª—è –ø—Ä–µ–¥-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        if self.prediction_queue:
            asyncio.create_task(self._warm_cache_background())
        
        return len(predictions)
    
    async def _warm_cache_background(self):
        """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è –ø—Ä–µ–¥-–≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤"""
        while self.prediction_queue:
            pred_query, expert_name = self.prediction_queue.pop(0)
            try:
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ ai_core
                from ai_core import run_smart_agent_async
                response = await run_smart_agent_async(pred_query, expert_name=expert_name)
                
                if response:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                    await self.cache.save_to_cache(pred_query, response, expert_name)
                    logger.debug(f"‚úÖ [PREDICTIVE CACHE] –ü—Ä–µ–¥-–∫—ç—à–∏—Ä–æ–≤–∞–Ω –æ—Ç–≤–µ—Ç –¥–ª—è: {pred_query[:50]}...")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [PREDICTIVE CACHE] –û—à–∏–±–∫–∞ –ø—Ä–µ–¥-–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

class ResponseStreamer:
    """
    –°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–æ–≤ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–µ–º–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
    """
    
    @staticmethod
    async def stream_response(response: str, chunk_size: int = 50):
        """–°—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ –ø–æ —á–∞—Å—Ç—è–º"""
        words = response.split()
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i+chunk_size])
            yield chunk
            await asyncio.sleep(0.05)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è

class SmartRouter:
    """
    –£–º–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ (—ç–∫–æ–Ω–æ–º–∏—è —Ç–æ–∫–µ–Ω–æ–≤ + —Å–∫–æ—Ä–æ—Å—Ç—å)
    """
    
    def __init__(self):
        self.route_cache = {}  # –ö—ç—à —Ä–µ—à–µ–Ω–∏–π —Ä–æ—É—Ç–∏–Ω–≥–∞
    
    async def choose_optimal_route(
        self, 
        prompt: str, 
        category: Optional[str] = None,
        history: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        –í—ã–±–∏—Ä–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        - –°–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞
        - –ò—Å—Ç–æ—Ä–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤
        - –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —É–∑–ª–æ–≤
        - –û–∂–∏–¥–∞–µ–º–æ–π —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –Ω–∞—á–∞–ª–∞
        route = {
            "use_local": True,
            "use_web": False,
            "use_cache": True,
            "expected_tokens": 0,
            "expected_latency": 0.5
        }
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        complexity = self._estimate_complexity(prompt, category)
        
        if complexity == "simple":
            route["use_local"] = True
            route["expected_tokens"] = 0
            route["expected_latency"] = 0.3
        elif complexity == "medium":
            route["use_local"] = True
            route["use_web"] = any(kw in prompt.lower() for kw in ["–Ω–æ–≤–æ—Å—Ç–∏", "—Ç—Ä–µ–Ω–¥—ã", "—Å–µ–π—á–∞—Å"])
            route["expected_tokens"] = 0
            route["expected_latency"] = 1.0
        else:  # complex
            route["use_local"] = False  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–ª–∞–∫–æ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
            route["expected_tokens"] = 2000
            route["expected_latency"] = 3.0
        
        return route
    
    def _estimate_complexity(self, prompt: str, category: Optional[str]) -> str:
        """–û—Ü–µ–Ω–∫–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        simple_keywords = ["–æ–±—ä—è—Å–Ω–∏", "—á—Ç–æ —Ç–∞–∫–æ–µ", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç", "–ø—Ä–∏–º–µ—Ä"]
        complex_keywords = ["–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è", "–ø—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ", "–¥–∏–∑–∞–π–Ω"]
        
        prompt_lower = prompt.lower()
        
        if any(kw in prompt_lower for kw in complex_keywords):
            return "complex"
        elif any(kw in prompt_lower for kw in simple_keywords):
            return "simple"
        else:
            return "medium"

class EmbeddingCache:
    """
    –ö—ç—à —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    """
    
    def __init__(self, db_url: str):
        self.db_url = db_url
        self.memory_cache = {}  # In-memory –∫—ç—à
        self.cache_size = 1000
    
    async def get_or_compute_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ –∫—ç—à–∞ –∏–ª–∏ –≤—ã—á–∏—Å–ª—è–µ—Ç"""
        # –•—ç—à —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–ª—é—á–∞
        text_hash = hashlib.md5(text.encode()).hexdigest()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º memory cache
        if text_hash in self.memory_cache:
            return self.memory_cache[text_hash]
        
        # –í—ã—á–∏—Å–ª—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ (–∑–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞)
        # embedding = await compute_embedding(text)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
        if len(self.memory_cache) >= self.cache_size:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ (FIFO)
            oldest_key = next(iter(self.memory_cache))
            del self.memory_cache[oldest_key]
        
        # self.memory_cache[text_hash] = embedding
        # return embedding
        
        return []  # Placeholder

class ParallelProcessor:
    """
    –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
    """
    
    def __init__(self, max_concurrent: int = 3):
        """
        Args:
            max_concurrent: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        """
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def process_parallel(self, tasks: List[Any]) -> List[Any]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –Ω–∞–≥—Ä—É–∑–∫–∏.
        
        Args:
            tasks: –°–ø–∏—Å–æ–∫ async —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        async def process_with_semaphore(task):
            async with self.semaphore:
                return await task
        
        results = await asyncio.gather(*[process_with_semaphore(task) for task in tasks], return_exceptions=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏—è
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå [PARALLEL] Task {i} failed: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def process_batch_parallel(
        self,
        prompts: List[str],
        expert_name: str,
        category: Optional[str] = None
    ) -> List[str]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–æ–º–ø—Ç–æ–≤.
        
        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
            expert_name: –ò–º—è —ç–∫—Å–ø–µ—Ä—Ç–∞
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–¥–∞—á–∏
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        from ai_core import run_smart_agent_async
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏
        tasks = [
            run_smart_agent_async(prompt, expert_name=expert_name, category=category)
            for prompt in prompts
        ]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        return await self.process_parallel(tasks)

class FrugalPrompt:
    """
    FrugalPrompt - —É–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞ —Å–∂–∞—Ç–∏—è –ø—Ä–æ–º–ø—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∏—Ä–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ 2024-2025.
    –£–¥–∞–ª—è–µ—Ç –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å, —Å–∂–∏–º–∞–µ—Ç –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
    """
    
    # –®–∞–±–ª–æ–Ω—ã –¥–ª—è –∑–∞–º–µ–Ω—ã –¥–ª–∏–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ
    INSTRUCTION_PATTERNS = {
        # –î–ª–∏–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ -> –∫–æ—Ä–æ—Ç–∫–∏–µ —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç—ã
        r'–ø–æ–∂–∞–ª—É–π—Å—Ç–∞\s+(?:–±—É–¥—å—Ç–µ\s+)?(?:—É–≤–µ—Ä–µ–Ω—ã|—É–±–µ–¥–∏—Ç–µ—Å—å|—É–±–µ–¥–∏—Å—å)\s+—á—Ç–æ': '—É–±–µ–¥–∏—Å—å:',
        r'—è\s+—Ö–æ—á—É\s+—á—Ç–æ–±—ã\s+—Ç—ã\s+': '—Å–¥–µ–ª–∞–π:',
        r'–º–æ–∂–µ—à—å\s+(?:–ª–∏\s+—Ç—ã\s+)?(?:–ø–æ–∂–∞–ª—É–π—Å—Ç–∞\s+)?': '',
        r'–µ—Å–ª–∏\s+–≤–æ–∑–º–æ–∂–Ω–æ[,\s]*': '',
        r'\s+–∏\s+—Ç–∞–∫–∂–µ\s+': ', ',
        r'\s+–≤\s+–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ\s+–∫\s+': '+',
        r'–ø–æ–∂–∞–ª—É–π—Å—Ç–∞\s+': '',
        r'\s+–æ—á–µ–Ω—å\s+': ' ',
        r'\s+–¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ\s+': ' ',
        r'\s+–Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ\s+': ' ',
    }
    
    # –°—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è
    STOP_WORDS = {
        '–∫–æ–Ω–µ—á–Ω–æ', '—Ä–∞–∑—É–º–µ–µ—Ç—Å—è', '–±–µ–∑—É—Å–ª–æ–≤–Ω–æ', '–æ—á–µ–≤–∏–¥–Ω–æ', '–ø–æ–Ω—è—Ç–Ω–æ',
        '—è—Å–Ω–æ', '–µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ', '–∫–æ–Ω–µ—á–Ω–æ –∂–µ', '—Ä–∞–∑—É–º–µ–µ—Ç—Å—è –∂–µ'
    }
    
    @staticmethod
    def compress_instruction(instruction: str) -> str:
        """–°–∂–∏–º–∞–µ—Ç –æ–¥–Ω—É –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é, —É–¥–∞–ª—è—è –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç—å"""
        import re
        compressed = instruction.strip().lower()
        
        # –ó–∞–º–µ–Ω—è–µ–º –¥–ª–∏–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ
        for pattern, replacement in FrugalPrompt.INSTRUCTION_PATTERNS.items():
            compressed = re.sub(pattern, replacement, compressed, flags=re.IGNORECASE)
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        words = compressed.split()
        words = [w for w in words if w.lower() not in FrugalPrompt.STOP_WORDS]
        compressed = ' '.join(words)
        
        # –£–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        compressed = re.sub(r'\s+', ' ', compressed)
        
        return compressed.strip()
    
    @staticmethod
    def remove_boilerplate(prompt: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç —à–∞–±–ª–æ–Ω–Ω—ã–µ —á–∞—Å—Ç–∏ –∏–∑ –ø—Ä–æ–º–ø—Ç–∞"""
        lines = prompt.split('\n')
        filtered = []
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å —Ç–∏–ø–∏—á–Ω—ã–º–∏ —à–∞–±–ª–æ–Ω–∞–º–∏
        boilerplate_patterns = [
            r'^(–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ|–ø—Ä–∏–≤–µ—Ç|–¥–æ–±—Ä—ã–π\s+(?:–¥–µ–Ω—å|–≤–µ—á–µ—Ä|—É—Ç—Ä–æ))',
            r'^(—Å–ø–∞—Å–∏–±–æ|–±–ª–∞–≥–æ–¥–∞—Ä—é|–±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å)',
            r'^(–Ω–∞–¥–µ—é—Å—å|–Ω–∞–¥–µ–µ–º—Å—è)\s+',
            r'^(—Å\s+—É–≤–∞–∂–µ–Ω–∏–µ–º|—Å\s+–Ω–∞–∏–ª—É—á—à–∏–º–∏\s+–ø–æ–∂–µ–ª–∞–Ω–∏—è–º–∏)',
        ]
        
        import re
        for line in lines:
            line_stripped = line.strip()
            if not line_stripped:
                continue
            
            is_boilerplate = False
            for pattern in boilerplate_patterns:
                if re.match(pattern, line_stripped, re.IGNORECASE):
                    is_boilerplate = True
                    break
            
            if not is_boilerplate:
                filtered.append(line)
        
        return '\n'.join(filtered)
    
    @staticmethod
    def compress_structure(prompt: str) -> str:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–º–ø—Ç–∞"""
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞—Å—Ç–∏
        sections = prompt.split('\n\n')
        compressed_sections = []
        
        for section in sections:
            section = section.strip()
            if not section:
                continue
            
            # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –º–∞—Ä–∫–µ—Ä–∞ —Å–ø–∏—Å–∫–∞, –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º
            if any(section.startswith(marker) for marker in ['-', '*', '‚Ä¢', '1.', '2.']):
                # –ö–æ–º–ø—Ä–µ—Å—Å–∏—è —Å–ø–∏—Å–∫–æ–≤
                lines = section.split('\n')
                compressed_lines = []
                for line in lines:
                    compressed_line = FrugalPrompt.compress_instruction(line)
                    if compressed_line:
                        compressed_lines.append(compressed_line)
                section = '\n'.join(compressed_lines)
            else:
                section = FrugalPrompt.compress_instruction(section)
            
            if section:
                compressed_sections.append(section)
        
        return '\n\n'.join(compressed_sections)
    
    @classmethod
    def compress(cls, prompt: str, max_length: int = 2000, aggressive: bool = True) -> str:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ —Å–∂–∞—Ç–∏—è –ø—Ä–æ–º–ø—Ç–∞ –ø–æ —Ç–µ—Ö–Ω–∏–∫–µ FrugalPrompt.
        
        Args:
            prompt: –ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
            aggressive: –ê–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ (—É–¥–∞–ª—è–µ—Ç –±–æ–ª—å—à–µ)
        
        Returns:
            –°–∂–∞—Ç—ã–π –ø—Ä–æ–º–ø—Ç
        """
        if len(prompt) <= max_length and not aggressive:
            return prompt
        
        # –®–∞–≥ 1: –£–¥–∞–ª–µ–Ω–∏–µ boilerplate
        compressed = cls.remove_boilerplate(prompt)
        
        # –®–∞–≥ 2: –ö–æ–º–ø—Ä–µ—Å—Å–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        compressed = cls.compress_structure(compressed)
        
        # –®–∞–≥ 3: –£–¥–∞–ª–µ–Ω–∏–µ –∏–∑–±—ã—Ç–æ—á–Ω–æ—Å—Ç–∏ (–∫–∞–∫ –≤ PromptOptimizer)
        compressed = PromptOptimizer.remove_redundancy(compressed)
        
        # –®–∞–≥ 4: –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–∂–∞—Ç–∏–µ –¥–æ max_length
        if len(compressed) > max_length:
            # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –¥–ª–∏–Ω–Ω—ã–π, –ø—Ä–∏–º–µ–Ω—è–µ–º –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
            compressed = PromptOptimizer.compress_prompt(compressed, max_length)
        
        return compressed

class BETokenManager:
    """
    BE-Token (Behavior-Equivalent Token) Manager.
    –ó–∞–º–µ–Ω—è–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ–¥–Ω–∏–º —Ç–æ–∫–µ–Ω–æ–º/—à–∞–±–ª–æ–Ω–æ–º –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤.
    """
    
    # –°–ª–æ–≤–∞—Ä—å BE-Token: —Ç–æ–∫–µ–Ω -> –ø–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
    _token_map: Dict[str, str] = {}
    
    # –°–ª–æ–≤–∞—Ä—å –æ–±—Ä–∞—Ç–Ω—ã–π: –ø–∞—Ç—Ç–µ—Ä–Ω -> —Ç–æ–∫–µ–Ω
    _pattern_map: Dict[str, str] = {}
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤
    _usage_stats: Dict[str, int] = {}
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –±–∞–∑–æ–≤—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏"""
        self._initialize_base_tokens()
    
    def _initialize_base_tokens(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö BE-Token"""
        base_tokens = {
            # –¢–æ—Ä–≥–æ–≤–ª—è
            'TRADE_SIGNAL': '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ä—ã–Ω–æ–∫ –∏ –≤—ã–¥–∞–π —Ç–æ—Ä–≥–æ–≤—ã–π —Å–∏–≥–Ω–∞–ª —Å entry/tp/sl',
            'BACKTEST': '–ø—Ä–æ–≤–µ–¥–∏ –±—ç–∫—Ç–µ—Å—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö',
            'RISK_ANALYSIS': '—Ä–∞—Å—Å—á–∏—Ç–∞–π —Ä–∏—Å–∫–∏ –∏ position sizing',
            
            # –ü—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
            'CODE_REVIEW': '–ø—Ä–æ–≤–µ–¥–∏ code review –∏ –Ω–∞–π–¥–∏ –ø—Ä–æ–±–ª–µ–º—ã',
            'WRITE_TEST': '–Ω–∞–ø–∏—à–∏ unit-—Ç–µ—Å—Ç—ã –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏',
            'REFACTOR': '–æ—Ç—Ä–µ—Ñ–∞–∫—Ç–æ—Ä—å –∫–æ–¥ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏',
            'OPTIMIZE': '–æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ–¥–∞',
            'FIX_BUG': '–Ω–∞–π–¥–∏ –∏ –∏—Å–ø—Ä–∞–≤—å –±–∞–≥',
            
            # –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
            'WRITE_DOCS': '–Ω–∞–ø–∏—à–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏/–∫–ª–∞—Å—Å–∞',
            'EXPLAIN': '–æ–±—ä—è—Å–Ω–∏ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–¥',
            
            # –û–±—â–µ–µ
            'ANALYZE': '–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∏ –≤—ã–¥–∞–π –≤—ã–≤–æ–¥—ã',
            'SUMMARIZE': '–∫—Ä–∞—Ç–∫–æ —Å—É–º–º–∏—Ä—É–π –æ—Å–Ω–æ–≤–Ω–æ–µ',
        }
        
        for token, instruction in base_tokens.items():
            self.register_token(token, instruction)
    
    def register_token(self, token: str, instruction: str, pattern: Optional[str] = None):
        """
        –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–π BE-Token.
        
        Args:
            token: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'TRADE_SIGNAL')
            instruction: –ü–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è
            pattern: –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç instruction)
        """
        self._token_map[token] = instruction
        pattern_key = pattern or instruction.lower()
        self._pattern_map[pattern_key] = token
        self._usage_stats[token] = 0
    
    def find_token(self, prompt: str) -> Optional[str]:
        """
        –ò—â–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π BE-Token –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞.
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞ –∏–ª–∏ None
        """
        prompt_lower = prompt.lower()
        
        # –ò—â–µ–º —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern, token in self._pattern_map.items():
            if pattern in prompt_lower:
                self._usage_stats[token] = self._usage_stats.get(token, 0) + 1
                return token
        
        # –ò—â–µ–º —á–∞—Å—Ç–∏—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è (–µ—Å–ª–∏ –ø–∞—Ç—Ç–µ—Ä–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞)
        for token, instruction in self._token_map.items():
            instruction_words = set(instruction.lower().split())
            prompt_words = set(prompt_lower.split())
            
            # –ï—Å–ª–∏ >50% —Å–ª–æ–≤ —Å–æ–≤–ø–∞–¥–∞—é—Ç, —Å—á–∏—Ç–∞–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º
            common_words = instruction_words.intersection(prompt_words)
            if len(instruction_words) > 0:
                similarity = len(common_words) / len(instruction_words)
                if similarity >= 0.5:
                    self._usage_stats[token] = self._usage_stats.get(token, 0) + 1
                    return token
        
        return None
    
    def replace_with_token(self, prompt: str) -> Tuple[str, Optional[str]]:
        """
        –ó–∞–º–µ–Ω—è–µ—Ç –ø—Ä–æ–º–ø—Ç –Ω–∞ BE-Token, –µ—Å–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â–∏–π –Ω–∞–π–¥–µ–Ω.
        
        Args:
            prompt: –ò—Å—Ö–æ–¥–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        
        Returns:
            (—Å–∂–∞—Ç—ã–π_–ø—Ä–æ–º–ø—Ç, —Ç–æ–∫–µ–Ω_–∏–ª–∏_None)
        """
        token = self.find_token(prompt)
        if token:
            # –ó–∞–º–µ–Ω—è–µ–º –¥–ª–∏–Ω–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –Ω–∞ –∫–æ—Ä–æ—Ç–∫–∏–π —Ç–æ–∫–µ–Ω
            compressed = f"[BE:{token}] {prompt}"  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            return compressed, token
        return prompt, None
    
    def expand_token(self, token: str) -> Optional[str]:
        """
        –†–∞–∑–≤–æ—Ä–∞—á–∏–≤–∞–µ—Ç BE-Token –æ–±—Ä–∞—Ç–Ω–æ –≤ –ø–æ–ª–Ω—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.
        
        Args:
            token: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∞
        
        Returns:
            –ü–æ–ª–Ω–∞—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –∏–ª–∏ None
        """
        return self._token_map.get(token)
    
    def get_usage_stats(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤"""
        return self._usage_stats.copy()
    
    def learn_from_history(self, prompts: List[str], instructions: List[str]):
        """
        –û–±—É—á–µ–Ω–∏–µ BE-Token –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ –∑–∞–ø—Ä–æ—Å–æ–≤.
        –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ—Ç –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤.
        
        Args:
            prompts: –°–ø–∏—Å–æ–∫ –ø—Ä–æ–º–ø—Ç–æ–≤
            instructions: –°–ø–∏—Å–æ–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
        """
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑: –Ω–∞—Ö–æ–¥–∏–º –æ–±—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö
        from collections import Counter
        
        instruction_counts = Counter()
        for instruction in instructions:
            instruction_counts[instruction.lower()] += 1
        
        # –ï—Å–ª–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è >= 3 —Ä–∞–∑–∞, —Å–æ–∑–¥–∞–µ–º —Ç–æ–∫–µ–Ω
        for instruction, count in instruction_counts.items():
            if count >= 3 and len(instruction) > 50:  # –î–ª–∏–Ω–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
                token_name = f"CUSTOM_{count}_{hash(instruction) % 10000}"
                if token_name not in self._token_map:
                    self.register_token(token_name, instruction)
                    logger.info(f"üìù [BE-TOKEN] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω —Ç–æ–∫–µ–Ω: {token_name}")

# Singleton instance –¥–ª—è BETokenManager
_betoken_manager = None

def get_betoken_manager() -> BETokenManager:
    """–ü–æ–ª—É—á–∞–µ—Ç singleton instance BETokenManager"""
    global _betoken_manager
    if _betoken_manager is None:
        _betoken_manager = BETokenManager()
    return _betoken_manager

# –≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
__all__ = [
    'PromptOptimizer',
    'FrugalPrompt',
    'BETokenManager',
    'get_betoken_manager',
    'BatchProcessor',
    'PredictiveCache',
    'ResponseStreamer',
    'SmartRouter',
    'EmbeddingCache',
    'ParallelProcessor'
]

