"""
Adaptive Agent - –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç —Å RL
–ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø–æ–≤–µ–¥–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ feedback
"""

import os
import asyncio
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone

from app.reinforcement_learning import get_rl, ReinforcementLearning
from app.human_in_the_loop import get_hitl

logger = logging.getLogger(__name__)


class AdaptiveAgent:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç —Å reinforcement learning"""
    
    def __init__(self, agent_name: str = "–í–∏–∫—Ç–æ—Ä–∏—è"):
        self.agent_name = agent_name
        self.rl = get_rl(agent_name)
        self.hitl = get_hitl()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        self.adaptation_history: List[Dict[str, Any]] = []
        self.performance_metrics: Dict[str, float] = {
            "success_rate": 0.0,
            "average_quality": 0.0,
            "efficiency": 0.0
        }
    
    async def adapt_from_feedback(
        self,
        action_id: str,
        feedback_type: str,
        feedback_value: float
    ) -> Dict[str, Any]:
        """
        –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ feedback
        
        Args:
            action_id: ID –¥–µ–π—Å—Ç–≤–∏—è
            feedback_type: –¢–∏–ø —Ñ–∏–¥–±–µ–∫–∞
            feedback_value: –ó–Ω–∞—á–µ–Ω–∏–µ —Ñ–∏–¥–±–µ–∫–∞
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º feedback –≤ reward
        reward_value = feedback_value if -1.0 <= feedback_value <= 1.0 else 0.0
        
        # –ù–∞–∑–Ω–∞—á–∞–µ–º reward
        reward = await self.rl.assign_reward(
            action_id=action_id,
            reward_value=reward_value,
            reward_type=feedback_type
        )
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ–º
        adaptation = {
            "action_id": action_id,
            "reward": reward_value,
            "adaptation_type": "feedback_based",
            "timestamp": datetime.now(timezone.utc)
        }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        await self._update_metrics(reward_value)
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º exploration rate
        if reward_value > 0.7:
            # –£—Å–ø–µ—à–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ - —É–º–µ–Ω—å—à–∞–µ–º exploration
            self.rl.exploration_rate = max(0.05, self.rl.exploration_rate * 0.95)
        elif reward_value < -0.3:
            # –ù–µ—É–¥–∞—á–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º exploration
            self.rl.exploration_rate = min(0.3, self.rl.exploration_rate * 1.1)
        
        self.adaptation_history.append(adaptation)
        
        logger.info(f"üîÑ –ê–¥–∞–ø—Ç–∞—Ü–∏—è: exploration_rate = {self.rl.exploration_rate:.3f}")
        
        return adaptation
    
    async def adapt_from_result(
        self,
        action_id: str,
        result: Any,
        expected_result: Optional[Any] = None
    ) -> Dict[str, Any]:
        """
        –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–µ–π—Å—Ç–≤–∏—è
        
        Args:
            action_id: ID –¥–µ–π—Å—Ç–≤–∏—è
            result: –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            expected_result: –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        """
        # –°–∞–º–æ–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ
        reward = await self.rl.self_reward(action_id, result, expected_result)
        
        # –ê–¥–∞–ø—Ç–∞—Ü–∏—è
        adaptation = {
            "action_id": action_id,
            "reward": reward.reward_value,
            "adaptation_type": "result_based",
            "timestamp": datetime.now(timezone.utc)
        }
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        await self._update_metrics(reward.reward_value)
        
        self.adaptation_history.append(adaptation)
        
        return adaptation
    
    async def _update_metrics(self, reward_value: float):
        """–û–±–Ω–æ–≤–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        # –û–±–Ω–æ–≤–ª—è–µ–º success rate
        total_rewards = len(self.rl.reward_history)
        if total_rewards > 0:
            positive = sum(1 for r in self.rl.reward_history if r.reward_value > 0)
            self.performance_metrics["success_rate"] = positive / total_rewards
        
        # –û–±–Ω–æ–≤–ª—è–µ–º average quality
        if total_rewards > 0:
            avg_reward = sum(r.reward_value for r in self.rl.reward_history) / total_rewards
            self.performance_metrics["average_quality"] = avg_reward
        
        # –û–±–Ω–æ–≤–ª—è–µ–º efficiency (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        self.performance_metrics["efficiency"] = self.performance_metrics["success_rate"] * 0.7 + \
                                                  self.performance_metrics["average_quality"] * 0.3
    
    async def select_adaptive_action(
        self,
        state: str,
        available_actions: List[str],
        context: Dict[str, Any] = None
    ) -> str:
        """
        –í—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            available_actions: –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç
        
        Returns:
            –í—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        """
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º RL –¥–ª—è –≤—ã–±–æ—Ä–∞
        action = await self.rl.select_action(state, available_actions, context)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–±–æ—Ä
        logger.debug(f"üéØ –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –≤—ã–±–æ—Ä: {action} –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è {state}")
        
        return action
    
    def get_adaptation_summary(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–≤–æ–¥–∫—É –∞–¥–∞–ø—Ç–∞—Ü–∏–∏"""
        stats = self.rl.get_statistics()
        
        return {
            "agent_name": self.agent_name,
            "performance_metrics": self.performance_metrics,
            "rl_statistics": stats,
            "adaptation_count": len(self.adaptation_history),
            "exploration_rate": self.rl.exploration_rate,
            "recent_adaptations": self.adaptation_history[-10:] if self.adaptation_history else []
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã
_adaptive_agents: Dict[str, AdaptiveAgent] = {}

def get_adaptive_agent(agent_name: str = "–í–∏–∫—Ç–æ—Ä–∏—è") -> AdaptiveAgent:
    """–ü–æ–ª—É—á–∏—Ç—å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –∞–≥–µ–Ω—Ç"""
    if agent_name not in _adaptive_agents:
        _adaptive_agents[agent_name] = AdaptiveAgent(agent_name=agent_name)
    return _adaptive_agents[agent_name]
