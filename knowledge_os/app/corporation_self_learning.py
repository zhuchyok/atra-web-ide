"""
–°–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–æ–∫ –∏ –º–µ—Ç—Ä–∏–∫
–î–≤–∏–∂–µ–Ω–∏–µ –∫ Singularity 10.0 - –ø–æ–ª–Ω–æ—Å—Ç—å—é –∞–≤—Ç–æ–Ω–æ–º–Ω–∞—è —Å–∞–º–æ—É–ª—É—á—à–∞—é—â–∞—è—Å—è —Å–∏—Å—Ç–µ–º–∞
"""
import asyncio
import asyncpg
import logging
from typing import Dict, List, Optional
from datetime import datetime, timedelta
import json

logger = logging.getLogger(__name__)

class CorporationSelfLearning:
    """
    –°–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏:
    - –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ—à–∏–±–∫–∏ –∏ –Ω–µ—É–¥–∞—á–∏
    - –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —É–ª—É—á—à–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    - –î–≤–∏–∂–µ—Ç—Å—è –∫ Singularity 10.0
    """
    
    def __init__(self, db_url: str):
        self.db_url = db_url
        self._pool = None
        self._learning_cycles = 0
        self._improvements_applied = 0
    
    async def get_pool(self):
        if self._pool is None:
            self._pool = await asyncpg.create_pool(
                self.db_url,
                min_size=1,
                max_size=3,
                max_inactive_connection_lifetime=300
            )
        return self._pool
    
    async def analyze_errors(self, hours: int = 24) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —á–∞—Å–æ–≤"""
        try:
            pool = await self.get_pool()
            async with pool.acquire() as conn:
                # –ê–Ω–∞–ª–∏–∑ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–¥–∞—á
                failed_tasks = await conn.fetch("""
                    SELECT 
                        COUNT(*) as total_failed,
                        COUNT(DISTINCT assignee_expert_id) as experts_with_failures,
                        AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_time_before_failure
                    FROM tasks
                    WHERE status = 'failed'
                    AND updated_at > NOW() - INTERVAL '%s hours'
                """ % hours)
                
                # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –≤ –ª–æ–≥–∞—Ö –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
                model_failures = await conn.fetch("""
                    SELECT 
                        model_name,
                        COUNT(*) as failure_count,
                        AVG(quality_score) as avg_quality_on_failure
                    FROM model_performance_log
                    WHERE success = false
                    AND created_at > NOW() - INTERVAL '%s hours'
                    GROUP BY model_name
                    ORDER BY failure_count DESC
                """ % hours)
                
                # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫
                error_patterns = await conn.fetch("""
                    SELECT 
                        metadata->>'last_error' as error_type,
                        COUNT(*) as count
                    FROM tasks
                    WHERE metadata->>'last_error' IS NOT NULL
                    AND updated_at > NOW() - INTERVAL '%s hours'
                    GROUP BY metadata->>'last_error'
                    ORDER BY count DESC
                    LIMIT 10
                """ % hours)
                
                return {
                    'failed_tasks': dict(failed_tasks[0]) if failed_tasks else {},
                    'model_failures': [dict(row) for row in model_failures],
                    'error_patterns': [dict(row) for row in error_patterns],
                    'analysis_time': datetime.now().isoformat()
                }
        except Exception as e:
            logger.error(f"Error analyzing errors: {e}")
            return {}
    
    async def analyze_performance_metrics(self, hours: int = 24) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        try:
            pool = await self.get_pool()
            async with pool.acquire() as conn:
                # –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π
                model_metrics = await conn.fetch("""
                    SELECT 
                        model_name,
                        COUNT(*) as total_attempts,
                        COUNT(*) FILTER (WHERE success = true)::float / COUNT(*) as success_rate,
                        AVG(quality_score) as avg_quality,
                        AVG(latency_ms) as avg_latency,
                        AVG(response_length) as avg_response_length
                    FROM model_performance_log
                    WHERE created_at > NOW() - INTERVAL '%s hours'
                    GROUP BY model_name
                """ % hours)
                
                # –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–¥–∞—á
                task_metrics = await conn.fetch("""
                    SELECT 
                        status,
                        COUNT(*) as count,
                        AVG(EXTRACT(EPOCH FROM (updated_at - created_at))) as avg_processing_time
                    FROM tasks
                    WHERE updated_at > NOW() - INTERVAL '%s hours'
                    GROUP BY status
                """ % hours)
                
                # –ú–µ—Ç—Ä–∏–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
                expert_metrics = await conn.fetch("""
                    SELECT 
                        e.name as expert_name,
                        COUNT(t.id) as tasks_assigned,
                        COUNT(*) FILTER (WHERE t.status = 'completed') as completed,
                        COUNT(*) FILTER (WHERE t.status = 'failed') as failed,
                        AVG(EXTRACT(EPOCH FROM (t.updated_at - t.created_at))) as avg_time
                    FROM tasks t
                    JOIN experts e ON t.assignee_expert_id = e.id
                    WHERE t.updated_at > NOW() - INTERVAL '%s hours'
                    GROUP BY e.name
                    ORDER BY tasks_assigned DESC
                    LIMIT 20
                """ % hours)
                
                return {
                    'model_metrics': [dict(row) for row in model_metrics],
                    'task_metrics': [dict(row) for row in task_metrics],
                    'expert_metrics': [dict(row) for row in expert_metrics],
                    'analysis_time': datetime.now().isoformat()
                }
        except Exception as e:
            logger.error(f"Error analyzing performance: {e}")
            return {}
    
    async def generate_improvements(self, error_analysis: Dict, performance_analysis: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        improvements = []
        
        # 1. –£–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–µ–π
        if error_analysis.get('model_failures'):
            for model_failure in error_analysis['model_failures']:
                model_name = model_failure['model_name']
                failure_count = model_failure['failure_count']
                
                if failure_count > 10:
                    improvements.append({
                        'type': 'model_downgrade',
                        'model': model_name,
                        'reason': f'–í—ã—Å–æ–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫: {failure_count}',
                        'action': '–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –∑–∞–¥–∞—á',
                        'priority': 'high'
                    })
        
        # 2. –£–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if performance_analysis.get('model_metrics'):
            for model_metric in performance_analysis['model_metrics']:
                model_name = model_metric['model_name']
                success_rate = model_metric['success_rate']
                avg_quality = model_metric['avg_quality']
                
                if success_rate < 0.7:
                    improvements.append({
                        'type': 'model_optimization',
                        'model': model_name,
                        'reason': f'–ù–∏–∑–∫–∏–π success_rate: {success_rate:.2%}',
                        'action': '–£–ª—É—á—à–∏—Ç—å –≤—ã–±–æ—Ä –∑–∞–¥–∞—á –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏',
                        'priority': 'medium'
                    })
                
                if avg_quality < 0.6:
                    improvements.append({
                        'type': 'quality_improvement',
                        'model': model_name,
                        'reason': f'–ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ: {avg_quality:.2f}',
                        'action': '–ü–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á',
                        'priority': 'high'
                    })
        
        # 3. –£–ª—É—á—à–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –æ—à–∏–±–æ–∫
        if error_analysis.get('error_patterns'):
            for pattern in error_analysis['error_patterns'][:3]:  # –¢–æ–ø-3 –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                error_type = pattern['error_type']
                count = pattern['count']
                
                if count > 5:
                    improvements.append({
                        'type': 'error_pattern_fix',
                        'pattern': error_type[:100],  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                        'reason': f'–ü–æ–≤—Ç–æ—Ä—è—é—â–∞—è—Å—è –æ—à–∏–±–∫–∞: {count} —Ä–∞–∑',
                        'action': '–î–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–ª—è —ç—Ç–æ–≥–æ —Ç–∏–ø–∞ –æ—à–∏–±–æ–∫',
                        'priority': 'high'
                    })
        
        return improvements
    
    async def apply_improvements(self, improvements: List[Dict]) -> Dict:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π"""
        applied = []
        failed = []
        
        for improvement in improvements:
            try:
                if improvement['type'] == 'model_downgrade':
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—é –º–æ–¥–µ–ª–µ–π
                    logger.info(f"üîÑ [LEARNING] –ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–∏–µ: {improvement['action']}")
                    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å MODEL_HIERARCHY –≤ intelligent_model_router
                    applied.append(improvement)
                
                elif improvement['type'] == 'model_optimization':
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
                    logger.info(f"üîß [LEARNING] –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å: {improvement['model']}")
                    applied.append(improvement)
                
                elif improvement['type'] == 'error_pattern_fix':
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    logger.info(f"üõ†Ô∏è [LEARNING] –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –æ—à–∏–±–æ–∫: {improvement['pattern']}")
                    applied.append(improvement)
                
                self._improvements_applied += 1
                
            except Exception as e:
                logger.error(f"Error applying improvement: {e}")
                failed.append(improvement)
        
        return {
            'applied': applied,
            'failed': failed,
            'total': len(improvements)
        }
    
    async def run_learning_cycle(self):
        """–ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è"""
        self._learning_cycles += 1
        logger.info(f"üîÑ [SELF-LEARNING] –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è #{self._learning_cycles}")
        
        try:
            # 1. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
            error_analysis = await self.analyze_errors(hours=24)
            logger.info(f"üìä [LEARNING] –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –æ—à–∏–±–æ–∫: {len(error_analysis.get('error_patterns', []))}")
            
            # 2. –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
            performance_analysis = await self.analyze_performance_metrics(hours=24)
            logger.info(f"üìà [LEARNING] –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(performance_analysis.get('model_metrics', []))}")
            
            # 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–∏–π
            improvements = await self.generate_improvements(error_analysis, performance_analysis)
            logger.info(f"üí° [LEARNING] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ —É–ª—É—á—à–µ–Ω–∏–π: {len(improvements)}")
            
            # 4. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π
            if improvements:
                result = await self.apply_improvements(improvements)
                logger.info(f"‚úÖ [LEARNING] –ü—Ä–∏–º–µ–Ω–µ–Ω–æ —É–ª—É—á—à–µ–Ω–∏–π: {len(result['applied'])}/{result['total']}")
            
            # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            await self.save_learning_results(error_analysis, performance_analysis, improvements)
            
            logger.info(f"‚úÖ [SELF-LEARNING] –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è #{self._learning_cycles} –∑–∞–≤–µ—Ä—à–µ–Ω")
            
        except Exception as e:
            logger.error(f"‚ùå [SELF-LEARNING] –û—à–∏–±–∫–∞ –≤ —Ü–∏–∫–ª–µ –æ–±—É—á–µ–Ω–∏—è: {e}")
            import traceback
            traceback.print_exc()
    
    async def save_learning_results(self, error_analysis: Dict, performance_analysis: Dict, improvements: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è"""
        try:
            pool = await self.get_pool()
            async with pool.acquire() as conn:
                await conn.execute("""
                    INSERT INTO corporation_learning_log (
                        cycle_number,
                        error_analysis,
                        performance_analysis,
                        improvements,
                        applied_count,
                        created_at
                    ) VALUES ($1, $2, $3, $4, $5, NOW())
                """, 
                    self._learning_cycles,
                    json.dumps(error_analysis),
                    json.dumps(performance_analysis),
                    json.dumps(improvements),
                    self._improvements_applied
                )
        except Exception as e:
            logger.debug(f"Error saving learning results: {e}")
    
    async def start_continuous_learning(self, interval_hours: int = 6):
        """–ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
        logger.info(f"üöÄ [SELF-LEARNING] –ó–∞–ø—É—Å–∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–∏–Ω—Ç–µ—Ä–≤–∞–ª: {interval_hours} —á–∞—Å–æ–≤)")
        
        while True:
            try:
                await self.run_learning_cycle()
                await asyncio.sleep(interval_hours * 3600)
            except Exception as e:
                logger.error(f"‚ùå [SELF-LEARNING] –û—à–∏–±–∫–∞ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏: {e}")
                await asyncio.sleep(3600)  # –ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ —á–∞—Å –ø—Ä–∏ –æ—à–∏–±–∫–µ

# Singleton
_learning_instance = None

def get_corporation_learner(db_url: str = None) -> CorporationSelfLearning:
    global _learning_instance
    if _learning_instance is None:
        import os
        db_url = db_url or os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
        _learning_instance = CorporationSelfLearning(db_url)
    return _learning_instance
