#!/usr/bin/env python3
"""
üïµÔ∏è –£–õ–£–ß–®–ï–ù–ù–´–ô –ú–û–î–£–õ–¨ –ö–û–ù–ö–£–†–ï–ù–¢–ù–û–ô –†–ê–ó–í–ï–î–ö–ò (–ì–õ–ï–ë ENHANCED)
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–∑–≤–µ–¥–∫–∞ —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ + –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –ø–æ –º–∏—Ä–æ–≤—ã–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞:
- Competitive Intelligence Best Practices 2025
- OSINT frameworks
- Multi-source data collection
- Structured analysis frameworks (SWOT, Porter's Five Forces, PEST)
"""

import asyncio
import os
import json
import sys
import logging
import re
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from collections import defaultdict
from dataclasses import dataclass, asdict

import asyncpg  # type: ignore # pylint: disable=import-error
import httpx
from duckduckgo_search import DDGS  # type: ignore # pylint: disable=import-error

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

DB_URL = os.getenv("DATABASE_URL", "postgresql://admin:secret@localhost:5432/knowledge_os")
VECTOR_CORE_URL = os.getenv("VECTOR_CORE_URL", "http://knowledge_vector_core:8001")
MLX_API_URL = os.getenv("MLX_API_URL", "http://host.docker.internal:11435")
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://host.docker.internal:11434")

@dataclass
class CompetitorInfo:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–µ"""
    name: str
    sources: List[str]  # URL –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    mentions: int = 0
    sentiment: str = "neutral"  # positive, negative, neutral
    pricing_info: Optional[str] = None
    services: List[str] = None
    reviews_count: int = 0
    avg_rating: float = 0.0
    location: Optional[str] = None
    contact_info: Optional[str] = None
    strengths: List[str] = None
    weaknesses: List[str] = None
    
    def __post_init__(self):
        if self.services is None:
            self.services = []
        if self.strengths is None:
            self.strengths = []
        if self.weaknesses is None:
            self.weaknesses = []


class EnhancedScoutResearcher:
    """
    –£–ª—É—á—à–µ–Ω–Ω—ã–π —Ä–∞–∑–≤–µ–¥—á–∏–∫ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∏ –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º.
    """
    
    def __init__(self):
        self.competitors: Dict[str, CompetitorInfo] = {}
        self.market_insights: List[Dict] = []
        self.pricing_data: List[Dict] = []
        self.review_sentiments: Dict[str, List[str]] = defaultdict(list)
        
    async def get_embedding(self, text: str) -> List[float]:
        """–ü–æ–ª—É—á–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ VectorCore."""
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    f"{VECTOR_CORE_URL}/encode", 
                    json={"text": text}, 
                    timeout=30.0
                )
                response.raise_for_status()
                return response.json()["embedding"]
            except (httpx.HTTPError, KeyError, ValueError) as e:
                logger.error("VectorCore error: %s", e)
                return [0.0] * 768  # nomic-embed-text; knowledge_nodes.embedding vector(768)
    
    async def search_multiple_sources(self, query: str, max_results: int = 15) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º (–º–∏—Ä–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏).
        """
        all_results = []
        
        # 1. DuckDuckGo (–æ—Å–Ω–æ–≤–Ω–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫)
        try:
            with DDGS() as ddgs:
                ddg_results = list(ddgs.text(query, max_results=max_results))
                logger.debug(f"üîç DuckDuckGo –¥–ª—è '{query}': –Ω–∞–π–¥–µ–Ω–æ {len(ddg_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                for res in ddg_results:
                    all_results.append({
                        "title": res.get('title', ''),
                        "url": res.get('href', ''),
                        "snippet": res.get('body', ''),
                        "source": "duckduckgo",
                        "query": query
                    })
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ DuckDuckGo –ø–æ–∏—Å–∫–∞ –¥–ª—è '{query}': {e}")
            import traceback
            logger.debug(traceback.format_exc())
        
        # 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏
        query_variations = [
            f"{query} –æ—Ç–∑—ã–≤—ã",
            f"{query} —Ü–µ–Ω—ã",
            f"{query} —Ä–µ–π—Ç–∏–Ω–≥",
            f"{query} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç",
            f"{query} –∫–æ–Ω—Ç–∞–∫—Ç—ã"
        ]
        
        for var_query in query_variations[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            try:
                with DDGS() as ddgs:
                    var_results = list(ddgs.text(var_query, max_results=5))
                    logger.debug(f"üîç –í–∞—Ä–∏–∞—Ü–∏—è '{var_query}': –Ω–∞–π–¥–µ–Ω–æ {len(var_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    for res in var_results:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ URL
                        if not any(r['url'] == res.get('href', '') for r in all_results):
                            all_results.append({
                                "title": res.get('title', ''),
                                "url": res.get('href', ''),
                                "snippet": res.get('body', ''),
                                "source": "duckduckgo_variation",
                                "query": var_query
                            })
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤–∞—Ä–∏–∞—Ü–∏–∏ –ø–æ–∏—Å–∫–∞ '{var_query}': {e}")
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ –¥–ª—è '{query}': —Å–æ–±—Ä–∞–Ω–æ {len(all_results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        return all_results
    
    def extract_competitor_info(self, results: List[Dict], business_name: str, location: str) -> Dict[str, CompetitorInfo]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞.
        """
        competitors = {}
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        competitor_patterns = [
            r'–∫–æ–º–ø–∞–Ω–∏—è\s+([–ê-–Ø–Å][–ê-–Ø–∞-—è—ë\s]+)',
            r'—Ñ–∏—Ä–º–∞\s+([–ê-–Ø–Å][–ê-–Ø–∞-—è—ë\s]+)',
            r'([–ê-–Ø–Å][–ê-–Ø–∞-—è—ë\s]+–û–∫–Ω–∞)',
            r'([–ê-–Ø–Å][–ê-–Ø–∞-—è—ë\s]+–û—Å—Ç–µ–∫–ª–µ–Ω–∏–µ)',
        ]
        
        price_pattern = r'(\d+[\s,.]?\d*)\s*(—Ä—É–±|‚ÇΩ|—Ä—É–±–ª–µ–π|—Ä—É–±\.)'
        phone_pattern = r'(\+?7|8)?[\s\-\(]?(\d{3})[\s\-\)]?[\s\-]?(\d{3})[\s\-]?(\d{2})[\s\-]?(\d{2})'
        rating_pattern = r'(\d+[.,]\d+)\s*(–∑–≤–µ–∑–¥|‚òÖ|‚≠ê|–±–∞–ª–ª)'
        
        for result in results:
            text = f"{result['title']} {result['snippet']}".lower()
            
            # –ò—â–µ–º –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
            for pattern in competitor_patterns:
                matches = re.findall(pattern, result['title'] + ' ' + result['snippet'], re.IGNORECASE)
                for match in matches:
                    comp_name = match.strip() if isinstance(match, str) else match[0].strip()
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ü–µ–ª–µ–≤–æ–π –±–∏–∑–Ω–µ—Å
                    if business_name.lower() in comp_name.lower() or comp_name.lower() in business_name.lower():
                        continue
                    
                    if comp_name not in competitors:
                        competitors[comp_name] = CompetitorInfo(
                            name=comp_name,
                            sources=[],
                            location=location
                        )
                    
                    comp = competitors[comp_name]
                    comp.mentions += 1
                    if result['url'] not in comp.sources:
                        comp.sources.append(result['url'])
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ü–µ–Ω—É
                    price_match = re.search(price_pattern, result['snippet'], re.IGNORECASE)
                    if price_match and not comp.pricing_info:
                        comp.pricing_info = price_match.group(0)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã
                    phone_match = re.search(phone_pattern, result['snippet'])
                    if phone_match and not comp.contact_info:
                        comp.contact_info = phone_match.group(0)
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–π—Ç–∏–Ω–≥
                    rating_match = re.search(rating_pattern, result['snippet'])
                    if rating_match:
                        try:
                            rating = float(rating_match.group(1).replace(',', '.'))
                            if comp.avg_rating == 0.0:
                                comp.avg_rating = rating
                            else:
                                comp.avg_rating = (comp.avg_rating + rating) / 2
                            comp.reviews_count += 1
                        except (ValueError, AttributeError, IndexError):
                            pass
                    
                    # –ê–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ (–ø—Ä–æ—Å—Ç–æ–π)
                    negative_words = ['–ø–ª–æ—Ö–æ', '–Ω–µ–¥–æ–≤–æ–ª–µ–Ω', '–ø—Ä–æ–±–ª–µ–º–∞', '–æ—à–∏–±–∫–∞', '–∂–∞–ª–æ–±–∞']
                    positive_words = ['–æ—Ç–ª–∏—á–Ω–æ', '—Ä–µ–∫–æ–º–µ–Ω–¥—É—é', '–¥–æ–≤–æ–ª–µ–Ω', '–∫–∞—á–µ—Å—Ç–≤–æ', '—Ö–æ—Ä–æ—à–æ']
                    
                    if any(word in text for word in negative_words):
                        comp.sentiment = "negative"
                        comp.weaknesses.append(result['snippet'][:100])
                    elif any(word in text for word in positive_words):
                        comp.sentiment = "positive"
                        comp.strengths.append(result['snippet'][:100])
        
        return competitors
    
    async def deep_analysis_with_llm(self, data_summary: str, business_name: str, location: str) -> Dict[str, Any]:
        """
        –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∏—Ä–æ–≤—ã—Ö —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–æ–≤.
        """
        analysis_prompt = f"""–¢–´ - –≠–ö–°–ü–ï–†–¢ –ü–û –ö–û–ù–ö–£–†–ï–ù–¢–ù–û–ô –†–ê–ó–í–ï–î–ö–ï. –ü—Ä–æ–≤–µ–¥–∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ä—ã–Ω–∫–∞.

–¶–ï–õ–ï–í–û–ô –ë–ò–ó–ù–ï–°: {business_name}
–õ–û–ö–ê–¶–ò–Ø: {location}

–°–û–ë–†–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï:
{data_summary}

–ó–ê–î–ê–ß–ê: –°–æ–∑–¥–∞–π –î–ï–¢–ê–õ–¨–ù–´–ô –æ—Ç—á–µ—Ç –ø–æ —Å–ª–µ–¥—É—é—â–∏–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞–º:

1. SWOT-–ê–ù–ê–õ–ò–ó (Strengths, Weaknesses, Opportunities, Threats)
   - –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä—ã–Ω–∫–∞
   - –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã —Ä—ã–Ω–∫–∞
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –¥–ª—è —Ä–æ—Å—Ç–∞
   - –£–≥—Ä–æ–∑—ã –¥–ª—è –±–∏–∑–Ω–µ—Å–∞

2. PORTER'S FIVE FORCES
   - –£–≥—Ä–æ–∑–∞ –Ω–æ–≤—ã—Ö –∏–≥—Ä–æ–∫–æ–≤
   - –£–≥—Ä–æ–∑–∞ —Ç–æ–≤–∞—Ä–æ–≤-–∑–∞–º–µ–Ω–∏—Ç–µ–ª–µ–π
   - –†—ã–Ω–æ—á–Ω–∞—è –≤–ª–∞—Å—Ç—å –ø–æ—Å—Ç–∞–≤—â–∏–∫–æ–≤
   - –†—ã–Ω–æ—á–Ω–∞—è –≤–ª–∞—Å—Ç—å –ø–æ–∫—É–ø–∞—Ç–µ–ª–µ–π
   - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ü–∏—è –≤ –æ—Ç—Ä–∞—Å–ª–∏

3. PEST-–ê–ù–ê–õ–ò–ó (Political, Economic, Social, Technological)
   - –ü–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
   - –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
   - –°–æ—Ü–∏–∞–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
   - –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã

4. –ö–û–ù–ö–£–†–ï–ù–¢–ù–ê–Ø –ö–ê–†–¢–ê
   - –¢–û–ü-10 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–æ–π
   - –ü–æ–∑–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ
   - –†—ã–Ω–æ—á–Ω–∞—è –¥–æ–ª—è (–æ—Ü–µ–Ω–∫–∞)
   - –ö–ª—é—á–µ–≤—ã–µ –æ—Ç–ª–∏—á–∏—è

5. –ê–ù–ê–õ–ò–ó –¶–ï–ù–û–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø
   - –î–∏–∞–ø–∞–∑–æ–Ω —Ü–µ–Ω –Ω–∞ —Ä—ã–Ω–∫–µ
   - –§–∞–∫—Ç–æ—Ä—ã —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—é

6. –ê–ù–ê–õ–ò–ó –û–¢–ó–´–í–û–í –ò –°–ï–ù–¢–ò–ú–ï–ù–¢–ê
   - –û—Å–Ω–æ–≤–Ω—ã–µ –±–æ–ª–∏ –∫–ª–∏–µ–Ω—Ç–æ–≤
   - –ß—Ç–æ —Ü–µ–Ω—è—Ç –∫–ª–∏–µ–Ω—Ç—ã
   - –ù–µ—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–Ω—ã–µ –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç–∏

7. –°–¢–†–ê–¢–ï–ì–ò–ß–ï–°–ö–ò–ï –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò
   - –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (1-3 –º–µ—Å—è—Ü–∞)
   - –°—Ä–µ–¥–Ω–µ—Å—Ä–æ—á–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (3-12 –º–µ—Å—è—Ü–µ–≤)
   - –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è (1-3 –≥–æ–¥–∞)
   - –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —É—Å–ø–µ—Ö–∞ (KPI)

8. –†–ò–°–ö–ò –ò –ú–ò–¢–ò–ì–ê–¶–ò–Ø
   - –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∏—Å–∫–∏
   - –ü–ª–∞–Ω –º–∏—Ç–∏–≥–∞—Ü–∏–∏ —Ä–∏—Å–∫–æ–≤

–§–û–†–ú–ê–¢: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏, —Ü–∏—Ñ—Ä–∞–º–∏, –ø—Ä–∏–º–µ—Ä–∞–º–∏.
–ò—Å–ø–æ–ª—å–∑—É–π –º–∞—Ä–∫–¥–∞—É–Ω –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è."""

        # –ü—Ä–æ–±—É–µ–º —Å–Ω–∞—á–∞–ª–∞ MLX (–±–æ–ª–µ–µ –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å)
        models_to_try = [
            (MLX_API_URL, "deepseek-r1-distill-llama:70b", "MLX"),
            (MLX_API_URL, "qwen2.5-coder:32b", "MLX"),
            (OLLAMA_URL, "glm-4.7-flash:q8_0", "Ollama"),
            (OLLAMA_URL, "phi3.5:3.8b", "Ollama"),
        ]
        
        last_error = None
        for api_url, model_name, source in models_to_try:
            try:
                async with httpx.AsyncClient(timeout=180.0) as client:
                    logger.info(f"üß† –ü—Ä–æ–±—É—é {source} –º–æ–¥–µ–ª—å {model_name} –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
                    response = await client.post(
                        f"{api_url}/api/generate",
                        json={
                            "model": model_name,
                            "prompt": analysis_prompt,
                            "stream": False,
                            "options": {
                                "temperature": 0.7,
                                "top_p": 0.9,
                                "num_predict": 4000 if "deepseek" in model_name or "glm" in model_name else 2000
                            }
                        }
                    )
                    if response.status_code == 200:
                        result = response.json()
                        analysis = result.get('response', '')
                        if analysis and len(analysis.strip()) > 100:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ —Ä–µ–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                            logger.info(f"‚úÖ –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —á–µ—Ä–µ–∑ {source} ({len(analysis)} —Å–∏–º–≤–æ–ª–æ–≤)")
                            return {
                                "analysis": analysis,
                                "model_used": model_name,
                                "timestamp": datetime.now(timezone.utc).isoformat()
                            }
                        else:
                            logger.warning(f"‚ö†Ô∏è {source} {model_name} –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                            continue
                    else:
                        logger.warning(f"‚ö†Ô∏è {source} {model_name} –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status_code}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                        continue
            except httpx.TimeoutException as e:
                last_error = f"Timeout –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ {source} {model_name}: {e}"
                logger.warning(f"‚è±Ô∏è {last_error}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                continue
            except httpx.ConnectError as e:
                last_error = f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ {source} {model_name}: {e}"
                logger.warning(f"üîå {last_error}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                continue
            except Exception as e:
                last_error = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ {source} {model_name}: {e}"
                logger.warning(f"‚ùå {last_error}, –ø—Ä–æ–±—É—é —Å–ª–µ–¥—É—é—â—É—é –º–æ–¥–µ–ª—å...")
                continue
        
        # –ï—Å–ª–∏ –≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
        logger.error(f"‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
        return {
            "analysis": f"‚ö†Ô∏è –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ –æ—Ç–≤–µ—á–∞—é—Ç).\n\n–ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}\n\n**–°–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:**\n{data_summary[:2000]}",
            "model_used": "N/A (–≤—Å–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã)",
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    async def perform_enhanced_research(
        self, 
        business_name: str, 
        locations: str,
        extra_competitors: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—É—é —Ä–∞–∑–≤–µ–¥–∫—É —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.
        """
        logger.info(f"üïµÔ∏è –ì–ª–µ–± Enhanced: –ù–∞—á–∏–Ω–∞—é –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Ä–∞–∑–≤–µ–¥–∫—É –¥–ª—è '{business_name}' –≤ {locations}...")
        
        pool = await asyncpg.create_pool(DB_URL, min_size=1, max_size=5)
        
        # üåü –ú–ò–†–û–í–´–ï –ü–†–ê–ö–¢–ò–ö–ò: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–∏–ø—ã –∑–∞–ø—Ä–æ—Å–æ–≤
        query_categories = {
            "competitors": [
                f"–∫–æ–º–ø–∞–Ω–∏–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã—Ö –æ–∫–æ–Ω {locations} 2025",
                f"–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã {business_name} {locations}",
                f"–æ–∫–æ–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ {locations} —Å–ø–∏—Å–æ–∫",
                f"–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–∏ –æ–∫–æ–Ω –ü–í–• {locations}",
            ],
            "pricing": [
                f"—Ü–µ–Ω—ã –Ω–∞ –ø–ª–∞—Å—Ç–∏–∫–æ–≤—ã–µ –æ–∫–Ω–∞ {locations} 2025",
                f"—Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–∫–æ–Ω –ü–í–• {locations}",
                f"–ø—Ä–∞–π—Å –ª–∏—Å—Ç –æ–∫–Ω–∞ {locations}",
            ],
            "reviews": [
                f"–æ—Ç–∑—ã–≤—ã –æ –∫–æ–º–ø–∞–Ω–∏—è—Ö –ø–æ –æ–∫–Ω–∞–º {locations}",
                f"—Ä–µ–π—Ç–∏–Ω–≥ –æ–∫–æ–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π {locations}",
                f"–∂–∞–ª–æ–±—ã –Ω–∞ —É—Å—Ç–∞–Ω–æ–≤–∫—É –æ–∫–æ–Ω {locations}",
            ],
            "services": [
                f"—É—Å–ª—É–≥–∏ –ø–æ —É—Å—Ç–∞–Ω–æ–≤–∫–µ –æ–∫–æ–Ω {locations}",
                f"–æ—Å—Ç–µ–∫–ª–µ–Ω–∏–µ –±–∞–ª–∫–æ–Ω–æ–≤ {locations}",
                f"—Ä–µ–º–æ–Ω—Ç –æ–∫–æ–Ω {locations}",
            ],
            "market_trends": [
                f"—Ç—Ä–µ–Ω–¥—ã —Ä—ã–Ω–∫–∞ –æ–∫–æ–Ω {locations} 2025",
                f"—Ä–∞–∑–≤–∏—Ç–∏–µ –æ–∫–æ–Ω–Ω–æ–≥–æ —Ä—ã–Ω–∫–∞ {locations}",
            ]
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤
        if extra_competitors:
            for comp in extra_competitors:
                query_categories["competitors"].append(f"{comp} {locations} –æ—Ç–∑—ã–≤—ã —Ü–µ–Ω—ã")
        
        all_results = []
        total_queries = sum(len(queries) for queries in query_categories.values())
        completed_queries = 0
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ –≤—Å–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        async def search_category(category: str, queries: List[str]):
            nonlocal completed_queries
            category_results = []
            for query in queries:
                try:
                    results = await self.search_multiple_sources(query, max_results=10)
                    for res in results:
                        res['category'] = category
                        category_results.append(res)
                    completed_queries += 1
                    logger.info(f"‚úÖ [{completed_queries}/{total_queries}] {category}: {query}")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ '{query}': {e}")
            return category_results
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫
        tasks = [
            search_category(cat, queries) 
            for cat, queries in query_categories.items()
        ]
        category_results_list = await asyncio.gather(*tasks)
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        for category_results in category_results_list:
            all_results.extend(category_results)
        
        logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        if len(all_results) == 0:
            logger.warning(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ü–æ–∏—Å–∫ –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è '{business_name}' –≤ {locations}. –í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
            logger.warning("   - –ü—Ä–æ–±–ª–µ–º—ã —Å DuckDuckGo API")
            logger.warning("   - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã")
            logger.warning("   - –°–ª–∏—à–∫–æ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω–∞—è –ª–æ–∫–∞—Ü–∏—è/–±–∏–∑–Ω–µ—Å")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        competitors = self.extract_competitor_info(all_results, business_name, locations)
        logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(competitors)} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤")
        
        if len(competitors) == 0 and len(all_results) > 0:
            logger.warning(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –ù–∞–π–¥–µ–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –Ω–æ –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤.")
            logger.warning("   - –í–æ–∑–º–æ–∂–Ω–æ, –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞ –±–∏–∑–Ω–µ—Å–∞")
            logger.warning("   - –ò–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î
        async with pool.acquire() as conn:
            expert = await conn.fetchrow("SELECT id, name FROM experts WHERE name = '–ì–ª–µ–±'")
            if not expert:
                logger.error("‚ùå –≠–∫—Å–ø–µ—Ä—Ç –ì–ª–µ–± –Ω–µ –Ω–∞–π–¥–µ–Ω")
                await pool.close()
                return {}
            
            domain_id = await conn.fetchval("SELECT id FROM domains WHERE name = 'Competitive Intelligence'")
            if not domain_id:
                domain_id = await conn.fetchval(
                    "INSERT INTO domains (name) VALUES ('Competitive Intelligence') RETURNING id"
                )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            total_saved = 0
            for result in all_results:
                content = f"{result['title']}\n–ò—Å—Ç–æ—á–Ω–∏–∫: {result['url']}\n{result['snippet']}"
                embedding = await self.get_embedding(content)
                
                metadata = {
                    "source": "enhanced_scout_research",
                    "category": result.get('category', 'general'),
                    "query": result.get('query', ''),
                    "expert_id": str(expert['id']),
                    "expert_name": expert['name'],
                    "url": result['url'],
                    "business_target": business_name,
                    "location": locations,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                
                await conn.execute("""
                    INSERT INTO knowledge_nodes (domain_id, content, embedding, confidence_score, metadata, is_verified)
                    VALUES ($1, $2, $3, 0.95, $4, FALSE)
                """, domain_id, content, str(embedding), json.dumps(metadata))
                total_saved += 1
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            data_summary = f"""
–ö–û–õ–ò–ß–ï–°–¢–í–û –ò–°–¢–û–ß–ù–ò–ö–û–í: {len(all_results)}
–ö–û–õ–ò–ß–ï–°–¢–í–û –ö–û–ù–ö–£–†–ï–ù–¢–û–í: {len(competitors)}

–¢–û–ü –ö–û–ù–ö–£–†–ï–ù–¢–´:
{chr(10).join([f"- {name}: {info.mentions} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π, —Ä–µ–π—Ç–∏–Ω–≥ {info.avg_rating:.1f}, —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å {info.sentiment}" 
                for name, info in sorted(competitors.items(), key=lambda x: x[1].mentions, reverse=True)[:10]])}

–î–ï–¢–ê–õ–ò –ö–û–ù–ö–£–†–ï–ù–¢–û–í:
{json.dumps({name: asdict(info) for name, info in list(competitors.items())[:5]}, ensure_ascii=False, indent=2)}

–ü–†–ò–ú–ï–†–´ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:
{chr(10).join([f"- {r['title'][:80]}... ({r.get('category', 'general')})" for r in all_results[:20]])}
"""
            
            # –ì–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
            logger.info("üß† –ó–∞–ø—É—Å–∫–∞—é –≥–ª—É–±–æ–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å...")
            analysis_result = await self.deep_analysis_with_llm(data_summary, business_name, locations)
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ model_used –Ω–µ None
            model_used_display = analysis_result.get('model_used') or "N/A (–º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
            
            # –°–æ–∑–¥–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            detailed_report = f"""# üïµÔ∏è –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ö–û–ù–ö–£–†–ï–ù–¢–ù–û–ô –†–ê–ó–í–ï–î–ö–ò

**–¶–µ–ª–µ–≤–æ–π –±–∏–∑–Ω–µ—Å:** {business_name}  
**–õ–æ–∫–∞—Ü–∏—è:** {locations}  
**–î–∞—Ç–∞:** {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}  
**–ú–æ–¥–µ–ª—å –∞–Ω–∞–ª–∏–∑–∞:** {model_used_display}

---

## üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê –î–ê–ù–ù–´–•

- **–í—Å–µ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤:** {len(all_results)}
- **–ù–∞–π–¥–µ–Ω–æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤:** {len(competitors)}
- **–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î:** {total_saved} –∑–∞–ø–∏—Å–µ–π
- **–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–∞–Ω–Ω—ã—Ö:**
  - –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã: {len([r for r in all_results if r.get('category') == 'competitors'])}
  - –¶–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ: {len([r for r in all_results if r.get('category') == 'pricing'])}
  - –û—Ç–∑—ã–≤—ã: {len([r for r in all_results if r.get('category') == 'reviews'])}
  - –£—Å–ª—É–≥–∏: {len([r for r in all_results if r.get('category') == 'services'])}
  - –¢—Ä–µ–Ω–¥—ã —Ä—ã–Ω–∫–∞: {len([r for r in all_results if r.get('category') == 'market_trends'])}

---

## üèÜ –¢–û–ü –ö–û–ù–ö–£–†–ï–ù–¢–´

{chr(10).join([f"### {i+1}. {name}" + chr(10) + f"- –£–ø–æ–º–∏–Ω–∞–Ω–∏–π: {info.mentions}" + chr(10) + f"- –†–µ–π—Ç–∏–Ω–≥: {info.avg_rating:.1f}/5" + chr(10) + f"- –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: {info.sentiment}" + chr(10) + f"- –ò—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(info.sources)}" + chr(10)
                for i, (name, info) in enumerate(sorted(competitors.items(), key=lambda x: x[1].mentions, reverse=True)[:10])])}

---

## üß† –ì–õ–£–ë–û–ö–ò–ô –ê–ù–ê–õ–ò–ó (–ß–ï–†–ï–ó –õ–û–ö–ê–õ–¨–ù–£–Æ –ú–û–î–ï–õ–¨)

{analysis_result.get('analysis', '–ê–Ω–∞–ª–∏–∑ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω')}

---

## üìã –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï

<details>
<summary>–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö (JSON)</summary>

```json
{json.dumps({name: asdict(info) for name, info in competitors.items()}, ensure_ascii=False, indent=2)}
```

</details>

---

**–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ:** –ì–ª–µ–± Enhanced (Competitive Intelligence Expert)  
**–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã:** –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (MLX API Server), –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
"""
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç
            report_embedding = await self.get_embedding(detailed_report)
            
            # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ model_used –Ω–µ None (–¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –¥–∞—à–±–æ—Ä–¥–µ)
            model_used = analysis_result.get('model_used')
            if model_used is None:
                model_used = "N/A (–º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)"
            
            metadata_for_report = {
                "source": "enhanced_scout_report",
                "expert_id": str(expert['id']),
                "expert_name": expert['name'],
                "business_target": business_name,
                "location": locations,
                "competitors_count": len(competitors),
                "sources_count": len(all_results),
                "model_used": model_used,
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
            
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω—è—é –æ—Ç—á–µ—Ç: {len(competitors)} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, {len(all_results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –º–æ–¥–µ–ª—å: {model_used}")
            
            await conn.execute("""
                INSERT INTO knowledge_nodes (domain_id, content, embedding, confidence_score, metadata, is_verified)
                VALUES ($1, $2, $3, 0.95, $4, TRUE)
            """, domain_id, detailed_report, str(report_embedding), json.dumps(metadata_for_report))
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)
            victoria_id = await conn.fetchval("SELECT id FROM experts WHERE name = '–í–∏–∫—Ç–æ—Ä–∏—è'")
            task_desc = (f"–ì–ª–µ–± Enhanced –∑–∞–≤–µ—Ä—à–∏–ª –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é —Ä–∞–∑–≤–µ–¥–∫—É –¥–ª—è '{business_name}' –≤ {locations}. "
                        f"–°–æ–±—Ä–∞–Ω–æ {len(all_results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤, –Ω–∞–π–¥–µ–Ω–æ {len(competitors)} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤. "
                        f"–î–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å SWOT, Porter's Five Forces, PEST –∞–Ω–∞–ª–∏–∑–æ–º –≥–æ—Ç–æ–≤. "
                        f"–ü—Ä–æ–≤–µ—Ä—å –æ—Ç—á–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–æ–≤—å –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—é –¥–ª—è —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞.")
            
            await conn.execute("""
                INSERT INTO tasks (title, description, status, assignee_expert_id, creator_expert_id, metadata)
                VALUES ($1, $2, 'pending', $3, $4, $5)
            """, f"üïµÔ∏è Enhanced –†–∞–∑–≤–µ–¥–∫–∞: {business_name}",
               task_desc, expert['id'], victoria_id,
               json.dumps({
                   "source": "enhanced_scout_orchestrator", 
                   "business": business_name,
                   "location": locations,
                   "competitors_count": len(competitors),
                   "sources_count": len(all_results),
                   "report_type": "enhanced"
               }))
            
            logger.info(f"‚úÖ Enhanced —Ä–∞–∑–≤–µ–¥–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {len(competitors)} –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤, {len(all_results)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        await pool.close()
        
        return {
            "competitors": {name: asdict(info) for name, info in competitors.items()},
            "total_sources": len(all_results),
            "analysis": analysis_result.get('analysis', ''),
            "report": detailed_report
        }


async def perform_enhanced_scout_research(
    business_name: str, 
    locations: str,
    extra_competitors: Optional[List[str]] = None
):
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —É–ª—É—á—à–µ–Ω–Ω–æ–π —Ä–∞–∑–≤–µ–¥–∫–∏."""
    researcher = EnhancedScoutResearcher()
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç—ã –∏–∑ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω—ã)
    if extra_competitors is None and len(sys.argv) > 3:
        extra_competitors = [c.strip() for c in sys.argv[3].split(",")]
    
    result = await researcher.perform_enhanced_research(
        business_name, 
        locations,
        extra_competitors
    )
    
    return result


if __name__ == "__main__":
    business = sys.argv[1] if len(sys.argv) > 1 else "–°—Ç–æ–ª–∏—á–Ω—ã–µ –æ–∫–Ω–∞"
    location = sys.argv[2] if len(sys.argv) > 2 else "–ß–µ–±–æ–∫—Å–∞—Ä—ã –∏ –ù–æ–≤–æ—á–µ–±–æ–∫—Å–∞—Ä—Å–∫"
    asyncio.run(perform_enhanced_scout_research(business, location))
