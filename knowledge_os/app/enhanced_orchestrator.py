"""
[KNOWLEDGE OS] Enhanced Orchestrator v3.1.
Enhanced Orchestrator with Task Prioritization and Workload Balancing.
Part of the ATRA Singularity framework.
"""

import asyncio
import getpass
import json
import logging
import os
import subprocess
import sys
import time
import traceback
from datetime import datetime, timedelta
from typing import Dict, Optional

# Third-party imports with fallback
try:
    import asyncpg
    ASYNCPG_AVAILABLE = True
except ImportError:
    asyncpg = None
    ASYNCPG_AVAILABLE = False

try:
    import redis.asyncio as redis
    REDIS_AVAILABLE = True
except ImportError:
    redis = None
    REDIS_AVAILABLE = False

# Local project imports with fallback
try:
    from resource_manager import acquire_resource_lock
except ImportError:
    def acquire_resource_lock(name):  # pylint: disable=unused-argument
        """Fallback for acquire_resource_lock."""
        class MockLock:
            async def __aenter__(self): return self
            async def __aexit__(self, *args): pass
        return MockLock()

try:
    from ai_core import run_smart_agent_sync, run_smart_agent_async
except ImportError:
    def run_smart_agent_sync(prompt, **kwargs):  # pylint: disable=unused-argument
        """Fallback for run_smart_agent_sync."""
        return None

    async def run_smart_agent_async(prompt, **kwargs):  # pylint: disable=unused-argument
        """Fallback for run_smart_agent_async."""
        return None

try:
    from global_scout import run_global_scout_cycle
except ImportError:
    async def run_global_scout_cycle(): pass

try:
    from distillation_engine import KnowledgeDistiller
except ImportError:
    class KnowledgeDistiller:
        """Fallback for KnowledgeDistiller."""
        async def collect_high_quality_samples(self, **kwargs): return 0

try:
    from synthetic_generator import SyntheticKnowledgeGenerator
except ImportError:
    class SyntheticKnowledgeGenerator:
        """Fallback for SyntheticKnowledgeGenerator."""
        async def generate_synthetic_samples(self, **kwargs): pass

try:
    from training_pipeline import LocalTrainingPipeline
except ImportError:
    class LocalTrainingPipeline:
        """Fallback for LocalTrainingPipeline."""
        def trigger_auto_upgrade(self): return "MOCK_OFFLINE"

try:
    from swarm_orchestrator import SwarmOrchestrator
except ImportError:
    class SwarmOrchestrator:
        """Fallback for SwarmOrchestrator."""
        async def handle_critical_failures(self): pass

try:
    from meta_architect import MetaArchitect
except ImportError:
    class MetaArchitect:
        """Fallback for MetaArchitect."""
        async def self_repair_cycle(self): pass

try:
    from knowledge_graph import run_auto_link_detection
except ImportError:
    async def run_auto_link_detection(): pass

try:
    from evolution_monitor import SingularityEvolutionMonitor
except ImportError:
    class SingularityEvolutionMonitor:
        """Fallback for SingularityEvolutionMonitor."""
        async def run_daily_check(self): return "MOCK_EVOLUTION_OFFLINE"

try:
    from curiosity_engine import CuriosityEngine
except ImportError:
    class CuriosityEngine:
        """Fallback for CuriosityEngine."""
        async def scan_for_gaps(self): return "MOCK_CURIOSITY_OFFLINE"

try:
    from memory_consolidator import MemoryConsolidator
except ImportError:
    class MemoryConsolidator:
        """Fallback for MemoryConsolidator."""
        async def consolidate_memory(self): return "MOCK_CONSOLIDATION_OFFLINE"

# Add scripts directory to path for sync
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "../scripts")))
try:
    from server_knowledge_sync import ServerKnowledgeSync
except ImportError:
    ServerKnowledgeSync = None

logger = logging.getLogger(__name__)


def _mask_db_url(url: str) -> str:
    """Mask password in DATABASE_URL for logging."""
    if not url or "@" not in url:
        return "***"
    try:
        pre, rest = url.split("@", 1)
        if ":" in pre:
            user_part, _ = pre.rsplit(":", 1)
            return f"{user_part}:***@{rest}"
    except Exception:
        pass
    return "***"

USER_NAME = getpass.getuser()
# –õ–æ–∫–∞–ª—å–Ω–∞—è –ë–î (Mac Studio): DATABASE_URL –∏–ª–∏ localhost
DEFAULT_DB_URL = os.getenv('DATABASE_URL') or 'postgresql://admin:secret@localhost:5432/knowledge_os'
DB_URL = os.getenv('DATABASE_URL', DEFAULT_DB_URL)
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')

# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –∑–∞–¥–∞—á
PRIORITY_WEIGHTS = {
    'urgent': 100,
    'high': 50,
    'medium': 25,
    'low': 10,
}


async def run_cursor_agent(prompt: str):
    """–ó–∞–ø—É—Å–∫ Cursor Agent –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ —É–º–Ω–æ–µ —è–¥—Ä–æ"""
    if run_smart_agent_async:
        return await run_smart_agent_async(prompt, expert_name="–í–∏–∫—Ç–æ—Ä–∏—è", category="orchestrator")
    return run_smart_agent_sync(prompt, expert_name="–í–∏–∫—Ç–æ—Ä–∏—è", category="orchestrator")


async def get_expert_workload(conn, expert_id: str) -> Dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–π –∑–∞–≥—Ä—É–∑–∫–∏ —ç–∫—Å–ø–µ—Ä—Ç–∞"""
    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á
    active_tasks = await conn.fetchval("""
        SELECT count(*)
        FROM tasks
        WHERE assignee_expert_id = $1
        AND status IN ('pending', 'in_progress')
    """, expert_id)

    # –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
    avg_duration = await conn.fetchval("""
        SELECT AVG(actual_duration_minutes)
        FROM tasks
        WHERE assignee_expert_id = $1
        AND status = 'completed'
        AND actual_duration_minutes IS NOT NULL
        AND completed_at > NOW() - INTERVAL '30 days'
    """, expert_id) or 60  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 60 –º–∏–Ω—É—Ç

    # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π
    completed_recent = await conn.fetchval("""
        SELECT count(*)
        FROM tasks
        WHERE assignee_expert_id = $1
        AND status = 'completed'
        AND completed_at > NOW() - INTERVAL '7 days'
    """, expert_id) or 0

    # –£—Å–ø–µ—à–Ω–æ—Å—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–ø—Ä–æ—Ü–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–Ω—ã—Ö)
    success_rate = await conn.fetchval("""
        SELECT
            CASE
                WHEN count(*) = 0 THEN 1.0
                ELSE count(*) FILTER (WHERE status = 'completed')::float / count(*)::float
            END
        FROM tasks
        WHERE assignee_expert_id = $1
        AND created_at > NOW() - INTERVAL '30 days'
    """, expert_id) or 1.0

    return {
        'active_tasks': active_tasks,
        'avg_duration_minutes': round(avg_duration, 1),
        'completed_recent': completed_recent,
        'success_rate': round(success_rate, 2),
        'workload_score': active_tasks * 10 + (avg_duration / 10),  # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏
    }


async def calculate_task_priority(
    conn,
    title: str,
    description: str,
    metadata: Dict,
    domain_id: Optional[str] = None
) -> str:
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –∑–∞–¥–∞—á–∏"""
    priority_score = 0

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è urgent
    urgent_keywords = ['–∫—Ä–∏—Ç–∏—á–Ω–æ', '—Å—Ä–æ—á–Ω–æ', 'urgent', 'critical', 'üî•', 'üö®']
    if any(kw in title.lower() or kw in description.lower() for kw in urgent_keywords):
        priority_score += 50

    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è high
    high_keywords = ['–≤–∞–∂–Ω–æ', 'important', 'high', '‚ö†Ô∏è']
    if any(kw in title.lower() or kw in description.lower() for kw in high_keywords):
        priority_score += 25

    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    if metadata.get('reason') == 'curiosity_engine_starvation':
        priority_score += 30  # –ì–æ–ª–æ–¥–Ω—ã–µ –¥–æ–º–µ–Ω—ã - –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

    if metadata.get('source') == 'code_auditor':
        severity = metadata.get('severity', 'medium')
        if severity == 'high':
            priority_score += 40
        elif severity == 'medium':
            priority_score += 20

    # –í—Ä–µ–º—è —Å –º–æ–º–µ–Ω—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è (—Å—Ç–∞—Ä—ã–µ –∑–∞–¥–∞—á–∏ –ø–æ–ª—É—á–∞—é—Ç –±–æ–Ω—É—Å)
    if domain_id:
        domain_starvation = await conn.fetchval("""
            SELECT count(*) < 50
            FROM knowledge_nodes
            WHERE domain_id = $1
        """, domain_id)
        if domain_starvation:
            priority_score += 20

    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    if priority_score >= 50:
        return 'urgent'
    if priority_score >= 30:
        return 'high'
    if priority_score >= 15:
        return 'medium'
    return 'low'


async def assign_task_to_best_expert(
    conn,
    task_id: str,
    domain_id: Optional[str] = None,
    required_role: Optional[str] = None
) -> Optional[str]:
    """–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –ª—É—á—à–µ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É —Å —É—á–µ—Ç–æ–º –∑–∞–≥—Ä—É–∑–∫–∏"""
    # –ü–æ–ª—É—á–∞–µ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
    candidates = None
    
    if domain_id:
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ –¥–æ–º–µ–Ω—É
        candidates = await conn.fetch("""
            SELECT id, name, role, department
            FROM experts
            WHERE is_active = true
            AND department = (SELECT name FROM domains WHERE id = $1)
        """, domain_id)
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ –¥–æ–º–µ–Ω—É, –ø—Ä–æ–±—É–µ–º –ø–æ —Å–≤—è–∑–∞–Ω–Ω—ã–º –¥–æ–º–µ–Ω–∞–º —á–µ—Ä–µ–∑ knowledge_nodes
        if not candidates:
            candidates = await conn.fetch("""
                SELECT DISTINCT e.id, e.name, e.role, e.department
                FROM experts e
                INNER JOIN knowledge_nodes kn ON kn.domain_id = $1
                WHERE e.is_active = true
                AND (e.department ILIKE '%' || (SELECT name FROM domains WHERE id = $1) || '%'
                     OR e.role ILIKE '%' || (SELECT name FROM domains WHERE id = $1) || '%')
                LIMIT 20
            """, domain_id)
    
    if not candidates and required_role:
        candidates = await conn.fetch("""
            SELECT id, name, role, department
            FROM experts
            WHERE is_active = true
            AND role ILIKE $1
        """, f"%{required_role}%")
    
    # Fallback: –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –±–µ—Ä–µ–º –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
    if not candidates:
        candidates = await conn.fetch("""
            SELECT id, name, role, department
            FROM experts
            WHERE is_active = true
            ORDER BY RANDOM()
            LIMIT 50
        """)

    if not candidates:
        logger.warning("No experts found for task %s (no active experts in system)", task_id)
        return None

    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥–æ–≥–æ –∫–∞–Ω–¥–∏–¥–∞—Ç–∞
    best_expert = None
    best_score = float('inf')  # –ú–µ–Ω—å—à–µ –∑–∞–≥—Ä—É–∑–∫–∞ = –ª—É—á—à–µ

    for expert in candidates:
        workload = await get_expert_workload(conn, expert['id'])

        # –°—á–∏—Ç–∞–µ–º score (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)
        # –£—á–∏—Ç—ã–≤–∞–µ–º: –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏, —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, —É—Å–ø–µ—à–Ω–æ—Å—Ç—å
        score = (
            workload['workload_score'] * 0.5 +  # –ó–∞–≥—Ä—É–∑–∫–∞
            (1.0 - workload['success_rate']) * 100 * 0.3 +  # –ù–µ—É—Å–ø–µ—à–Ω–æ—Å—Ç—å (—à—Ç—Ä–∞—Ñ)
            (workload['avg_duration_minutes'] / 10) * 0.2  # –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        )

        if score < best_score:
            best_score = score
            best_expert = expert

    if best_expert:
        # –ù–∞–∑–Ω–∞—á–∞–µ–º –∑–∞–¥–∞—á—É
        await conn.execute("""
            UPDATE tasks
            SET assignee_expert_id = $1,
                status = 'pending',
                updated_at = NOW()
            WHERE id = $2
        """, best_expert['id'], task_id)

        logger.info("‚úÖ Task %s assigned to %s (workload: %.2f)", task_id, best_expert['name'], best_score)
        return best_expert['id']

    return None


async def rebalance_workload(conn):
    """–ü–µ—Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏"""
    logger.info("‚öñÔ∏è Starting workload rebalancing...")

    # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ (> 5 –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–¥–∞—á)
    overloaded = await conn.fetch("""
        SELECT assignee_expert_id, count(*) as task_count
        FROM tasks
        WHERE status IN ('pending', 'in_progress')
        GROUP BY assignee_expert_id
        HAVING count(*) > 5
    """)

    # –ü–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–¥–∞—á–∏
    for overloaded_expert in overloaded:
        expert_id = overloaded_expert['assignee_expert_id']
        excess_tasks = overloaded_expert['task_count'] - 5

        # –ë–µ—Ä–µ–º –∑–∞–¥–∞—á–∏ —Å –Ω–∏–∑–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –¥–ª—è –ø–µ—Ä–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
        tasks_to_reassign = await conn.fetch("""
            SELECT id, priority, domain_id
            FROM tasks
            WHERE assignee_expert_id = $1
            AND status = 'pending'
            ORDER BY
                CASE priority
                    WHEN 'urgent' THEN 1
                    WHEN 'high' THEN 2
                    WHEN 'medium' THEN 3
                    WHEN 'low' THEN 4
                END DESC,
                created_at ASC
            LIMIT $2
        """, expert_id, excess_tasks)

        for task in tasks_to_reassign:
            # –ù–∞–∑–Ω–∞—á–∞–µ–º –Ω–µ–∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É –∏–∑ —Ç–æ–≥–æ –∂–µ –¥–æ–º–µ–Ω–∞
            if task['domain_id']:
                new_expert = await conn.fetchrow("""
                    SELECT e.id
                    FROM experts e
                    JOIN domains d ON e.department = d.name
                    WHERE d.id = $1
                    AND e.id != $2
                    AND e.id IN (
                        SELECT id FROM (
                            SELECT e2.id, count(t2.id) as task_count
                            FROM experts e2
                            LEFT JOIN tasks t2 ON t2.assignee_expert_id = e2.id
                                AND t2.status IN ('pending', 'in_progress')
                            GROUP BY e2.id
                            HAVING count(t2.id) < 2
                        ) underloaded_inner
                    )
                    ORDER BY RANDOM()
                    LIMIT 1
                """, task['domain_id'], expert_id)

                if new_expert:
                    await conn.execute("""
                        UPDATE tasks
                        SET assignee_expert_id = $1,
                            updated_at = NOW()
                        WHERE id = $2
                    """, new_expert['id'], task['id'])
                    logger.info("  ‚Üª Task %s reassigned from overloaded expert", task['id'])


async def run_enhanced_orchestration_cycle():
    """–ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ Enhanced Orchestrator —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –∑–Ω–∞–Ω–∏–π –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏"""
    # –û–±–Ω–æ–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∫–∞–∂–¥—ã–º —Ü–∏–∫–ª–æ–º
    try:
        from corporation_knowledge_system import update_all_agents_knowledge
        await update_all_agents_knowledge()
        logger.info("‚úÖ –ó–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –ø–µ—Ä–µ–¥ —Ü–∏–∫–ª–æ–º –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏")
    except Exception as e:
        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏: {e}")
    
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ Orchestrator"""
    if not ASYNCPG_AVAILABLE:
        logger.error("‚ùå asyncpg is not installed. Orchestration aborted.")
        return

    async with acquire_resource_lock("orchestrator"):
        logger.info("[ENHANCED_ORCHESTRATOR] cycle start DATABASE_URL=%s", _mask_db_url(DB_URL))
        conn = await asyncpg.connect(DB_URL)
        try:
            unassigned_count = await conn.fetchval(
                "SELECT COUNT(*) FROM tasks WHERE assignee_expert_id IS NULL"
            )
            logger.info("[ENHANCED_ORCHESTRATOR] unassigned_tasks=%s", unassigned_count)
        except Exception as e:
            logger.warning("[ENHANCED_ORCHESTRATOR] could not count unassigned: %s", e)
        rd = None
        if REDIS_AVAILABLE:
            rd = await redis.from_url(REDIS_URL, decode_responses=True)

        try:
            # --- –§–ê–ó–ê 0: AUTONOMOUS ERROR FIXING (–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫) ---
            t0 = time.time()
            logger.info("üîß Phase 0: Auto-fixing errors...")
            phase_result = "skipped"
            try:
                from error_auto_fixer import auto_fix_all_errors
                fix_results = await auto_fix_all_errors(conn)
                if fix_results.get('stuck_tasks_fixed', 0) > 0 or fix_results.get('unassigned_tasks', 0) > 0:
                    phase_result = str(fix_results)
                else:
                    phase_result = "ok"
            except ImportError:
                logger.debug("error_auto_fixer module not found, skipping")
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.warning("Auto-fix error: %s", exc)
                phase_result = str(exc)
            logger.info("[ENHANCED_ORCHESTRATOR] phase=0 duration_ms=%.0f result=%s", (time.time() - t0) * 1000, phase_result)

            # --- –§–ê–ó–ê 0.5: AUTONOMOUS MIGRATIONS (—Ä–∞–±–æ—Ç–∞–µ—Ç –±–µ–∑ knowledge_nodes/domains) ---
            t05 = time.time()
            logger.info("üóÑÔ∏è Phase 0.5: Autonomous Migrations...")
            try:
                await conn.execute("""
                    CREATE TABLE IF NOT EXISTS schema_migrations (
                        id SERIAL PRIMARY KEY,
                        migration_name VARCHAR(255) UNIQUE NOT NULL,
                        applied_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
                    )
                """)
                base_dir_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                migration_dir = os.path.join(base_dir_path, "db", "migrations")
                if os.path.exists(migration_dir):
                    applied_list = await conn.fetch(
                        "SELECT migration_name FROM schema_migrations"
                    )
                    applied_set = {r["migration_name"] for r in applied_list}
                    for file_name in sorted(os.listdir(migration_dir)):
                        if not file_name.endswith(".sql"):
                            continue
                        if file_name in applied_set:
                            continue
                        logger.info("  ‚ö° Applying migration: %s", file_name)
                        try:
                            with open(os.path.join(migration_dir, file_name), 'r', encoding='utf-8') as f:
                                await conn.execute(f.read())
                            await conn.execute(
                                "INSERT INTO schema_migrations (migration_name, applied_at) VALUES ($1, NOW()) ON CONFLICT (migration_name) DO NOTHING",
                                file_name
                            )
                            logger.info("  ‚úÖ Applied: %s", file_name)
                        except Exception as mig_err:  # pylint: disable=broad-exception-caught
                            logger.error("  ‚ùå Migration %s failed: %s", file_name, mig_err)
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Migration error: %s", exc)
            logger.info("[ENHANCED_ORCHESTRATOR] phase=0.5 duration_ms=%.0f result=migrations", (time.time() - t05) * 1000)

            victoria_id = await conn.fetchval("SELECT id FROM experts WHERE name = '–í–∏–∫—Ç–æ—Ä–∏—è'")
            if not victoria_id:
                logger.warning("Victoria not found, creating...")
                victoria_id = await conn.fetchval("""
                    INSERT INTO experts (name, role, system_prompt, department)
                    VALUES ('–í–∏–∫—Ç–æ—Ä–∏—è', 'Team Lead', 'Team Lead and Coordinator', 'Management')
                    RETURNING id
                """)

            # --- –§–ê–ó–ê 1: –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–Ø –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –ó–ê–î–ê–ß ---
            t1 = time.time()
            logger.info("üìä Phase 1: Prioritizing existing tasks...")
            unprioritized_tasks = await conn.fetch("""
                SELECT id, title, description, metadata, domain_id
                FROM tasks
                WHERE priority = 'medium'
                AND status = 'pending'
                AND created_at > NOW() - INTERVAL '24 hours'
            """)

            for task in unprioritized_tasks:
                task_meta = json.loads(task['metadata']) if isinstance(task['metadata'], str) else task['metadata']
                new_priority = await calculate_task_priority(
                    conn, task['title'], task['description'], task_meta, task['domain_id']
                )
                if new_priority != 'medium':
                    await conn.execute("""
                        UPDATE tasks
                        SET priority = $1,
                            updated_at = NOW()
                        WHERE id = $2
                    """, new_priority, task['id'])
                    logger.info("  üìå Task %s: priority updated to %s", task['id'], new_priority)
            logger.info("[ENHANCED_ORCHESTRATOR] phase=1 duration_ms=%.0f result=%s tasks reprioritized", (time.time() - t1) * 1000, len(unprioritized_tasks))

            # --- –§–ê–ó–ê 2: –ù–ê–ó–ù–ê–ß–ï–ù–ò–ï –ó–ê–î–ê–ß –ë–ï–ó –ò–°–ü–û–õ–ù–ò–¢–ï–õ–Ø ---
            t2 = time.time()
            logger.info("üë• Phase 2: Assigning unassigned tasks...")
            unassigned_tasks = await conn.fetch("""
                SELECT id, title, description, domain_id, priority, metadata
                FROM tasks
                WHERE assignee_expert_id IS NULL
                AND status = 'pending'
                ORDER BY
                    CASE priority
                        WHEN 'urgent' THEN 1
                        WHEN 'high' THEN 2
                        WHEN 'medium' THEN 3
                        WHEN 'low' THEN 4
                    END,
                    created_at ASC
            """)

            for task in unassigned_tasks:
                await assign_task_to_best_expert(conn, task['id'], task['domain_id'])
            logger.info("[ENHANCED_ORCHESTRATOR] phase=2 duration_ms=%.0f result=%s tasks assigned", (time.time() - t2) * 1000, len(unassigned_tasks))

            # --- –§–ê–ó–ê 3: –ü–ï–†–ï–ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê –ù–ê–ì–†–£–ó–ö–ò ---
            t3 = time.time()
            logger.info("‚öñÔ∏è Phase 3: Rebalancing workload...")
            await rebalance_workload(conn)
            logger.info("[ENHANCED_ORCHESTRATOR] phase=3 duration_ms=%.0f result=rebalance", (time.time() - t3) * 1000)

            # --- –§–ê–ó–ê 4: –ê–°–°–û–¶–ò–ê–¢–ò–í–ù–´–ô –ú–û–ó–ì (–∫–∞–∫ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª–µ) ---
            logger.info("üß© Phase 4: Cross-domain linking...")
            new_knowledge = await conn.fetch("""
                SELECT k.id, k.content, d.name as domain, k.metadata, k.domain_id
                FROM knowledge_nodes k
                JOIN domains d ON k.domain_id = d.id
                WHERE k.created_at > NOW() - INTERVAL '6 hours'
                AND (k.metadata->>'orchestrated' IS NULL OR k.metadata->>'orchestrated' = 'false')
                LIMIT 10
            """)

            for node in new_knowledge:
                random_node = await conn.fetchrow("""
                    SELECT k.content, d.name as domain
                    FROM knowledge_nodes k JOIN domains d ON k.domain_id = d.id
                    WHERE k.domain_id != $1 ORDER BY RANDOM() LIMIT 1
                """, node['domain_id'])

                if random_node:
                    link_prompt = f"""
                    –í—ã - –í–∏–∫—Ç–æ—Ä–∏—è (Team Lead). –ù–∞–π–¥–∏—Ç–µ –Ω–µ–æ—á–µ–≤–∏–¥–Ω—É—é —Å–≤—è–∑—å –º–µ–∂–¥—É –¥–≤—É–º—è —Ñ–∞–∫—Ç–∞–º–∏:
                    –§–ê–ö–¢ –ê ({node['domain']}): {node['content']}
                    –§–ê–ö–¢ –ë ({random_node['domain']}): {random_node['content']}

                    –ó–ê–î–ê–ß–ê: –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –æ–¥–Ω—É –∏–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω—É—é –≥–∏–ø–æ—Ç–µ–∑—É –Ω–∞ —Å—Ç—ã–∫–µ —ç—Ç–∏—Ö –∑–Ω–∞–Ω–∏–π.
                    –í–µ—Ä–Ω–∏—Ç–µ –¢–û–õ–¨–ö–û —Ç–µ–∫—Å—Ç –≥–∏–ø–æ—Ç–µ–∑—ã.
                    """
                    hypothesis = await run_cursor_agent(link_prompt)
                    if hypothesis:
                        await conn.execute("""
                            INSERT INTO knowledge_nodes (domain_id, content, confidence_score, metadata, is_verified)
                            VALUES ($1, $2, 0.95, $3, true)
                        """, node['domain_id'], f"üî¨ –ö–†–û–°–°-–î–û–ú–ï–ù–ù–ê–Ø –ì–ò–ü–û–¢–ï–ó–ê: {hypothesis}",
                        json.dumps({"source": "cross_domain_linker", "parents": [str(node['id'])]}))
                        if rd:
                            await rd.xadd("knowledge_stream", {"type": "synthetic_link", "content": hypothesis})

                await conn.execute("""
                    UPDATE knowledge_nodes
                    SET metadata = metadata || '{"orchestrated": "true"}'::jsonb
                    WHERE id = $1
                """, node['id'])

            # --- –§–ê–ó–ê 5: –î–í–ò–ì–ê–¢–ï–õ–¨ –õ–Æ–ë–û–ü–´–¢–°–¢–í–ê (—Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π) ---
            logger.info("üîç Phase 5: Curiosity Engine...")
            deserts = await conn.fetch("""
                SELECT d.id, d.name, count(k.id) as node_count
                FROM domains d LEFT JOIN knowledge_nodes k ON d.id = k.domain_id
                GROUP BY d.id, d.name
                HAVING count(k.id) < 50 OR max(k.created_at) < NOW() - INTERVAL '48 hours'
                ORDER BY count(k.id) ASC
                LIMIT 5
            """)

            for desert in deserts:
                expert_count = await conn.fetchval("SELECT count(*) FROM experts WHERE department = $1", desert['name'])
                if expert_count == 0:
                    logger.info("  üîç Recruiting expert for %s...", desert['name'])
                    expert_gen_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "expert_generator.py")
                    subprocess.run(["python3", expert_gen_path, desert['name']], check=False)

                curiosity_task = (
                    f"–ü—Ä–æ–≤–µ–¥–∏ –≥–ª—É–±–æ–∫–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —Ç—Ä–µ–Ω–¥–æ–≤ 2026 "
                    f"–≤ –æ–±–ª–∞—Å—Ç–∏ {desert['name']}. –ù–∞–π–¥–∏ 3 –ø—Ä–æ—Ä—ã–≤–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–∞."
                )
                priority = 'high' if desert['node_count'] < 20 else 'medium'
                task_id = await conn.fetchval("""
                    INSERT INTO tasks (title, description, status, priority, creator_expert_id, domain_id, metadata)
                    VALUES ($1, $2, 'pending', $3, $4, $5, $6)
                    RETURNING id
                """, f"üî• –ò–°–°–õ–ï–î–û–í–ê–ù–ò–ï: {desert['name']}", curiosity_task, priority, victoria_id, desert['id'],
                json.dumps({"reason": "curiosity_engine_starvation", "node_count": desert['node_count']}))
                await assign_task_to_best_expert(conn, task_id, desert['id'])

            logger.info("üåê Phase 5: Running Global Scout validation...")
            try:
                await run_global_scout_cycle()
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Global Scout error: %s", exc)

            logger.info("üîó Phase 6: Running auto-link detection...")
            try:
                await run_auto_link_detection()
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Auto-link detection error: %s", exc)

            logger.info("üß¨ Phase 7: Knowledge Distillation & Auto-Upgrade...")
            try:
                distiller = KnowledgeDistiller()
                distilled_count = await distiller.collect_high_quality_samples(days=1)
                if distilled_count > 0:
                    logger.info("  ‚ú® Distilled %d high-quality samples.", distilled_count)
                generator = SyntheticKnowledgeGenerator()
                await generator.generate_synthetic_samples(limit=5)
                pipeline = LocalTrainingPipeline()
                status = pipeline.trigger_auto_upgrade()
                if "–ó–ê–ü–£–©–ï–ù" in status or "–ì–û–¢–û–í" in status:
                    logger.info("  üî• AUTONOMOUS UPGRADE STATUS: %s", status)
                    await conn.execute("INSERT INTO notifications (message) VALUES ($1)", status)
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Distillation error: %s", exc)

            logger.info("üîß Phase 8: Self-Repair Engine...")
            try:
                errors = await conn.fetch("""
                    SELECT id, user_query, assistant_response, metadata
                    FROM interaction_logs
                    WHERE (assistant_response LIKE '‚ùå%' OR assistant_response LIKE '‚ö†Ô∏è%')
                    AND created_at > NOW() - INTERVAL '1 hour'
                    AND (metadata->>'repaired' IS NULL OR metadata->>'repaired' = 'false')
                    LIMIT 5
                """)
                for err in errors:
                    repair_task = (
                        f"–û–®–ò–ë–ö–ê –í –°–ò–°–¢–ï–ú–ï: {err['assistant_response']}\n"
                        f"–ó–ê–ü–†–û–°: {err['user_query']}\n\n"
                        f"–ó–ê–î–ê–ß–ê: –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ª–æ–≥–∏ –∏ –∫–æ–¥, –Ω–∞–π–¥–∏ –ø—Ä–∏—á–∏–Ω—É –∏ –ø—Ä–µ–¥–ª–æ–∂–∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ."
                    )
                    await conn.execute("""
                        INSERT INTO tasks (title, description, status, priority, creator_expert_id, metadata)
                        VALUES ($1, $2, 'pending', 'urgent', $3, $4)
                    """, "üö® –ê–í–¢–û-–†–ï–ú–û–ù–¢: –û—à–∏–±–∫–∞", repair_task, victoria_id,
                    json.dumps({"source": "self_repair", "log_id": str(err['id'])}))
                    await conn.execute("""
                        UPDATE interaction_logs
                        SET metadata = metadata || '{"repaired": "true"}'::jsonb
                        WHERE id = $1
                    """, err['id'])
                    logger.info("  üîß Created repair task for log %s", err['id'])
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Self-repair error: %s", exc)

            logger.info("üêù Phase 10: Swarm War-Room...")
            try:
                swarm = SwarmOrchestrator()
                await swarm.handle_critical_failures()
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Swarm error: %s", exc)

            logger.info("üèóÔ∏è Phase 11: Meta-Architect Review...")
            try:
                architect = MetaArchitect()
                await architect.self_repair_cycle()
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Meta-Architect error: %s", exc)

            logger.info("üß¨ Phase 12: Autonomous Evolution...")
            try:
                evolution = SingularityEvolutionMonitor()
                evolution_report = await evolution.run_daily_check()
                logger.info("  %s", evolution_report)
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Evolution error: %s", exc)

            logger.info("üîç Phase 13: Curiosity Engine Gap Analysis...")
            try:
                curiosity = CuriosityEngine()
                gap_result = await curiosity.scan_for_gaps()
                logger.info("  %s", gap_result)
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Curiosity error: %s", exc)

            logger.info("üß† Phase 14: Memory Consolidation (The Dreaming)...")
            try:
                consolidator = MemoryConsolidator()
                consolidation_result = await consolidator.consolidate_memory()
                logger.info("  %s", consolidation_result)
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Consolidation error: %s", exc)

            logger.info("üåê Phase 15: Global Team Knowledge Sync...")
            if ServerKnowledgeSync:
                try:
                    last_sync_key = "last_global_sync"
                    last_sync = None
                    if rd:
                        last_sync = await rd.get(last_sync_key)
                    now_str = datetime.now().isoformat()
                    should_sync = True
                    if last_sync:
                        last_sync_dt = datetime.fromisoformat(last_sync)
                        if datetime.now() - last_sync_dt < timedelta(hours=1):
                            should_sync = False
                    if should_sync:
                        sync_manager = ServerKnowledgeSync()
                        await sync_manager.sync_experts()
                        synced_count = await sync_manager.sync_reports(limit=50)
                        logger.info("  üì• Synced %d reports and full team hierarchy.", synced_count)
                        if rd:
                            await rd.set(last_sync_key, now_str)
                    else:
                        logger.info("  ‚è≠Ô∏è Sync skipped (already synced recently).")
                except Exception as exc:  # pylint: disable=broad-exception-caught
                    logger.error("Sync error: %s", exc)
            else:
                logger.info("  ‚ö†Ô∏è ServerKnowledgeSync module not found.")

            logger.info("üì¶ Phase 16: Knowledge Archivation...")
            try:
                from knowledge_archiver import KnowledgeArchiver
                from strategy_session_manager import StrategySessionManager
                session_manager = StrategySessionManager()
                archiver = KnowledgeArchiver(session_manager)
                archive_key = "last_knowledge_archive"
                last_archive = None
                if rd:
                    last_archive = await rd.get(archive_key)
                now_str = datetime.now().isoformat()
                should_archive = True
                if last_archive:
                    last_archive_dt = datetime.fromisoformat(last_archive)
                    if datetime.now() - last_archive_dt < timedelta(days=1):
                        should_archive = False
                if should_archive:
                    await archiver.periodic_archive_task()
                    if rd:
                        await rd.set(archive_key, now_str)
                    logger.info("  ‚úÖ Knowledge archivation completed.")
                else:
                    logger.info("  ‚è≠Ô∏è Archive skipped (already archived today).")
            except ImportError:
                logger.info("  ‚ö†Ô∏è KnowledgeArchiver module not found.")
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Archive error: %s", exc)

            await conn.close()
            logger.info("[ENHANCED_ORCHESTRATOR] cycle finished successfully.")
        except Exception as cycle_exc:  # pylint: disable=broad-exception-caught
            logger.error("[ENHANCED_ORCHESTRATOR] cycle exception: %s", cycle_exc)
            logger.error(traceback.format_exc())
            try:
                await conn.close()
            except Exception:
                pass
            raise


async def run_continuous(interval_seconds: int = 60, quick_poll_seconds: int = 30):
    """
    –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª: –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä ¬´–≤—Å–µ –≤—Ä–µ–º—è —Å–ª—É—à–∞–µ—Ç¬ª ‚Äî –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç —Ü–∏–∫–ª –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏.
    –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ü–∏–∫–ª–∞ —Å–ø–∏—Ç interval_seconds; –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ ‚Äî —Å–ª–µ–¥—É—é—â–∏–π —Ü–∏–∫–ª
    —á–µ—Ä–µ–∑ quick_poll_seconds (—Ä–µ–∞–≥–∏—Ä—É–µ–º –±—ã—Å—Ç—Ä–µ–µ –ø—Ä–∏ –ø–æ—è–≤–ª–µ–Ω–∏–∏ —Ä–∞–±–æ—Ç—ã).
    """
    logger.info(
        "[ENHANCED_ORCHESTRATOR] continuous mode: interval=%ss, quick_poll=%ss",
        interval_seconds,
        quick_poll_seconds,
    )
    while True:
        try:
            await run_enhanced_orchestration_cycle()
        except Exception as e:  # pylint: disable=broad-exception-caught
            logger.error("[ENHANCED_ORCHESTRATOR] cycle error: %s", e)
            logger.error(traceback.format_exc())
        # –†–µ—à–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Å–ø–∞—Ç—å: –µ—Å–ª–∏ –µ—Å—Ç—å –Ω–µ—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ ‚Äî –∫–æ—Ä–æ—Ç–∫–∏–π —Å–æ–Ω
        sleep_sec = interval_seconds
        if ASYNCPG_AVAILABLE and interval_seconds > quick_poll_seconds:
            try:
                conn = await asyncpg.connect(DB_URL)
                try:
                    unassigned = await conn.fetchval(
                        "SELECT COUNT(*) FROM tasks WHERE assignee_expert_id IS NULL AND status = 'pending'"
                    )
                    if unassigned and unassigned > 0:
                        sleep_sec = quick_poll_seconds
                        logger.info("[ENHANCED_ORCHESTRATOR] %s unassigned tasks, next cycle in %ss", unassigned, sleep_sec)
                finally:
                    await conn.close()
            except Exception as e:  # pylint: disable=broad-exception-caught
                logger.debug("Quick poll failed: %s, using full interval", e)
        await asyncio.sleep(sleep_sec)


if __name__ == '__main__':
    import argparse
    parser = argparse.ArgumentParser(description="Enhanced Orchestrator")
    parser.add_argument("--prompt", nargs="*", help="Single prompt (for Telegram gateway)")
    parser.add_argument("--continuous", action="store_true", help="Run forever: listen and orchestrate on interval")
    parser.add_argument("--interval", type=int, default=60, help="Seconds between cycles in continuous mode (default: 60)")
    parser.add_argument("--quick-poll", type=int, default=30, help="Seconds to next cycle when unassigned tasks exist (default: 30)")
    args = parser.parse_args()

    # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø—Ä—è–º–æ–≥–æ –≤—ã–∑–æ–≤–∞ —á–µ—Ä–µ–∑ –∞—Ä–≥—É–º–µ–Ω—Ç—ã (–¥–ª—è Telegram —à–ª—é–∑–∞)
    if args.prompt:
        PROMPT_TEXT_INPUT = " ".join(args.prompt)
        try:
            main_result = asyncio.run(run_cursor_agent(PROMPT_TEXT_INPUT))
        except RuntimeError:
            main_result = run_smart_agent_sync(PROMPT_TEXT_INPUT, expert_name="–í–∏–∫—Ç–æ—Ä–∏—è", category="orchestrator")
        if main_result:
            print(main_result)
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –≤ —è–¥—Ä–µ.")
    elif args.continuous:
        asyncio.run(run_continuous(interval_seconds=args.interval, quick_poll_seconds=args.quick_poll))
    else:
        asyncio.run(run_enhanced_orchestration_cycle())
