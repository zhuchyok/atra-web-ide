"""
Model Fine-Tuner - –î–æ–æ–±—É—á–µ–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞, —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç MLX (Apple Silicon) –∏ Ollama –º–æ–¥–µ–ª–∏
"""

import os
import json
import asyncio
import logging
import subprocess
from datetime import datetime, timezone
from typing import List, Dict, Optional, Tuple
import asyncpg

logger = logging.getLogger(__name__)

DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
MLX_MODELS_DIR = os.getenv('MLX_MODELS_DIR', os.path.expanduser('~/.mlx_models'))
TRAINING_DATA_DIR = os.getenv('TRAINING_DATA_DIR', './training_data')

class ModelFineTuner:
    """
    –î–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
    
    ‚ö†Ô∏è –í–ê–ñ–ù–û: –ù–ï –¥–æ–æ–±—É—á–∞–µ–º –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π!
    - –§–∞–∫—Ç—ã —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ RAG (–í–∏–∫—Ç–æ—Ä–∏—è –∏ –í–µ—Ä–æ–Ω–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç)
    - –î–æ–æ–±—É—á–∞–µ–º –¢–û–õ–¨–ö–û –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö —Å—Ç–∏–ª—è –∏ —Ñ–æ—Ä–º–∞—Ç–∞—Ö –æ—Ç–≤–µ—Ç–æ–≤
    """
    
    def __init__(self, db_url: str = DB_URL):
        self.db_url = db_url
        self.mlx_models_dir = MLX_MODELS_DIR
        self.training_data_dir = TRAINING_DATA_DIR
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        os.makedirs(self.training_data_dir, exist_ok=True)
        os.makedirs(self.mlx_models_dir, exist_ok=True)
    
    async def collect_style_patterns(self, limit: int = 500) -> List[Dict]:
        """
        –°–æ–±—Ä–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –°–¢–ò–õ–Ø –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (–ù–ï —Ñ–∞–∫—Ç—ã!)
        
        ‚ö†Ô∏è –í–ê–ñ–ù–û: –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–æ–≤, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ, —Å—Ç—Ä—É–∫—Ç—É—Ä—É.
        –ù–ï —Å–æ–±–∏—Ä–∞–µ–º —Ñ–∞–∫—Ç—ã - –æ–Ω–∏ —É–∂–µ –≤ RAG!
        
        –ß—Ç–æ —Å–æ–±–∏—Ä–∞–µ–º:
        - –°—Ç–∏–ª—å –æ—Ç–≤–µ—Ç–æ–≤ –í–∏–∫—Ç–æ—Ä–∏–∏ (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ, —Å —ç–º–æ–¥–∑–∏)
        - –§–æ—Ä–º–∞—Ç—ã (–ø–ª–∞–Ω—ã, –æ—Ç—á–µ—Ç—ã, –∫–æ–¥)
        - –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
        - –°—Ç—Ä—É–∫—Ç—É—Ä—É –æ—Ç–≤–µ—Ç–æ–≤
        """
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                # –°–æ–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä—ã —Å–æ —Å—Ç–∏–ª–µ–º –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
                # –ò—Å–∫–ª—é—á–∞–µ–º —á–∏—Å—Ç—ã–µ —Ñ–∞–∫—Ç—ã (type='fact', 'definition')
                rows = await conn.fetch("""
                    SELECT content, metadata, confidence_score, domain_id
                    FROM knowledge_nodes
                    WHERE is_verified = TRUE
                    AND confidence_score >= 0.8
                    AND (
                        metadata->>'type' IN ('code_example', 'plan', 'report', 'analysis', 'solution')
                        OR content LIKE '%üìã%' OR content LIKE '%‚úÖ%' OR content LIKE '%üí°%'
                        OR content LIKE '%üîç%' OR content LIKE '%üìä%'
                        OR content LIKE '%–ü–õ–ê–ù%' OR content LIKE '%–®–ê–ì%' OR content LIKE '%–≠–¢–ê–ü%'
                    )
                    AND metadata->>'type' NOT IN ('fact', 'definition', 'data')
                    ORDER BY confidence_score DESC, usage_count DESC
                    LIMIT $1
                """, limit)
                
                style_patterns = []
                for row in rows:
                    domain = await conn.fetchval("SELECT name FROM domains WHERE id = $1", row['domain_id'])
                    content = row['content']
                    metadata = row['metadata'] or {}
                    task_type = metadata.get('type', 'general')
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å—Ç–∏–ª—è
                    style_type = self._determine_style_type(content, task_type)
                    
                    style_patterns.append({
                        "instruction": self._create_style_instruction(style_type, task_type, domain),
                        "input": "",  # –°—Ç–∏–ª—å –Ω–µ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –≤—Ö–æ–¥–∞
                        "output": content,  # –ü—Ä–∏–º–µ—Ä —Å—Ç–∏–ª—è
                        "style_type": style_type,
                        "domain": domain,
                        "confidence": float(row['confidence_score']),
                        "metadata": {
                            **metadata,
                            "is_style_pattern": True,
                            "not_fact": True  # –Ø–≤–Ω–æ –ø–æ–º–µ—á–∞–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ñ–∞–∫—Ç
                        }
                    })
                
                logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(style_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –°–¢–ò–õ–Ø (–Ω–µ —Ñ–∞–∫—Ç–æ–≤!)")
                return style_patterns
                
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å—Ç–∏–ª—è: {e}")
            return []
    
    def _determine_style_type(self, content: str, task_type: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–∏–ø —Å—Ç–∏–ª—è –æ—Ç–≤–µ—Ç–∞"""
        content_lower = content.lower()
        
        if 'üìã' in content or '–ø–ª–∞–Ω' in content_lower or '—ç—Ç–∞–ø' in content_lower:
            return "structured_plan"
        elif '‚úÖ' in content or '—à–∞–≥' in content_lower:
            return "step_by_step"
        elif 'üí°' in content or '—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è' in content_lower:
            return "recommendation"
        elif 'üîç' in content or '–∞–Ω–∞–ª–∏–∑' in content_lower:
            return "analysis"
        elif task_type == "coding" or 'def ' in content or 'function' in content_lower:
            return "code_pattern"
        elif 'üìä' in content or '–æ—Ç—á–µ—Ç' in content_lower:
            return "report"
        else:
            return "general_structured"
    
    def _create_style_instruction(self, style_type: str, task_type: str, domain: str) -> str:
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç–∏–ª—é"""
        style_instructions = {
            "structured_plan": f"–°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ (—Å —ç–º–æ–¥–∑–∏, –ø–æ —ç—Ç–∞–ø–∞–º) –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}",
            "step_by_step": f"–û—Ç–≤–µ—Ç—å –ø–æ—à–∞–≥–æ–≤–æ –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ (—Å ‚úÖ, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ) –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}",
            "recommendation": f"–î–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ (—Å üí°, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ) –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}",
            "analysis": f"–ü—Ä–æ–≤–µ–¥–∏ –∞–Ω–∞–ª–∏–∑ –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ (—Å üîç, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ) –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}",
            "code_pattern": f"–ù–∞–ø–∏—à–∏ –∫–æ–¥ –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ (—á–∏—Å—Ç—ã–π, –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π) –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}",
            "report": f"–°–æ–∑–¥–∞–π –æ—Ç—á–µ—Ç –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ (—Å üìä, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ) –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}",
            "general_structured": f"–û—Ç–≤–µ—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}"
        }
        return style_instructions.get(style_type, f"–û—Ç–≤–µ—Ç—å –≤ —Å—Ç–∏–ª–µ –í–∏–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}")
    
    async def collect_training_data_from_knowledge_base(self, limit: int = 1000) -> List[Dict]:
        """
        ‚ö†Ô∏è –£–°–¢–ê–†–ï–í–®–ò–ô –ú–ï–¢–û–î: –°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∞–∫—Ç—ã (–Ω–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è!)
        
        –í–ê–ñ–ù–û: –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ—Ç –º–µ—Ç–æ–¥ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö!
        –§–∞–∫—Ç—ã —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ RAG (–í–∏–∫—Ç–æ—Ä–∏—è –∏ –í–µ—Ä–æ–Ω–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç).
        
        –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ collect_style_patterns() –≤–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ!
        """
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è —Å –≤—ã—Å–æ–∫–∏–º confidence_score
                rows = await conn.fetch("""
                    SELECT content, metadata, confidence_score, domain_id
                    FROM knowledge_nodes
                    WHERE is_verified = TRUE
                    AND confidence_score >= 0.8
                    ORDER BY confidence_score DESC, usage_count DESC
                    LIMIT $1
                """, limit)
                
                training_data = []
                for row in rows:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–æ–º–µ–Ω
                    domain = await conn.fetchval("SELECT name FROM domains WHERE id = $1", row['domain_id'])
                    
                    # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏ –æ—Ç–≤–µ—Ç
                    content = row['content']
                    metadata = row['metadata'] or {}
                    
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏
                    task_type = metadata.get('type', 'general')
                    
                    training_data.append({
                        "instruction": self._create_instruction(content, task_type, domain),
                        "input": "",
                        "output": content,
                        "domain": domain,
                        "confidence": float(row['confidence_score']),
                        "metadata": metadata
                    })
                
                logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(training_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
                return training_data
                
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π: {e}")
            return []
    
    def _create_instruction(self, content: str, task_type: str, domain: str) -> str:
        """–°–æ–∑–¥–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if task_type == "coding":
            return f"–ù–∞–ø–∏—à–∏ –∫–æ–¥ –¥–ª—è –∑–∞–¥–∞—á–∏ –≤ –¥–æ–º–µ–Ω–µ {domain}"
        elif task_type == "reasoning":
            return f"–†–µ—à–∏ –∑–∞–¥–∞—á—É —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –≤ –¥–æ–º–µ–Ω–µ {domain}"
        elif task_type == "explanation":
            return f"–û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é –∏–∑ –¥–æ–º–µ–Ω–∞ {domain}"
        else:
            return f"–û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –∏–∑ –¥–æ–º–µ–Ω–∞ {domain}"
    
    async def collect_anti_hallucination_data(self) -> List[Dict]:
        """–°–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π"""
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Å —è–≤–Ω—ã–º–∏ —É–∫–∞–∑–∞–Ω–∏—è–º–∏ –Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å
                rows = await conn.fetch("""
                    SELECT content, metadata
                    FROM knowledge_nodes
                    WHERE is_verified = TRUE
                    AND confidence_score >= 0.9
                    AND metadata->>'type' IN ('fact', 'definition', 'code_example')
                    ORDER BY confidence_score DESC
                    LIMIT 500
                """)
                
                anti_hallucination_data = []
                for row in rows:
                    content = row['content']
                    metadata = row['metadata'] or {}
                    
                    anti_hallucination_data.append({
                        "instruction": "–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤. –ï—Å–ª–∏ –Ω–µ –∑–Ω–∞–µ—à—å —Ç–æ—á–Ω–æ - —Å–∫–∞–∂–∏ '–ù–µ —É–≤–µ—Ä–µ–Ω'.",
                        "input": content[:200],  # –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        "output": content,
                        "metadata": {
                            **metadata,
                            "anti_hallucination": True,
                            "verified": True
                        }
                    })
                
                logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(anti_hallucination_data)} –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π")
                return anti_hallucination_data
                
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π: {e}")
            return []
    
    def prepare_training_dataset(self, training_data: List[Dict], output_file: str) -> str:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSONL"""
        output_path = os.path.join(self.training_data_dir, output_file)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in training_data:
                # –§–æ—Ä–º–∞—Ç –¥–ª—è MLX-LM
                formatted = {
                    "text": f"### Instruction:\n{item['instruction']}\n\n### Input:\n{item.get('input', '')}\n\n### Response:\n{item['output']}"
                }
                f.write(json.dumps(formatted, ensure_ascii=False) + '\n')
        
        logger.info(f"‚úÖ –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path} ({len(training_data)} –ø—Ä–∏–º–µ—Ä–æ–≤)")
        return output_path
    
    async def fine_tune_mlx_model(
        self,
        base_model: str,
        training_data_path: str,
        output_model_name: str,
        lora_rank: int = 16,
        lora_alpha: int = 32,
        batch_size: int = 4,
        learning_rate: float = 1e-4,
        num_epochs: int = 3
    ) -> Tuple[bool, str]:
        """
        –î–æ–æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ MLX-LM (LoRA)
        
        Args:
            base_model: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (–ø—É—Ç—å –∏–ª–∏ HuggingFace ID)
            training_data_path: –ü—É—Ç—å –∫ JSONL —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            output_model_name: –ò–º—è –≤—ã—Ö–æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏
            lora_rank: –†–∞–Ω–≥ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞
            lora_alpha: –ê–ª—å—Ñ–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä LoRA
            batch_size: –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
            learning_rate: –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
            num_epochs: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
        
        Returns:
            (success, message)
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ mlx-lm
            result = subprocess.run(
                ['python3', '-c', 'import mlx_lm; print(mlx_lm.__version__)'],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode != 0:
                return False, "MLX-LM –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install mlx-lm"
            
            # –ö–æ–º–∞–Ω–¥–∞ –¥–ª—è fine-tuning
            output_path = os.path.join(self.mlx_models_dir, output_model_name)
            
            cmd = [
                'python3', '-m', 'mlx_lm.lora',
                '--model', base_model,
                '--data', training_data_path,
                '--train',
                '--lora-layers', '16',  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –¥–ª—è LoRA
                '--rank', str(lora_rank),
                '--alpha', str(lora_alpha),
                '--batch-size', str(batch_size),
                '--learning-rate', str(learning_rate),
                '--iters', str(num_epochs * 100),  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Ç–µ—Ä–∞—Ü–∏–π
                '--output-dir', output_path
            ]
            
            logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ fine-tuning –º–æ–¥–µ–ª–∏ {base_model}...")
            logger.info(f"   –ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=os.path.dirname(training_data_path)
            )
            
            # –õ–æ–≥–∏—Ä—É–µ–º –≤—ã–≤–æ–¥
            stdout_lines = []
            stderr_lines = []
            
            while True:
                output = process.stdout.readline()
                if output == '' and process.poll() is not None:
                    break
                if output:
                    stdout_lines.append(output.strip())
                    logger.info(f"   {output.strip()}")
            
            stderr = process.stderr.read()
            if stderr:
                stderr_lines = stderr.split('\n')
                for line in stderr_lines:
                    if line.strip():
                        logger.warning(f"   {line.strip()}")
            
            return_code = process.poll()
            
            if return_code == 0:
                logger.info(f"‚úÖ Fine-tuning –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ: {output_path}")
                return True, f"–ú–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–µ–Ω–∞: {output_path}"
            else:
                error_msg = '\n'.join(stderr_lines[-10:])  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å—Ç—Ä–æ–∫ –æ—à–∏–±–æ–∫
                return False, f"–û—à–∏–±–∫–∞ fine-tuning: {error_msg}"
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ fine-tuning: {e}", exc_info=True)
            return False, f"–û—à–∏–±–∫–∞: {str(e)}"
    
    async def optimize_model_speed(self, model_path: str, quantization: str = "Q4_K_M") -> Tuple[bool, str]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
        
        Args:
            model_path: –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
            quantization: –£—Ä–æ–≤–µ–Ω—å –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (Q4_K_M, Q6_K, Q8_0)
        
        Returns:
            (success, message)
        """
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º llama.cpp –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            # –ò–ª–∏ mlx-lm –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
            logger.info(f"‚ö° –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ {model_path} –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏...")
            
            # –î–ª—è MLX –º–æ–¥–µ–ª–µ–π –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –æ–±—ã—á–Ω–æ —É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–æ
            # –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ
            return True, f"–ú–æ–¥–µ–ª—å —É–∂–µ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (MLX –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—É—é –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ)"
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏: {e}")
            return False, f"–û—à–∏–±–∫–∞: {str(e)}"
    
    async def create_finetuning_pipeline(
        self,
        model_name: str,
        include_style_patterns: bool = True,
        include_anti_hallucination: bool = False,
        include_knowledge_base: bool = False  # ‚ö†Ô∏è –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é FALSE - –Ω–µ —Å–æ–±–∏—Ä–∞–µ–º —Ñ–∞–∫—Ç—ã!
    ) -> Dict:
        """
        –°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω—ã–π pipeline –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        
        ‚ö†Ô∏è –í–ê–ñ–ù–û: –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç–∏–ª—è, –ù–ï —Ñ–∞–∫—Ç—ã!
        –§–∞–∫—Ç—ã —É–∂–µ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ RAG.
        
        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
            include_style_patterns: –°–æ–±–∏—Ä–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç–∏–ª—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è: True)
            include_anti_hallucination: –°–æ–±–∏—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ—Ç–∏–≤ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π
            include_knowledge_base: –°–æ–±–∏—Ä–∞—Ç—å —Ñ–∞–∫—Ç—ã –∏–∑ –±–∞–∑—ã (–ù–ï —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è!)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
        """
        results = {
            "model": model_name,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "steps": [],
            "warning": "‚ö†Ô∏è –î–æ–æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö —Å—Ç–∏–ª—è, –ù–ï –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö (—Ñ–∞–∫—Ç—ã –≤ RAG)"
        }
        
        # –®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö
        logger.info("üìä –®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è...")
        logger.warning("‚ö†Ô∏è –°–æ–±–∏—Ä–∞–µ–º –¢–û–õ–¨–ö–û –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç–∏–ª—è, –ù–ï —Ñ–∞–∫—Ç—ã (—Ñ–∞–∫—Ç—ã —É–∂–µ –≤ RAG)")
        training_data = []
        
        if include_style_patterns:
            style_data = await self.collect_style_patterns()
            training_data.extend(style_data)
            results["steps"].append({
                "step": "collect_style_patterns",
                "status": "success",
                "count": len(style_data),
                "note": "–ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç–∏–ª—è (–ù–ï —Ñ–∞–∫—Ç—ã)"
            })
        
        if include_knowledge_base:
            logger.warning("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –°–æ–±–∏—Ä–∞–µ–º —Ñ–∞–∫—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. –≠—Ç–æ –ù–ï —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è - —Ñ–∞–∫—Ç—ã —É–∂–µ –≤ RAG!")
            kb_data = await self.collect_training_data_from_knowledge_base()
            training_data.extend(kb_data)
            results["steps"].append({
                "step": "collect_knowledge_base",
                "status": "warning",
                "count": len(kb_data),
                "note": "‚ö†Ô∏è –§–∞–∫—Ç—ã - –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å RAG!"
            })
        
        if include_anti_hallucination:
            ah_data = await self.collect_anti_hallucination_data()
            training_data.extend(ah_data)
            results["steps"].append({
                "step": "collect_anti_hallucination",
                "status": "success",
                "count": len(ah_data)
            })
        
        if not training_data:
            results["status"] = "error"
            results["message"] = "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
            return results
        
        # –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        logger.info("üìù –®–∞–≥ 2: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
        dataset_file = f"{model_name}_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        dataset_path = self.prepare_training_dataset(training_data, dataset_file)
        results["steps"].append({
            "step": "prepare_dataset",
            "status": "success",
            "file": dataset_path,
            "count": len(training_data)
        })
        
        # –®–∞–≥ 3: Fine-tuning
        logger.info("üöÄ –®–∞–≥ 3: –ó–∞–ø—É—Å–∫ fine-tuning...")
        base_model = self._get_model_path(model_name)
        output_model = f"{model_name}_finetuned"
        
        success, message = await self.fine_tune_mlx_model(
            base_model=base_model,
            training_data_path=dataset_path,
            output_model_name=output_model
        )
        
        results["steps"].append({
            "step": "fine_tuning",
            "status": "success" if success else "error",
            "message": message
        })
        
        if success:
            results["status"] = "success"
            results["output_model"] = output_model
        else:
            results["status"] = "error"
            results["message"] = message
        
        return results
    
    def _get_model_path(self, model_name: str) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏"""
        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ MLX –º–æ–¥–µ–ª—è—Ö
        mlx_path = os.path.join(self.mlx_models_dir, model_name)
        if os.path.exists(mlx_path):
            return mlx_path
        
        # –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º HuggingFace ID
        model_map = {
            "qwen2.5-coder:32b": "mlx-community/Qwen2.5-Coder-32B-Instruct-Q8",
            "deepseek-r1-distill-llama:70b": "mlx-community/DeepSeek-R1-Distill-Llama-70B-Q6",
            "llama3.3:70b": "mlx-community/Llama-3.3-70B-Instruct-Q6",
            "phi3.5:3.8b": "mlx-community/Phi-3.5-mini-instruct-Q4",
        }
        
        return model_map.get(model_name, model_name)


async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    tuner = ModelFineTuner()
    
    # ‚úÖ –ü–†–ê–í–ò–õ–¨–ù–û: –î–æ–æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–∞—Ö —Å—Ç–∏–ª—è (–ù–ï –Ω–∞ —Ñ–∞–∫—Ç–∞—Ö!)
    results = await tuner.create_finetuning_pipeline(
        model_name="qwen2.5-coder:32b",
        include_style_patterns=True,      # ‚úÖ –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∏–ª—å
        include_anti_hallucination=False, # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ
        include_knowledge_base=False      # ‚ùå –ù–ï —Å–æ–±–∏—Ä–∞–µ–º —Ñ–∞–∫—Ç—ã (–æ–Ω–∏ –≤ RAG!)
    )
    
    print(json.dumps(results, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    asyncio.run(main())
