"""
–°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏
–í—Å–µ –∞–≥–µ–Ω—Ç—ã –∏ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è –∑–Ω–∞—é—Ç –í–°–Å: –º–æ–¥–µ–ª–∏, —Å–∫—Ä–∏–ø—Ç—ã, –≤–Ω–µ–¥—Ä–µ–Ω–∏—è, –∏–∑–º–µ–Ω–µ–Ω–∏—è
Singularity 10.0: prompt_change_log –¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ—Ç–∫–∞—Ç–∞
"""
import asyncio
import hashlib
import os
import json
import httpx
import logging
import subprocess
from datetime import datetime
from typing import Dict, List, Optional, Any
from pathlib import Path

logger = logging.getLogger(__name__)

# Database connection
try:
    import asyncpg
    ASYNCPG_AVAILABLE = True
except ImportError:
    asyncpg = None
    ASYNCPG_AVAILABLE = False


class CorporationKnowledgeSystem:
    """
    –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏.
    –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ:
    - –î–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö (MLX –∏ Ollama)
    - –í—Å–µ—Ö —Å–∫—Ä–∏–ø—Ç–∞—Ö
    - –í–Ω–µ–¥—Ä–µ–Ω–∏—è—Ö –∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è—Ö
    - –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã
    """
    
    def __init__(self, db_url: Optional[str] = None):
        self.db_url = db_url or os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
        self.project_root = Path(__file__).parent.parent.parent
        self.scripts_dir = self.project_root / "scripts"
        self.knowledge_os_dir = self.project_root / "knowledge_os"
        
        # URLs –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π
        self.ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        self.mlx_url = os.getenv('MLX_API_URL', 'http://localhost:11435')
        
    async def discover_ollama_models(self) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏ Ollama"""
        models = []
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.ollama_url}/api/tags")
                if response.status_code == 200:
                    data = response.json()
                    for model in data.get('models', []):
                        models.append({
                            'name': model.get('name', ''),
                            'size': model.get('size', 0),
                            'modified': model.get('modified_at', ''),
                            'source': 'ollama'
                        })
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π Ollama: {e}")
        
        return models
    
    async def discover_mlx_models(self) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –≤—Å–µ –º–æ–¥–µ–ª–∏ MLX"""
        models = []
        
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.mlx_url}/v1/models")
                if response.status_code == 200:
                    data = response.json()
                    for model in data.get('data', []):
                        models.append({
                            'name': model.get('id', ''),
                            'source': 'mlx'
                        })
        except Exception as e:
            logger.debug(f"MLX API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        mlx_dirs = [
            Path.home() / "mlx-models",
            Path.home() / ".mlx_models",
            Path.home() / ".cache" / "huggingface" / "hub"
        ]
        
        for mlx_dir in mlx_dirs:
            if mlx_dir.exists():
                for model_dir in mlx_dir.iterdir():
                    if model_dir.is_dir():
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏
                        if any(model_dir.glob("*.safetensors")) or any(model_dir.glob("*.bin")):
                            models.append({
                                'name': model_dir.name,
                                'path': str(model_dir),
                                'source': 'mlx_local'
                            })
        
        return models
    
    def discover_scripts(self) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –≤—Å–µ —Å–∫—Ä–∏–ø—Ç—ã –≤ –ø—Ä–æ–µ–∫—Ç–µ"""
        scripts = []
        
        # –ò—â–µ–º –≤—Å–µ .sh –∏ .py —Ñ–∞–π–ª—ã –≤ scripts/
        if self.scripts_dir.exists():
            for script_file in self.scripts_dir.rglob("*.sh"):
                scripts.append({
                    'name': script_file.name,
                    'path': str(script_file.relative_to(self.project_root)),
                    'type': 'shell',
                    'size': script_file.stat().st_size,
                    'modified': datetime.fromtimestamp(script_file.stat().st_mtime).isoformat()
                })
            
            for script_file in self.scripts_dir.rglob("*.py"):
                scripts.append({
                    'name': script_file.name,
                    'path': str(script_file.relative_to(self.project_root)),
                    'type': 'python',
                    'size': script_file.stat().st_size,
                    'modified': datetime.fromtimestamp(script_file.stat().st_mtime).isoformat()
                })
        
        return scripts
    
    async def discover_recent_changes(self) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–∏—Ç—å –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –ø—Ä–æ–µ–∫—Ç–µ"""
        changes = []
        
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º git –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π
            result = subprocess.run(
                ['git', 'log', '--since', '7 days ago', '--pretty=format:%H|%an|%ae|%ad|%s', '--date=iso'],
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                for line in result.stdout.strip().split('\n'):
                    if line:
                        parts = line.split('|', 4)
                        if len(parts) == 5:
                            changes.append({
                                'commit': parts[0],
                                'author': parts[1],
                                'email': parts[2],
                                'date': parts[3],
                                'message': parts[4]
                            })
        except Exception as e:
            logger.debug(f"Git –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        
        return changes
    
    async def update_corporation_knowledge(self) -> Dict[str, Any]:
        """–û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏"""
        logger.info("üîÑ –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏...")
        
        # –û–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ–º –≤—Å–µ
        ollama_models = await self.discover_ollama_models()
        mlx_models = await self.discover_mlx_models()
        scripts = self.discover_scripts()
        recent_changes = await self.discover_recent_changes()
        
        knowledge = {
            'timestamp': datetime.now().isoformat(),
            'ollama_models': ollama_models,
            'mlx_models': mlx_models,
            'scripts': scripts,
            'recent_changes': recent_changes,
            'total_ollama_models': len(ollama_models),
            'total_mlx_models': len(mlx_models),
            'total_scripts': len(scripts)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –ë–î —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞
        if ASYNCPG_AVAILABLE:
            try:
                # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º get_embedding: —Å–Ω–∞—á–∞–ª–∞ semantic_cache (–ª—ë–≥–∫–∏–π, Ollama), —á—Ç–æ–±—ã –Ω–µ —Ç—è–Ω—É—Ç—å app.main (MCP, redis, pool) ‚Äî –º–µ–Ω—å—à–µ –ø–∞–º—è—Ç–∏ –≤ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–µ
                get_embedding = None
                try:
                    from semantic_cache import get_embedding as _get_embedding
                    get_embedding = _get_embedding
                except ImportError:
                    try:
                        from app.main import get_embedding
                    except ImportError:
                        try:
                            from app.enhanced_search import get_embedding
                        except ImportError:
                            logger.warning("‚ö†Ô∏è get_embedding –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –±–µ–∑ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –ø—É–ª–∞
                conn = await asyncpg.connect(self.db_url, command_timeout=30)
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –¥–æ–º–µ–Ω System
                    domain_id = await conn.fetchval("""
                        SELECT id FROM domains WHERE name = 'System' LIMIT 1
                    """)
                    if not domain_id:
                        domain_id = await conn.fetchval("""
                            INSERT INTO domains (name) VALUES ('System') RETURNING id
                        """)
                    
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏
                    await conn.execute("""
                        DELETE FROM knowledge_nodes
                        WHERE metadata->>'source' = 'corporation_knowledge_system'
                    """)
                    
                    saved_count = 0
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å Ollama –æ—Ç–¥–µ–ª—å–Ω–æ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
                    for model in ollama_models:
                        content = f"–ú–æ–¥–µ–ª—å Ollama: {model['name']}. –†–∞–∑–º–µ—Ä: {model.get('size', 0) / 1024 / 1024 / 1024:.2f} GB. –î–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏."
                        embedding = None
                        if get_embedding:
                            try:
                                embedding = await get_embedding(content)
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –º–æ–¥–µ–ª–∏ {model['name']}: {e}")
                        
                        await conn.execute("""
                            INSERT INTO knowledge_nodes (domain_id, content, embedding, confidence_score, metadata, is_verified)
                            VALUES ($1, $2, $3, 1.0, $4, true)
                        """, domain_id, content, str(embedding) if embedding else None, json.dumps({
                            'source': 'corporation_knowledge_system',
                            'type': 'ollama_model',
                            'model_name': model['name'],
                            'size': model.get('size', 0),
                            'timestamp': knowledge['timestamp']
                        }))
                        saved_count += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å MLX –æ—Ç–¥–µ–ª—å–Ω–æ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
                    for model in mlx_models:
                        content = f"–ú–æ–¥–µ–ª—å MLX: {model['name']}. –ü—É—Ç—å: {model.get('path', 'N/A')}. –î–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏."
                        embedding = None
                        if get_embedding:
                            try:
                                embedding = await get_embedding(content)
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è MLX –º–æ–¥–µ–ª–∏ {model['name']}: {e}")
                        
                        await conn.execute("""
                            INSERT INTO knowledge_nodes (domain_id, content, embedding, confidence_score, metadata, is_verified)
                            VALUES ($1, $2, $3, 1.0, $4, true)
                        """, domain_id, content, str(embedding) if embedding else None, json.dumps({
                            'source': 'corporation_knowledge_system',
                            'type': 'mlx_model',
                            'model_name': model['name'],
                            'path': model.get('path'),
                            'timestamp': knowledge['timestamp']
                        }))
                        saved_count += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –≥—Ä—É–ø–ø–∞–º–∏ –ø–æ —Ç–∏–ø–∞–º —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º–∏
                    scripts_by_type = {}
                    for script in scripts:
                        script_type = script['type']
                        if script_type not in scripts_by_type:
                            scripts_by_type[script_type] = []
                        scripts_by_type[script_type].append(script)
                    
                    for script_type, type_scripts in scripts_by_type.items():
                        scripts_list = "\n".join([f"- {s['path']}" for s in type_scripts[:20]])
                        content = f"–î–æ—Å—Ç—É–ø–Ω—ã–µ {script_type} —Å–∫—Ä–∏–ø—Ç—ã –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ({len(type_scripts)}):\n{scripts_list}"
                        embedding = None
                        if get_embedding:
                            try:
                                embedding = await get_embedding(content)
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Å–∫—Ä–∏–ø—Ç–æ–≤ {script_type}: {e}")
                        
                        await conn.execute("""
                            INSERT INTO knowledge_nodes (domain_id, content, embedding, confidence_score, metadata, is_verified)
                            VALUES ($1, $2, $3, 1.0, $4, true)
                        """, domain_id, content, str(embedding) if embedding else None, json.dumps({
                            'source': 'corporation_knowledge_system',
                            'type': f'scripts_{script_type}',
                            'count': len(type_scripts),
                            'timestamp': knowledge['timestamp']
                        }))
                        saved_count += 1
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–¥–∞–≤–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
                    if recent_changes:
                        changes_text = "\n".join([
                            f"- {c.get('date', '')[:19]}: {c.get('message', '')[:100]}"
                            for c in recent_changes[:10]
                        ])
                        content = f"–ù–µ–¥–∞–≤–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ({len(recent_changes)}):\n{changes_text}"
                        embedding = None
                        if get_embedding:
                            try:
                                embedding = await get_embedding(content)
                            except Exception as e:
                                logger.debug(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è –∏–∑–º–µ–Ω–µ–Ω–∏–π: {e}")
                        
                        await conn.execute("""
                            INSERT INTO knowledge_nodes (domain_id, content, embedding, confidence_score, metadata, is_verified)
                            VALUES ($1, $2, $3, 1.0, $4, true)
                        """, domain_id, content, str(embedding) if embedding else None, json.dumps({
                            'source': 'corporation_knowledge_system',
                            'type': 'recent_changes',
                            'count': len(recent_changes),
                            'timestamp': knowledge['timestamp']
                        }))
                        saved_count += 1
                    
                    # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â–∏–π —Å–≤–æ–¥–Ω—ã–π —É–∑–µ–ª –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞
                    summary_content = f"""–ê–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω–æ {knowledge['timestamp']}):
- –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π Ollama: {len(ollama_models)}
- –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π MLX: {len(mlx_models)}
- –î–æ—Å—Ç—É–ø–Ω–æ —Å–∫—Ä–∏–ø—Ç–æ–≤: {len(scripts)}
- –ù–µ–¥–∞–≤–Ω–∏—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π: {len(recent_changes)}
–í—Å–µ –∑–Ω–∞–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ search_knowledge."""
                    
                    embedding = None
                    if get_embedding:
                        try:
                            embedding = await get_embedding(summary_content)
                        except Exception as e:
                            logger.debug(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Å–≤–æ–¥–∫–∏: {e}")
                    
                    await conn.execute("""
                        INSERT INTO knowledge_nodes (domain_id, content, embedding, confidence_score, metadata, is_verified)
                        VALUES ($1, $2, $3, 1.0, $4, true)
                    """, domain_id, summary_content, str(embedding) if embedding else None, json.dumps({
                        'source': 'corporation_knowledge_system',
                        'type': 'system_summary',
                        'version': '1.0',
                        'timestamp': knowledge['timestamp']
                    }))
                    saved_count += 1
                    
                    logger.info(f"‚úÖ –ó–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π: {saved_count} —É–∑–ª–æ–≤ ({len(ollama_models)} Ollama, {len(mlx_models)} MLX, {len(scripts)} —Å–∫—Ä–∏–ø—Ç–æ–≤)")
                finally:
                    await conn.close()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –≤ –ë–î: {e}", exc_info=True)
        
        return knowledge
    
    def generate_system_prompt_update(self, knowledge: Dict[str, Any], top_insights: Optional[List[Dict[str, Any]]] = None, lessons_learned: Optional[List[Dict[str, Any]]] = None) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ system prompt —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏ (Singularity 10.0: + —Ç–æ–ø-–∏–Ω—Å–∞–π—Ç—ã)"""
        
        prompt = """
üìö –ê–ö–¢–£–ê–õ–¨–ù–´–ï –ó–ù–ê–ù–ò–Ø –ö–û–†–ü–û–†–ê–¶–ò–ò (–æ–±–Ω–æ–≤–ª–µ–Ω–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ {}):

ü§ñ –î–û–°–¢–£–ü–ù–´–ï –ú–û–î–ï–õ–ò:

Ollama –º–æ–¥–µ–ª–∏ ({}):
""".format(knowledge.get('timestamp', ''), knowledge['total_ollama_models'])
        
        for model in knowledge['ollama_models']:
            size_info = f" ({model.get('size', 0) / 1024 / 1024 / 1024:.1f} GB)" if model.get('size') else ""
            prompt += f"- {model['name']}{size_info}\n"
        
        prompt += f"\nMLX –º–æ–¥–µ–ª–∏ ({knowledge['total_mlx_models']}):\n"
        for model in knowledge['mlx_models']:
            path_info = f" ({model.get('path', '')})" if model.get('path') else ""
            prompt += f"- {model['name']}{path_info}\n"
        
        prompt += f"\nüìú –î–û–°–¢–£–ü–ù–´–ï –°–ö–†–ò–ü–¢–´ ({knowledge['total_scripts']}):\n"
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        scripts_by_type = {}
        for script in knowledge['scripts']:
            script_type = script['type']
            if script_type not in scripts_by_type:
                scripts_by_type[script_type] = []
            scripts_by_type[script_type].append(script)
        
        for script_type, scripts in scripts_by_type.items():
            prompt += f"\n{script_type.upper()} —Å–∫—Ä–∏–ø—Ç—ã ({len(scripts)}):\n"
            for script in scripts[:20]:  # –ü–µ—Ä–≤—ã–µ 20 –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞
                prompt += f"  - {script['path']}\n"
        
        if knowledge['recent_changes']:
            prompt += f"\nüîÑ –ù–ï–î–ê–í–ù–ò–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø ({len(knowledge['recent_changes'])}):\n"
            for change in knowledge['recent_changes'][:10]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10
                prompt += f"- {change.get('date', '')[:19]}: {change.get('message', '')[:100]}\n"
        
        # Singularity 10.0: —Ç–æ–ø lessons learned –∏–∑ adaptive_learning_logs
        if lessons_learned:
            prompt += "\nüìñ LESSONS LEARNED (adaptive_learning_logs):\n"
            for ll in lessons_learned[:5]:
                prompt += f"- {ll.get('learned_insight', '')[:300]}\n"
        # Singularity 10.0: —Ç–æ–ø-–∏–Ω—Å–∞–π—Ç—ã –∏–∑ knowledge_nodes (–º–∞–∫—Å 2000 —Å–∏–º–≤–æ–ª–æ–≤ –Ω–∞ –±–ª–æ–∫)
        if top_insights:
            insights_block = "\nüí° –¢–û–ü-–ò–ù–°–ê–ô–¢–´ –ö–û–†–ü–û–†–ê–¶–ò–ò (–∏–∑ knowledge_nodes):\n"
            for ins in top_insights:
                if len(insights_block) >= 2000:
                    break
                content = (ins.get('content') or '')[:350]
                if content:
                    domain_name = ins.get('domain_name') or ''
                    prefix = f"[{domain_name}] " if domain_name else ""
                    line = f"- {prefix}{content}\n"
                    if len(insights_block) + len(line) > 2000:
                        line = line[:2000 - len(insights_block) - 3] + "...\n"
                    insights_block += line
            prompt += insights_block
        
        prompt += "\n‚ö†Ô∏è –í–ê–ñ–ù–û: –¢—ã –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω –∑–Ω–∞—Ç—å –∞–∫—Ç—É–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏!"
        prompt += "\nüí° –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –∑–Ω–∞–Ω–∏—è –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –º–æ–¥–µ–ª–µ–π –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤!"
        
        return prompt


async def update_all_agents_knowledge():
    """–û–±–Ω–æ–≤–∏—Ç—å –∑–Ω–∞–Ω–∏—è –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤"""
    system = CorporationKnowledgeSystem()
    knowledge = await system.update_corporation_knowledge()
    
    # –¢–∞–∫–∂–µ –∏–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ (—Å–∏—Å—Ç–µ–º—ã, –ª–æ–≥–∏–∫–∞, —É–º–µ–Ω–∏—è)
    try:
        # –ü—Ä–æ–±—É–µ–º –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω—ã–µ –ø—É—Ç–∏
        try:
            from app.corporation_complete_knowledge import CorporationCompleteKnowledge
        except ImportError:
            # –ü—Ä–æ–±—É–µ–º —á–µ—Ä–µ–∑ knowledge_os –ø—É—Ç—å
            import sys
            knowledge_os_path = os.path.dirname(os.path.dirname(__file__))
            if knowledge_os_path not in sys.path:
                sys.path.insert(0, knowledge_os_path)
            from app.corporation_complete_knowledge import CorporationCompleteKnowledge
        
        complete_extractor = CorporationCompleteKnowledge()
        complete_result = await complete_extractor.extract_all()
        logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–Ω—ã—Ö –∑–Ω–∞–Ω–∏–π –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏: {complete_result['total_extracted']} (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {complete_result['saved_to_db']})")
    except Exception as e:
        logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø–æ–ª–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏: {e}")
    
    # Singularity 10.0: —Ç–æ–ø-–∏–Ω—Å–∞–π—Ç—ã –∏ lessons learned
    top_insights = []
    lessons_learned = []
    if ASYNCPG_AVAILABLE:
        try:
            conn_ins = await asyncpg.connect(system.db_url, command_timeout=10)
            try:
                rows = await conn_ins.fetch("""
                    SELECT k.content, k.confidence_score, d.name as domain_name
                    FROM knowledge_nodes k
                    LEFT JOIN domains d ON k.domain_id = d.id
                    WHERE (k.is_verified = true OR k.confidence_score > 0.7)
                      AND k.created_at > NOW() - INTERVAL '7 days'
                      AND k.metadata->>'source' != 'corporation_knowledge_system'
                    ORDER BY k.confidence_score DESC, k.created_at DESC
                    LIMIT 10
                """)
                top_insights = [dict(r) for r in rows]
                # adaptive_learning_logs (high impact_score)
                ll_rows = await conn_ins.fetch("""
                    SELECT learned_insight, impact_score, learning_type
                    FROM adaptive_learning_logs
                    WHERE impact_score > 0.6
                    ORDER BY impact_score DESC
                    LIMIT 5
                """)
                lessons_learned = [dict(r) for r in ll_rows]
            finally:
                await conn_ins.close()
        except Exception as e:
            logger.debug("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ç–æ–ø-–∏–Ω—Å–∞–π—Ç—ã/lessons: %s", e)
    
    prompt_update = system.generate_system_prompt_update(knowledge, top_insights=top_insights or None, lessons_learned=lessons_learned or None)
    
    # –û–±–Ω–æ–≤–ª—è–µ–º system prompts –≤—Å–µ—Ö –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ë–î
    if ASYNCPG_AVAILABLE:
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –≤–º–µ—Å—Ç–æ –ø—É–ª–∞
            conn = await asyncpg.connect(system.db_url, command_timeout=30)
            try:
                # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–Ω–∞–Ω–∏–π –∏–∑ system_prompt
                experts = await conn.fetch("""
                    SELECT id, name, system_prompt
                    FROM experts
                """)
                
                for expert in experts:
                    system_prompt = expert['system_prompt'] or ""
                    
                    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π (–µ—Å–ª–∏ –µ—Å—Ç—å)
                    lines = system_prompt.split('\n')
                    new_lines = []
                    skip_until_end = False
                    
                    for line in lines:
                        if '–ê–ö–¢–£–ê–õ–¨–ù–´–ï –ó–ù–ê–ù–ò–Ø –ö–û–†–ü–û–†–ê–¶–ò–ò' in line:
                            skip_until_end = True
                        if skip_until_end and line.strip() == '' and new_lines and new_lines[-1].strip() == '':
                            skip_until_end = False
                            continue
                        if not skip_until_end:
                            new_lines.append(line)
                    
                    cleaned_prompt = '\n'.join(new_lines).strip()
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
                    updated_prompt = cleaned_prompt + '\n\n' + prompt_update
                    
                    # Singularity 10.0: –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ ‚Äî backup –ø–µ—Ä–µ–¥ UPDATE
                    old_hash = hashlib.sha256((system_prompt or "").encode()).hexdigest()
                    new_hash = hashlib.sha256(updated_prompt.encode()).hexdigest()
                    if old_hash != new_hash:
                        try:
                            await conn.execute("""
                                INSERT INTO prompt_change_log (expert_id, old_prompt_hash, new_prompt_hash, metadata)
                                VALUES ($1, $2, $3, $4::jsonb)
                            """, expert['id'], old_hash, new_hash, json.dumps({"source": "update_all_agents_knowledge"}))
                        except Exception as e:
                            logger.debug("prompt_change_log insert skipped (table may not exist): %s", e)
                    
                    await conn.execute("""
                        UPDATE experts
                        SET system_prompt = $1
                        WHERE id = $2
                    """, updated_prompt, expert['id'])
                
                logger.info(f"‚úÖ System prompts –≤—Å–µ—Ö {len(experts)} –∞–≥–µ–Ω—Ç–æ–≤ –æ–±–Ω–æ–≤–ª–µ–Ω—ã —Å –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏")
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è system prompts: {e}")
    
    return knowledge


if __name__ == "__main__":
    asyncio.run(update_all_agents_knowledge())
