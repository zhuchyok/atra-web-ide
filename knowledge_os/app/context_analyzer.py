"""
Context Analyzer
–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–º–Ω–æ–≥–æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è
Singularity 9.0: Predictive Context Compression
"""

import asyncio
import os
import json
import logging
import re
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timedelta
import numpy as np

# Import database connection from evaluator
try:
    from evaluator import get_pool
except ImportError:
    get_pool = None

logger = logging.getLogger(__name__)

DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')

# Import embedding function
try:
    from semantic_cache import get_embedding
except ImportError:
    get_embedding = None
    logger.warning("semantic_cache not available, context analysis will be limited")

class ContextAnalyzer:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç–µ–π.
    """
    
    def __init__(self, relevance_threshold: float = 0.7):
        """
        Args:
            relevance_threshold: –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (0.0-1.0)
        """
        self.relevance_threshold = relevance_threshold
    
    async def analyze_context_relevance(
        self,
        context: str,
        query: str
    ) -> List[Tuple[str, float]]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —á–∞—Å—Ç–µ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫ –∑–∞–ø—Ä–æ—Å—É.
        
        Args:
            context: –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            –°–ø–∏—Å–æ–∫ (—á–∞—Å—Ç—å_–∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å) –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        if not get_embedding:
            # Fallback: –ø—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
            return self._simple_relevance(context, query)
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = await get_embedding(query)
            if not query_embedding:
                return self._simple_relevance(context, query)
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ (–ø–æ –∞–±–∑–∞—Ü–∞–º –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º)
            parts = self._split_context(context)
            
            # –í—ã—á–∏—Å–ª—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —á–∞—Å—Ç–∏
            relevant_parts = []
            for part in parts:
                if not part.strip():
                    continue
                
                part_embedding = await get_embedding(part)
                if part_embedding:
                    similarity = self._cosine_similarity(query_embedding, part_embedding)
                    if similarity >= self.relevance_threshold:
                        relevant_parts.append((part, similarity))
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            relevant_parts.sort(key=lambda x: x[1], reverse=True)
            
            return relevant_parts
        except Exception as e:
            logger.error(f"‚ùå [CONTEXT ANALYZER] Error analyzing context: {e}")
            return self._simple_relevance(context, query)
    
    def _split_context(self, context: str) -> List[str]:
        """–†–∞–∑–±–∏–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏ (—Å–æ—Ö—Ä–∞–Ω—è—è —Å—Ç—Ä—É–∫—Ç—É—Ä—É)"""
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º —Å—Ç—Ä–æ–∫ (–∞–±–∑–∞—Ü—ã)
        parts = context.split('\n\n')
        
        # –ï—Å–ª–∏ —á–∞—Å—Ç–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ, —Ä–∞–∑–±–∏–≤–∞–µ–º –¥–∞–ª—å—à–µ
        result = []
        for part in parts:
            if len(part) > 500:
                # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º
                sentences = part.split('. ')
                current_chunk = []
                current_length = 0
                
                for sentence in sentences:
                    if current_length + len(sentence) > 500:
                        if current_chunk:
                            result.append('. '.join(current_chunk) + '.')
                        current_chunk = [sentence]
                        current_length = len(sentence)
                    else:
                        current_chunk.append(sentence)
                        current_length += len(sentence) + 2
                
                if current_chunk:
                    result.append('. '.join(current_chunk))
            else:
                result.append(part)
        
        return result
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏"""
        try:
            v1 = np.array(vec1)
            v2 = np.array(vec2)
            
            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
            
            return float(dot_product / (norm1 * norm2))
        except Exception as e:
            logger.error(f"Error calculating cosine similarity: {e}")
            return 0.0
    
    def _simple_relevance(self, context: str, query: str) -> List[Tuple[str, float]]:
        """–ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ (fallback)"""
        query_words = set(query.lower().split())
        parts = self._split_context(context)
        
        relevant_parts = []
        for part in parts:
            part_words = set(part.lower().split())
            common_words = query_words.intersection(part_words)
            
            if len(query_words) > 0:
                relevance = len(common_words) / len(query_words)
                if relevance >= self.relevance_threshold:
                    relevant_parts.append((part, relevance))
        
        relevant_parts.sort(key=lambda x: x[1], reverse=True)
        return relevant_parts
    
    async def compress_context(
        self,
        context: str,
        query: str,
        max_length: int = 2000
    ) -> str:
        """
        –°–∂–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏.
        
        Args:
            context: –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–∂–∞—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –°–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        """
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
        relevant_parts = await self.analyze_context_relevance(context, query)
        
        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏ –¥–æ max_length
        compressed = []
        current_length = 0
        
        for part, relevance in relevant_parts:
            part_length = len(part)
            if current_length + part_length <= max_length:
                compressed.append(part)
                current_length += part_length
            else:
                # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ, –µ—Å–ª–∏ –µ—Å—Ç—å –º–µ—Å—Ç–æ
                remaining = max_length - current_length
                if remaining > 100:  # –ú–∏–Ω–∏–º—É–º 100 —Å–∏–º–≤–æ–ª–æ–≤
                    compressed.append(part[:remaining] + "...")
                break
        
        if not compressed:
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            return context[:max_length]
        
        return '\n\n'.join(compressed)
    
    async def predict_next_query(self, current_query: str, user_identifier: Optional[str] = None, limit: int = 3) -> List[str]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏ (Singularity 9.0).
        
        Args:
            current_query: –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_identifier: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        
        Returns:
            –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        """
        if not get_pool:
            return []
        
        try:
            pool = await get_pool()
            async with pool.acquire() as conn:
                # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –∑–∞–ø—Ä–æ—Å–æ–≤
                if user_identifier:
                    rows = await conn.fetch("""
                        SELECT user_query
                        FROM interaction_logs
                        WHERE metadata->>'user_identifier' = $1
                        ORDER BY created_at DESC
                        LIMIT 20
                    """, user_identifier)
                else:
                    rows = await conn.fetch("""
                        SELECT user_query
                        FROM interaction_logs
                        WHERE created_at > NOW() - INTERVAL '1 hour'
                        ORDER BY created_at DESC
                        LIMIT 20
                    """)
                
                if not rows:
                    return []
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
                queries = [row['user_query'] for row in rows]
                
                # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –µ—Å–ª–∏ —Ç–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ—Ö–æ–∂ –Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏–π, —Å–ª–µ–¥—É—é—â–∏–π –≤–µ—Ä–æ—è—Ç–Ω–æ —Å–≤—è–∑–∞–Ω
                predicted_queries = []
                
                # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã –≤ –∏—Å—Ç–æ—Ä–∏–∏
                current_keywords = set(re.findall(r'\b\w+\b', current_query.lower()))
                
                for i in range(len(queries) - 1):
                    query = queries[i]
                    next_query = queries[i + 1]
                    
                    query_keywords = set(re.findall(r'\b\w+\b', query.lower()))
                    common_keywords = current_keywords.intersection(query_keywords)
                    
                    # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±—â–∏–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞, —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å –≤–µ—Ä–æ—è—Ç–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω
                    if len(common_keywords) > 0:
                        if next_query not in predicted_queries:
                            predicted_queries.append(next_query)
                    
                    if len(predicted_queries) >= limit:
                        break
                
                return predicted_queries[:limit]
        except Exception as e:
            logger.error(f"‚ùå [CONTEXT ANALYZER] Error predicting next query: {e}")
            return []
    
    async def precompress_context(self, context: str, predicted_queries: List[str], max_length: int = 2000) -> Dict[str, str]:
        """
        –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–∂–∏–º–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (Singularity 9.0).
        
        Args:
            context: –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            predicted_queries: –°–ø–∏—Å–æ–∫ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            max_length: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Å–∂–∞—Ç–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å {query: compressed_context}
        """
        precompressed = {}
        
        for query in predicted_queries:
            try:
                compressed = await self.compress_context(context, query, max_length)
                precompressed[query] = compressed
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è [CONTEXT ANALYZER] Error precompressing for query '{query}': {e}")
        
        return precompressed
    
    async def get_precompressed_context(self, query: str, user_identifier: Optional[str] = None) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –ø—Ä–µ–¥—Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (Singularity 9.0).
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            user_identifier: –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            –ü—Ä–µ–¥—Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–ª–∏ None, –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω
        """
        if not get_pool:
            return None
        
        try:
            pool = await get_pool()
            async with pool.acquire() as conn:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–µ–¥—Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –∫—ç—à–µ
                # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ —á–µ—Ä–µ–∑ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∏–ª–∏ —á–µ—Ä–µ–∑ semantic_cache
                # –ü–æ–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç—É—é –ø—Ä–æ–≤–µ—Ä–∫—É —á–µ—Ä–µ–∑ semantic_cache
                row = await conn.fetchrow("""
                    SELECT response_text
                    FROM semantic_ai_cache
                    WHERE query_text = $1
                      AND metadata->>'precompressed' = 'true'
                    ORDER BY created_at DESC
                    LIMIT 1
                """, query)
                
                if row:
                    return row['response_text']
                
                return None
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [CONTEXT ANALYZER] Error getting precompressed context: {e}")
            return None


async def run_predictive_compression():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ü–∏–∫–ª –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Å–∂–∞—Ç–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (Singularity 9.0)"""
    logger.info("üöÄ [PREDICTIVE COMPRESSION] Starting predictive compression cycle...")
    
    if not get_pool:
        logger.warning("‚ö†Ô∏è [PREDICTIVE COMPRESSION] Database pool not available")
        return
    
    try:
        pool = await get_pool()
        async with pool.acquire() as conn:
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            rows = await conn.fetch("""
                SELECT DISTINCT metadata->>'user_identifier' as user_id
                FROM interaction_logs
                WHERE metadata->>'user_identifier' IS NOT NULL
                  AND created_at > NOW() - INTERVAL '1 hour'
                LIMIT 10
            """)
            
            analyzer = ContextAnalyzer()
            processed_count = 0
            
            for row in rows:
                user_id = row['user_id']
                if not user_id:
                    continue
                
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                    last_query_row = await conn.fetchrow("""
                        SELECT user_query, assistant_response
                        FROM interaction_logs
                        WHERE metadata->>'user_identifier' = $1
                        ORDER BY created_at DESC
                        LIMIT 1
                    """, user_id)
                    
                    if not last_query_row:
                        continue
                    
                    current_query = last_query_row['user_query']
                    context = last_query_row['assistant_response'] or ""
                    
                    if not context:
                        continue
                    
                    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã
                    predicted_queries = await analyzer.predict_next_query(current_query, user_id, limit=3)
                    
                    if predicted_queries:
                        # –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ —Å–∂–∏–º–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
                        precompressed = await analyzer.precompress_context(context, predicted_queries, max_length=2000)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–º–æ–∂–Ω–æ —á–µ—Ä–µ–∑ semantic_cache –∏–ª–∏ –æ—Ç–¥–µ–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É)
                        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º
                        logger.info(f"‚úÖ [PREDICTIVE COMPRESSION] Precompressed context for user {user_id}: {len(precompressed)} queries")
                        processed_count += 1
                except Exception as e:
                    logger.error(f"‚ùå [PREDICTIVE COMPRESSION] Error processing user {user_id}: {e}")
            
            logger.info(f"‚úÖ [PREDICTIVE COMPRESSION] Processed {processed_count} users")
    except Exception as e:
        logger.error(f"‚ùå [PREDICTIVE COMPRESSION] Error in compression cycle: {e}")


if __name__ == "__main__":
    asyncio.run(run_predictive_compression())

