"""
Performance Optimizer: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—á–∏—Å–ª–µ–Ω–∏–π
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç—è–∂–µ–ª—ã—Ö –∑–∞–¥–∞—á
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
"""

import asyncio
import os
import json
import asyncpg
import redis.asyncio as redis
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Callable
from functools import wraps
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
CACHE_TTL = 3600  # 1 —á–∞—Å
CACHE_PREFIX = "knowledge_os:cache:"


class QueryCache:
    """–ö–ª–∞—Å—Å –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, redis_url: str = REDIS_URL):
        self.redis_url = redis_url
        self.redis_client: Optional[redis.Redis] = None
    
    async def get_redis(self) -> redis.Redis:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ Redis –∫–ª–∏–µ–Ω—Ç–∞"""
        if self.redis_client is None:
            self.redis_client = await redis.from_url(self.redis_url, decode_responses=True)
        return self.redis_client
    
    def _make_cache_key(self, query: str, params: tuple = ()) -> str:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª—é—á–∞ –∫—ç—à–∞ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        key_data = f"{query}:{json.dumps(params, sort_keys=True)}"
        key_hash = hashlib.md5(key_data.encode()).hexdigest()
        return f"{CACHE_PREFIX}{key_hash}"
    
    async def get(self, query: str, params: tuple = ()) -> Optional[Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∏–∑ –∫—ç—à–∞"""
        try:
            rd = await self.get_redis()
            cache_key = self._make_cache_key(query, params)
            cached = await rd.get(cache_key)
            
            if cached:
                return json.loads(cached)
            return None
        except Exception as e:
            logger.error(f"Cache get error: {e}")
            return None
    
    async def set(self, query: str, params: tuple, result: Any, ttl: int = CACHE_TTL) -> bool:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –≤ –∫—ç—à"""
        try:
            rd = await self.get_redis()
            cache_key = self._make_cache_key(query, params)
            await rd.setex(cache_key, ttl, json.dumps(result, default=str))
            return True
        except Exception as e:
            logger.error(f"Cache set error: {e}")
            return False
    
    async def invalidate(self, pattern: str) -> int:
        """–ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É"""
        try:
            rd = await self.get_redis()
            keys = await rd.keys(f"{CACHE_PREFIX}*{pattern}*")
            if keys:
                return await rd.delete(*keys)
            return 0
        except Exception as e:
            logger.error(f"Cache invalidate error: {e}")
            return 0
    
    async def clear_all(self) -> bool:
        """–û—á–∏—Å—Ç–∫–∞ –≤—Å–µ–≥–æ –∫—ç—à–∞"""
        try:
            rd = await self.get_redis()
            keys = await rd.keys(f"{CACHE_PREFIX}*")
            if keys:
                await rd.delete(*keys)
            return True
        except Exception as e:
            logger.error(f"Cache clear error: {e}")
            return False


def cached_query(ttl: int = CACHE_TTL):
    """–î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    def decorator(func: Callable):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            cache = QueryCache()
            
            # –°–æ–∑–¥–∞–µ–º –∫–ª—é—á –∫—ç—à–∞ –∏–∑ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
            cache_key = f"{func.__name__}:{json.dumps(args, sort_keys=True)}:{json.dumps(kwargs, sort_keys=True)}"
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–∑ –∫—ç—à–∞
            cached = await cache.get(cache_key)
            if cached is not None:
                logger.debug(f"Cache hit: {func.__name__}")
                return cached
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
            result = await func(*args, **kwargs)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
            await cache.set(cache_key, (), result, ttl)
            logger.debug(f"Cache miss: {func.__name__}")
            
            return result
        return wrapper
    return decorator


class AsyncTaskQueue:
    """–û—á–µ—Ä–µ–¥—å –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç—è–∂–µ–ª—ã—Ö –∑–∞–¥–∞—á"""
    
    def __init__(self, db_url: str = DB_URL):
        self.db_url = db_url
        self.max_workers = 5
        self.semaphore = asyncio.Semaphore(self.max_workers)
    
    async def execute_async(
        self,
        task_name: str,
        task_func: Callable,
        *args,
        **kwargs
    ) -> Any:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞"""
        async with self.semaphore:
            try:
                logger.info(f"Starting async task: {task_name}")
                start_time = datetime.now()
                
                result = await task_func(*args, **kwargs)
                
                duration = (datetime.now() - start_time).total_seconds()
                logger.info(f"Completed async task: {task_name} (took {duration:.2f}s)")
                
                return result
            except Exception as e:
                logger.error(f"Async task error: {task_name}: {e}")
                raise
    
    async def execute_batch(
        self,
        tasks: List[Dict[str, Any]]
    ) -> List[Any]:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –±–∞—Ç—á–∞ –∑–∞–¥–∞—á –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ"""
        async def execute_task(task):
            return await self.execute_async(
                task['name'],
                task['func'],
                *task.get('args', []),
                **task.get('kwargs', {})
            )
        
        results = await asyncio.gather(*[execute_task(task) for task in tasks])
        return results


class PerformanceMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    
    def __init__(self, db_url: str = DB_URL):
        self.db_url = db_url
    
    async def get_slow_queries(self, limit: int = 20) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                rows = await conn.fetch("""
                    SELECT * FROM analyze_slow_queries()
                """)
                return [dict(row) for row in rows[:limit]]
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"Error getting slow queries: {e}")
            return []
    
    async def get_query_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                stats = await conn.fetchrow("""
                    SELECT 
                        count(*) as total_queries,
                        sum(calls) as total_calls,
                        avg(mean_exec_time) as avg_exec_time,
                        max(mean_exec_time) as max_exec_time
                    FROM pg_stat_statements
                """)
                
                return {
                    "total_queries": stats['total_queries'] or 0,
                    "total_calls": stats['total_calls'] or 0,
                    "avg_exec_time_ms": round(float(stats['avg_exec_time'] or 0), 2),
                    "max_exec_time_ms": round(float(stats['max_exec_time'] or 0), 2)
                }
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"Error getting query stats: {e}")
            return {}
    
    async def refresh_cache(self) -> bool:
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π (–∫—ç—à–∞)"""
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                await conn.execute("SELECT refresh_performance_cache()")
                logger.info("‚úÖ Performance cache refreshed")
                return True
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"Error refreshing cache: {e}")
            return False


async def run_performance_optimization():
    """–ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    logger.info("üöÄ Starting performance optimization...")
    
    monitor = PerformanceMonitor()
    cache = QueryCache()
    
    # 1. –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
    await monitor.refresh_cache()
    
    # 2. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
    slow_queries = await monitor.get_slow_queries()
    if slow_queries:
        logger.warning(f"Found {len(slow_queries)} slow queries")
        for query in slow_queries[:5]:
            logger.warning(f"  - {query.get('query_text', '')[:100]}... (avg: {query.get('mean_time', 0):.2f}ms)")
    
    # 3. –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    stats = await monitor.get_query_stats()
    logger.info(f"Query stats: {stats}")
    
    logger.info("‚úÖ Performance optimization completed")


if __name__ == "__main__":
    asyncio.run(run_performance_optimization())

