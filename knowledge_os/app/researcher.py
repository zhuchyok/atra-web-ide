import asyncio
import os
import json
import asyncpg
from datetime import datetime, timezone
import httpx
import logging

logger = logging.getLogger(__name__)

# DuckDuckGo search with fallback
try:
    from duckduckgo_search import DDGS
    DDGS_AVAILABLE = True
except ImportError:
    DDGS = None
    DDGS_AVAILABLE = False
    logger.warning("duckduckgo-search –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install duckduckgo-search")

async def get_pool():
    return await asyncpg.create_pool(
        os.getenv("DATABASE_URL", "postgresql://admin:secret@localhost:5432/knowledge_os"),
        min_size=1,
        max_size=5
    )

async def process_with_local_model(prompt: str, node_url: str = "http://localhost:11434", model: str = "phi3.5:3.8b") -> str:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤)"""
    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{node_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                }
            )
            if response.status_code == 200:
                result = response.json()
                return result.get('response', '')
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
    return ""

async def perform_research():
    """–ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –≤–µ–±-–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤)"""
    print("üåê Starting Autonomous Web Research (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤)...")
    
    if not DDGS_AVAILABLE:
        print("‚ö†Ô∏è duckduckgo-search –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install duckduckgo-search")
        return
    
    pool = await get_pool()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ —É–∑–ª—ã –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (Mac Studio)
    local_nodes = [
        {"url": os.getenv('MAC_LLM_URL', 'http://localhost:11434'), "name": "Mac Studio (Ollama)"}
    ]
    
    # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π —É–∑–µ–ª
    available_node = None
    for node in local_nodes:
        try:
            async with httpx.AsyncClient(timeout=3.0) as client:
                response = await client.get(f"{node['url']}/api/tags")
                if response.status_code == 200:
                    available_node = node
                    print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –Ω–∞ {node['name']} (0 —Ç–æ–∫–µ–Ω–æ–≤)")
                    break
        except:
            continue
    
    if not available_node:
        print("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏")
    
    async with pool.acquire() as conn:
        # –ü–æ–ª—É—á–∞–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–º –Ω—É–∂–Ω–æ –æ–±–Ω–æ–≤–∏—Ç—å –∑–Ω–∞–Ω–∏—è
        experts = await conn.fetch("SELECT id, name, department, role FROM experts ORDER BY RANDOM() LIMIT 3")
        
        for expert in experts:
            query = f"latest trends and best practices 2025 in {expert['role']} for {expert['department']}"
            print(f"üîç Expert {expert['name']} researching: {query}")
            
            try:
                # –®–∞–≥ 1: –í–µ–±-–ø–æ–∏—Å–∫ (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤)
                with DDGS() as ddgs:
                    results = list(ddgs.text(query, max_results=3))
                
                if not results:
                    print(f"‚ö†Ô∏è No results found for {expert['name']}")
                    continue
                
                # –®–∞–≥ 2: –û–±—Ä–∞–±–æ—Ç–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤)
                if available_node:
                    # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-–ø–æ–∏—Å–∫–∞
                    web_content = "\n\n".join([
                        f"Title: {res['title']}\nURL: {res['href']}\nContent: {res['body']}"
                        for res in results
                    ])
                    
                    # –ü—Ä–æ–º–ø—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
                    analysis_prompt = f"""
                    –¢—ã - —ç–∫—Å–ø–µ—Ä—Ç {expert['name']} –≤ –æ–±–ª–∞—Å—Ç–∏ {expert['role']} –¥–ª—è {expert['department']}.
                    –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-–ø–æ–∏—Å–∫–∞ –∏ —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤.
                    
                    –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ï–ë-–ü–û–ò–°–ö–ê:
                    {web_content}
                    
                    –°–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Ä–µ–∑—é–º–µ —Å –∫–ª—é—á–µ–≤—ã–º–∏ –≤—ã–≤–æ–¥–∞–º–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä—Ç–∞.
                    """
                    
                    print(f"ü§ñ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é (0 —Ç–æ–∫–µ–Ω–æ–≤)...")
                    analyzed_content = await process_with_local_model(
                        analysis_prompt,
                        node_url=available_node['url'],
                        model="phi3.5:3.8b"
                    )
                    
                    if analyzed_content:
                        content = f"üìö –ê–ù–ê–õ–ò–ó –í–ï–ë-–ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é, 0 —Ç–æ–∫–µ–Ω–æ–≤):\n\n{analyzed_content}\n\nüìé –ò–°–¢–û–ß–ù–ò–ö–ò:\n" + "\n".join([f"- {res['title']}: {res['href']}" for res in results])
                        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é ({len(analyzed_content)} —Å–∏–º–≤–æ–ª–æ–≤)")
                    else:
                        # Fallback: –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        content = "\n\n".join([
                            f"Source: {res['href']}\nTitle: {res['title']}\nSnippet: {res['body']}"
                            for res in results
                        ])
                else:
                    # –ë–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    content = "\n\n".join([
                        f"Source: {res['href']}\nTitle: {res['title']}\nSnippet: {res['body']}"
                        for res in results
                    ])
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–î
                domain_id = await conn.fetchval("SELECT id FROM domains WHERE name = $1", expert['department'])
                if not domain_id:
                    domain_id = await conn.fetchval("INSERT INTO domains (name) VALUES ($1) RETURNING id", expert['department'])
                
                # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –≤–Ω–µ—à–Ω–µ–µ –∑–Ω–∞–Ω–∏–µ, –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
                metadata = {
                    "source": "web_research",
                    "processed_by": "local_model" if available_node else "raw",
                    "node": available_node['name'] if available_node else None,
                    "tokens_used": 0,  # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å = 0 —Ç–æ–∫–µ–Ω–æ–≤
                    "expert_id": str(expert['id']),
                    "expert_name": expert['name'],
                    "urls": [res['href'] for res in results],
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
                
                embedding = None
                try:
                    from semantic_cache import get_embedding
                    embedding = await get_embedding(content[:8000])
                except Exception:
                    pass
                if embedding is not None:
                    await conn.execute("""
                        INSERT INTO knowledge_nodes (domain_id, content, confidence_score, metadata, is_verified, embedding)
                        VALUES ($1, $2, 0.85, $3, FALSE, $4::vector)
                    """, domain_id, content, json.dumps(metadata), str(embedding))
                else:
                    await conn.execute("""
                        INSERT INTO knowledge_nodes (domain_id, content, confidence_score, metadata, is_verified)
                        VALUES ($1, $2, 0.85, $3, FALSE)
                    """, domain_id, content, json.dumps(metadata))
                
                print(f"‚úÖ Research for {expert['name']} completed. {len(results)} insights added (0 —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ!)")
                
            except Exception as e:
                print(f"‚ùå Research error for {expert['name']}: {e}")
                import traceback
                traceback.print_exc()
    
    await pool.close()

if __name__ == "__main__":
    asyncio.run(perform_research())

