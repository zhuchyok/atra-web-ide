import asyncio
import logging
import json
import os
from typing import List, Dict, Any, Optional
from datetime import datetime, timezone
import asyncpg

logger = logging.getLogger(__name__)

class GraphOptimizer:
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –≥—Ä–∞—Ñ–∞ –∑–Ω–∞–Ω–∏–π: Semantic Pruning –∏ Hot-Path Caching.
    """
    def __init__(self, db_url: str = os.getenv('DATABASE_URL')):
        self.db_url = db_url
        self.redis_url = os.getenv('REDIS_URL', 'redis://knowledge_os_redis:6379/0')

    async def _get_conn(self):
        return await asyncpg.connect(self.db_url)

    async def semantic_pruning(self, threshold: float = 0.3, min_usage: int = 5) -> Dict[str, Any]:
        """
        –£–¥–∞–ª—è–µ—Ç —Å–ª–∞–±—ã–µ —Å–≤—è–∑–∏ (–Ω–∏–∑–∫–∏–π strength) –∏–ª–∏ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Å–≤—è–∑–∏.
        """
        conn = await self._get_conn()
        try:
            # –°—á–∏—Ç–∞–µ–º —Å–∫–æ–ª—å–∫–æ —Å–≤—è–∑–µ–π –ø–æ–¥ —É–≥—Ä–æ–∑–æ–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º strength –≤–º–µ—Å—Ç–æ weight)
            to_delete = await conn.fetchval(
                "SELECT COUNT(*) FROM knowledge_links WHERE strength < $1", threshold
            )
            
            if to_delete > 0:
                # –£–¥–∞–ª—è–µ–º
                await conn.execute(
                    "DELETE FROM knowledge_links WHERE strength < $1", threshold
                )
                logger.info(f"‚úÇÔ∏è [PRUNING] –£–¥–∞–ª–µ–Ω–æ {to_delete} —Å–ª–∞–±—ã—Ö —Å–≤—è–∑–µ–π (strength < {threshold})")
            
            return {"deleted_links": to_delete, "threshold": threshold}
        finally:
            await conn.close()

    async def identify_hot_paths(self, limit: int = 100) -> List[Dict[str, Any]]:
        """
        –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ —Ü–µ–ø–æ—á–∫–∏ —Å–≤—è–∑–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–æ–≤.
        """
        conn = await self._get_conn()
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ª–æ–≥–∏ –¥–æ—Å—Ç—É–ø–∞
            hot_links = await conn.fetch("""
                SELECT source_node_id, target_node_id, strength, link_type 
                FROM knowledge_links 
                WHERE strength > 0.9 
                ORDER BY strength DESC 
                LIMIT $1
            """, limit)
            return [dict(r) for r in hot_links]
        finally:
            await conn.close()

    async def cache_hot_paths(self):
        """
        –ö—ç—à–∏—Ä—É–µ—Ç –≥–æ—Ä—è—á–∏–µ –ø—É—Ç–∏ –≤ Redis –¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞.
        """
        try:
            import redis.asyncio as redis
            r = redis.from_url(self.redis_url)
            
            hot_paths = await self.identify_hot_paths()
            if hot_paths:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º UUID –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è JSON
                serializable_paths = []
                for p in hot_paths:
                    path_dict = dict(p)
                    for k, v in path_dict.items():
                        if hasattr(v, 'hex'): # UUID check
                            path_dict[k] = str(v)
                    serializable_paths.append(path_dict)
                
                await r.set("graph:hot_paths", json.dumps(serializable_paths), ex=3600)
                logger.info(f"üöÄ [CACHING] –ó–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–æ {len(serializable_paths)} –≥–æ—Ä—è—á–∏—Ö –ø—É—Ç–µ–π –≤ Redis")
            
            await r.close()
        except ImportError:
            logger.warning("redis-py not installed, caching skipped")
        except Exception as e:
            logger.error(f"‚ùå [CACHING] –û—à–∏–±–∫–∞ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")

async def run_optimization_cycle():
    optimizer = GraphOptimizer()
    print("Starting Graph Optimization Cycle...")
    
    # 1. Pruning
    prune_res = await optimizer.semantic_pruning()
    print(f"Pruning finished: {prune_res}")
    
    # 2. Caching
    await optimizer.cache_hot_paths()
    print("Caching finished.")

if __name__ == "__main__":
    asyncio.run(run_optimization_cycle())
