import os
import json
import logging
import asyncio
from typing import Any, Optional, Dict, List
import redis.asyncio as redis
from datetime import datetime, timezone

logger = logging.getLogger(__name__)

REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")

class RedisManager:
    """
    –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å Redis: –∫—ç—à, —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–¥–∞—á –∏ –æ—á–µ—Ä–µ–¥–∏.
    –†–µ–∞–ª–∏–∑—É–µ—Ç –ª—É—á—à–∏–µ –º–∏—Ä–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏: –ø—É–ª–∏–Ω–≥ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π, –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç—å, JSON-—Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏—è.
    """
    _instance = None
    _pool = None

    def __new__(cls, *args, **kwargs):
        if not cls._instance:
            cls._instance = super(RedisManager, cls).__new__(cls)
        return cls._instance

    def __init__(self, url: str = REDIS_URL):
        if not hasattr(self, 'initialized'):
            self.url = url
            self.initialized = True

    async def get_client(self) -> redis.Redis:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞–µ—Ç –∫–ª–∏–µ–Ω—Ç Redis –∏–∑ –ø—É–ª–∞."""
        if self._pool is None:
            try:
                self._pool = redis.ConnectionPool.from_url(
                    self.url, 
                    max_connections=20, 
                    decode_responses=True
                )
                logger.info(f"‚úÖ [REDIS] –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π —Å–æ–∑–¥–∞–Ω: {self.url}")
            except Exception as e:
                logger.error(f"‚ùå [REDIS] –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—É–ª–∞: {e}")
                raise
        return redis.Redis(connection_pool=self._pool)

    # --- –ö–≠–®–ò–†–û–í–ê–ù–ò–ï ---
    async def set_cache(self, key: str, value: Any, ttl: int = 3600):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫—ç—à (—Å —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤ JSON)."""
        try:
            client = await self.get_client()
            val = json.dumps(value)
            await client.set(f"cache:{key}", val, ex=ttl)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [REDIS] –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫—ç—à {key}: {e}")

    async def get_cache(self, key: str) -> Optional[Any]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞."""
        try:
            client = await self.get_client()
            val = await client.get(f"cache:{key}")
            return json.loads(val) if val else None
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [REDIS] –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫—ç—à–∞ {key}: {e}")
            return None

    # --- –°–û–°–¢–û–Ø–ù–ò–ï –ó–ê–î–ê–ß (Shared State) ---
    async def update_task_status(self, task_id: str, status: str, result: Any = None, metadata: Dict = None):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–¥–∞—á–∏ –≤ Redis (–¥–ª—è –º–≥–Ω–æ–≤–µ–Ω–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ Gateway)."""
        try:
            client = await self.get_client()
            data = {
                "status": status,
                "updated_at": datetime.now(timezone.utc).isoformat()
            }
            if result is not None: data["result"] = result
            if metadata: data["metadata"] = metadata
            
            await client.hset(f"task:{task_id}", mapping={k: json.dumps(v) for k, v in data.items()})
            # TTL –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ - 24 —á–∞—Å–∞
            await client.expire(f"task:{task_id}", 86400)
        except Exception as e:
            logger.error(f"‚ùå [REDIS] –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {task_id}: {e}")

    async def get_task_status(self, task_id: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å –∑–∞–¥–∞—á–∏."""
        try:
            client = await self.get_client()
            data = await client.hgetall(f"task:{task_id}")
            return {k: json.loads(v) for k, v in data.items()} if data else None
        except Exception as e:
            logger.error(f"‚ùå [REDIS] –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∑–∞–¥–∞—á–∏ {task_id}: {e}")
            return None

    # --- –û–ß–ï–†–ï–î–ò (Redis Streams) ---
    async def push_to_stream(self, stream_name: str, data: Dict, deduplicate: bool = True):
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç –∑–∞–¥–∞—á—É –≤ –ø–æ—Ç–æ–∫ (Redis Stream) ‚Äî –ø—Ä–æ–º—ã—à–ª–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç.
        deduplicate: –µ—Å–ª–∏ True, –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∞–∫—Ç–∏–≤–Ω–æ–π –∑–∞–¥–∞—á–∏ —Å —Ç–∞–∫–∏–º –∂–µ ID –∏–ª–∏ —Ö—ç—à–µ–º.
        """
        try:
            client = await self.get_client()
            task_id = data.get("task_id")
            
            if deduplicate and task_id:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ª–∏ —É–∂–µ —ç—Ç–∞ –∑–∞–¥–∞—á–∞ (–∏–¥–µ–º–ø–æ—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å)
                lock_key = f"lock:task:{task_id}"
                is_locked = await client.set(lock_key, "processing", ex=1800, nx=True) # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –Ω–∞ 30 –º–∏–Ω
                if not is_locked:
                    logger.warning(f"üö´ [REDIS] –î—É–±–ª–∏–∫–∞—Ç –∑–∞–¥–∞—á–∏ {task_id} –ø—Ä–æ–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω")
                    return False

            # –°–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å 10.0: –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è RAG
            data["created_at"] = datetime.now(timezone.utc).isoformat()
            
            # –ï—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ AI, –ø–æ–º–µ—á–∞–µ–º –¥–ª—è –≤–æ—Ä–∫–µ—Ä–∞
            goal = data.get("description", "").lower()
            if any(kw in goal for kw in ["anthropic", "google", "openai", "deepseek", "claude", "gemini"]):
                data["rag_domain"] = "AI Research"
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –ø–æ—Ç–æ–∫–∞ 10000 –∑–∞–ø–∏—Å–µ–π (–º–∏—Ä–æ–≤–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞)
            await client.xadd(f"stream:{stream_name}", {"payload": json.dumps(data)}, maxlen=10000)
            logger.info(f"üì• [REDIS] –ó–∞–¥–∞—á–∞ {task_id} –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –ø–æ—Ç–æ–∫ {stream_name}")
            return True
        except Exception as e:
            logger.error(f"‚ùå [REDIS] –û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –ø–æ—Ç–æ–∫ {stream_name}: {e}")
            return False

    async def autoclaim_tasks(self, stream_name: str, group_name: str, consumer_name: str, min_idle_time_ms: int = 60000):
        """
        –ú–∏—Ä–æ–≤–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞ (Reliable Queue): –ü–µ—Ä–µ—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–∏—Å—à–∏–µ –∑–∞–¥–∞—á–∏ –¥—Ä—É–≥–∏—Ö –≤–æ—Ä–∫–µ—Ä–æ–≤.
        –ï—Å–ª–∏ –≤–æ—Ä–∫–µ—Ä —É–ø–∞–ª, –∑–∞–¥–∞—á–∞ —á–µ—Ä–µ–∑ min_idle_time_ms –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–µ–Ω–∞ —Ç–µ–∫—É—â–µ–º—É –≤–æ—Ä–∫–µ—Ä—É.
        """
        try:
            client = await self.get_client()
            # XAUTOCLAIM –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç [next_start_id, [entries], [deleted_ids]]
            res = await client.xautoclaim(
                f"stream:{stream_name}", 
                group_name, 
                consumer_name, 
                min_idle_time_ms, 
                start_id="0-0", 
                count=5
            )
            if res and res[1]:
                logger.info(f"üîÑ [REDIS] –ü–µ—Ä–µ—Ö–≤–∞—á–µ–Ω–æ {len(res[1])} –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–¥–∞—á –∏–∑ –ø–æ—Ç–æ–∫–∞ {stream_name}")
                return res[1]
            return []
        except Exception as e:
            logger.error(f"‚ùå [REDIS] –û—à–∏–±–∫–∞ autoclaim –≤ –ø–æ—Ç–æ–∫–µ {stream_name}: {e}")
            return []

    async def release_task_lock(self, task_id: str):
        """–°–Ω–∏–º–∞–µ—Ç –±–ª–æ–∫–∏—Ä–æ–≤–∫—É —Å –∑–∞–¥–∞—á–∏ (–ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∏–ª–∏ –æ—à–∏–±–∫–∏)."""
        try:
            client = await self.get_client()
            await client.delete(f"lock:task:{task_id}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [REDIS] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–Ω—è—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫—É {task_id}: {e}")

    async def close(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ—Ç –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π."""
        if self._pool:
            await self._pool.disconnect()
            logger.info("üõë [REDIS] –ü—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π –∑–∞–∫—Ä—ã—Ç")

# –°–∏–Ω–≥–ª—Ç–æ–Ω –¥–ª—è —É–¥–æ–±–Ω–æ–≥–æ –∏–º–ø–æ—Ä—Ç–∞
redis_manager = RedisManager()
