"""
Model Enhancer - –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
–û–±—ä–µ–¥–∏–Ω—è–µ—Ç: Self-Consistency, Speculative Decoding, Enhanced RAG, CoT, Ensemble
"""

import os
import asyncio
import logging
import json
from typing import List, Dict, Optional, Tuple
from datetime import datetime, timezone
from collections import Counter

logger = logging.getLogger(__name__)

DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
OLLAMA_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')


class SelfConsistencyEngine:
    """Self-Consistency: –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞"""
    
    def __init__(self, ollama_url: str = OLLAMA_URL):
        self.ollama_url = ollama_url
    
    async def generate_multiple(
        self,
        prompt: str,
        model_name: str,
        num_samples: int = 5,
        temperature: float = 0.7
    ) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏
            num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
        
        Returns:
            –°–ø–∏—Å–æ–∫ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤
        """
        import httpx
        
        tasks = []
        async with httpx.AsyncClient(timeout=60.0) as client:
            for i in range(num_samples):
                # –ù–µ–º–Ω–æ–≥–æ –≤–∞—Ä—å–∏—Ä—É–µ–º temperature –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                current_temp = temperature + (i * 0.1) % 0.3
                task = client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": current_temp,
                            "num_predict": 2048
                        }
                    }
                )
                tasks.append(task)
            
            responses = await asyncio.gather(*tasks, return_exceptions=True)
        
        results = []
        for resp in responses:
            if isinstance(resp, Exception):
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {resp}")
                continue
            if resp.status_code == 200:
                data = resp.json()
                results.append(data.get('response', ''))
        
        logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(results)} –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –∏–∑ {num_samples}")
        return results
    
    def select_best_answer(
        self,
        responses: List[str],
        method: str = "majority_voting"
    ) -> Tuple[str, float]:
        """
        –í—ã–±—Ä–∞—Ç—å –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π
        
        Args:
            responses: –°–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤
            method: –ú–µ—Ç–æ–¥ –≤—ã–±–æ—Ä–∞ (majority_voting, longest, most_confident)
        
        Returns:
            (–ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç, —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)
        """
        if not responses:
            return "", 0.0
        
        if method == "majority_voting":
            # –î–ª—è –∑–∞–¥–∞—á —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º - –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –≥–æ–ª–æ—Å–æ–≤
            # –î–ª—è —Å–≤–æ–±–æ–¥–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ - —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
            response_lengths = [len(r) for r in responses]
            avg_length = sum(response_lengths) / len(response_lengths)
            
            # –í—ã–±–∏—Ä–∞–µ–º –æ—Ç–≤–µ—Ç—ã –±–ª–∏–∑–∫–∏–µ –∫ —Å—Ä–µ–¥–Ω–µ–º—É (–Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ/–¥–ª–∏–Ω–Ω—ã–µ)
            filtered = [r for r in responses if 0.7 * avg_length <= len(r) <= 1.3 * avg_length]
            
            if filtered:
                # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –∏–∑ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö (–æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã–π)
                best = max(filtered, key=len)
                confidence = len(filtered) / len(responses)
                return best, confidence
            else:
                # Fallback –Ω–∞ —Å—Ä–µ–¥–Ω–∏–π
                best = responses[len(responses) // 2]
                return best, 0.5
        
        elif method == "longest":
            # –°–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã–π)
            best = max(responses, key=len)
            return best, 0.7
        
        elif method == "most_confident":
            # –û—Ç–≤–µ—Ç —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–π (–º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å —á–µ—Ä–µ–∑ LLM)
            best = max(responses, key=lambda x: x.count('‚úÖ') + x.count('.'))
            return best, 0.6
        
        return responses[0], 0.5
    
    async def generate_with_consistency(
        self,
        prompt: str,
        model_name: str,
        num_samples: int = 5,
        use_for: str = "reasoning"  # reasoning, coding, general
    ) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Self-Consistency
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏
            num_samples: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
            use_for: –¢–∏–ø –∑–∞–¥–∞—á–∏
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å –ª—É—á—à–∏–º –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        start_time = datetime.now(timezone.utc)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        responses = await self.generate_multiple(prompt, model_name, num_samples)
        
        if not responses:
            return {
                "response": "",
                "confidence": 0.0,
                "method": "self_consistency",
                "num_samples": num_samples,
                "error": "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∞—Ä–∏–∞–Ω—Ç—ã"
            }
        
        # –í—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
        method = "majority_voting" if use_for == "reasoning" else "longest"
        best_answer, confidence = self.select_best_answer(responses, method)
        
        elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
        
        return {
            "response": best_answer,
            "confidence": confidence,
            "method": "self_consistency",
            "num_samples": len(responses),
            "all_responses": responses[:3],  # –ü–µ—Ä–≤—ã–µ 3 –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            "time_elapsed": elapsed
        }


class SpeculativeDecodingEngine:
    """Speculative Decoding: —É—Å–∫–æ—Ä–µ–Ω–∏–µ —á–µ—Ä–µ–∑ draft –º–æ–¥–µ–ª—å"""
    
    def __init__(self, ollama_url: str = OLLAMA_URL):
        self.ollama_url = ollama_url
        
        # Draft –º–æ–¥–µ–ª–∏ (–±—ã—Å—Ç—Ä—ã–µ) –¥–ª—è —Ä–∞–∑–Ω—ã—Ö target –º–æ–¥–µ–ª–µ–π
        self.draft_models = {
            "command-r-plus:104b": "qwen2.5:3b",  # Tiny –¥–ª—è –±–æ–ª—å—à–æ–π
            "deepseek-r1-distill-llama:70b": "phi3.5:3.8b",  # –ú–∞–ª–µ–Ω—å–∫–∞—è –¥–ª—è —Å—Ä–µ–¥–Ω–µ–π
            "llama3.3:70b": "phi3.5:3.8b",
            "qwen2.5-coder:32b": "qwen2.5:3b",
            "phi3.5:3.8b": "tinyllama:1.1b-chat",  # Tiny –¥–ª—è –º–∞–ª–µ–Ω—å–∫–æ–π
        }
    
    async def generate_with_speculation(
        self,
        prompt: str,
        target_model: str,
        draft_model: Optional[str] = None,
        num_draft_tokens: int = 5
    ) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Speculative Decoding
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç
            target_model: –¶–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å (–±–æ–ª—å—à–∞—è, –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è)
            draft_model: Draft –º–æ–¥–µ–ª—å (–±—ã—Å—Ç—Ä–∞—è, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
            num_draft_tokens: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è draft
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å —É—Å–∫–æ—Ä–µ–Ω–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        """
        if draft_model is None:
            draft_model = self.draft_models.get(target_model, "tinyllama:1.1b-chat")
        
        import httpx
        
        start_time = datetime.now(timezone.utc)
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            # 1. Draft –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –±—ã—Å—Ç—Ä—ã–π —á–µ—Ä–Ω–æ–≤–∏–∫
            draft_resp = await client.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": draft_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.8,  # –í—ã—à–µ –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                        "num_predict": num_draft_tokens
                    }
                }
            )
            
            if draft_resp.status_code != 200:
                # Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                return await self._fallback_generate(client, prompt, target_model)
            
            draft_text = draft_resp.json().get('response', '')
            
            # 2. Target –º–æ–¥–µ–ª—å –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç draft
            full_prompt = f"{prompt}\n\n[Draft]: {draft_text}\n\n[Complete the response]:"
            
            target_resp = await client.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": target_model,
                    "prompt": full_prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "num_predict": 2048
                    }
                }
            )
            
            if target_resp.status_code != 200:
                return await self._fallback_generate(client, prompt, target_model)
            
            final_response = target_resp.json().get('response', '')
            elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
            
            return {
                "response": final_response,
                "method": "speculative_decoding",
                "draft_model": draft_model,
                "target_model": target_model,
                "draft_text": draft_text[:100],  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤
                "time_elapsed": elapsed,
                "speedup_estimate": 1.5  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —É—Å–∫–æ—Ä–µ–Ω–∏—è
            }
    
    async def _fallback_generate(self, client, prompt: str, model: str) -> Dict:
        """Fallback –Ω–∞ –æ–±—ã—á–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é"""
        resp = await client.post(
            f"{self.ollama_url}/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False
            }
        )
        
        if resp.status_code == 200:
            return {
                "response": resp.json().get('response', ''),
                "method": "standard",
                "time_elapsed": 0.0
            }
        
        return {"response": "", "method": "error", "error": "Generation failed"}


class EnhancedRAGEngine:
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π RAG —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
    
    def __init__(self, db_url: str = DB_URL):
        self.db_url = db_url
    
    async def retrieve_enhanced_context(
        self,
        query: str,
        limit: int = 5,
        min_confidence: float = 0.7,
        use_reranking: bool = True
    ) -> List[Dict]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–æ–º
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å
            limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            min_confidence: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
            use_reranking: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥
        
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
        """
        import asyncpg
        
        try:
            conn = await asyncpg.connect(self.db_url)
            try:
                # 1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
                # TODO: –î–æ–±–∞–≤–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ —á–µ—Ä–µ–∑ pgvector
                
                # 2. –ü–æ–∏—Å–∫ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π
                keywords = query.lower().split()[:10]
                keyword_pattern = '|'.join(keywords)
                
                rows = await conn.fetch("""
                    SELECT 
                        id, content, confidence_score, usage_count,
                        metadata, domain_id, is_verified,
                        -- –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                        (
                            CASE WHEN is_verified THEN 1.0 ELSE 0.8 END *
                            confidence_score *
                            (1.0 + LEAST(usage_count / 100.0, 0.2))
                        ) as relevance_score
                    FROM knowledge_nodes
                    WHERE is_verified = TRUE
                    AND confidence_score >= $1
                    AND (
                        content ILIKE ANY(ARRAY[SELECT '%' || keyword || '%' 
                            FROM unnest(string_to_array($2, '|')) AS keyword])
                        OR metadata::text ILIKE ANY(ARRAY[SELECT '%' || keyword || '%' 
                            FROM unnest(string_to_array($2, '|')) AS keyword])
                    )
                    ORDER BY relevance_score DESC
                    LIMIT $3
                """, min_confidence, keyword_pattern, limit * 2)  # –ë–µ—Ä–µ–º –≤ 2 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –¥–ª—è —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥–∞
                
                # 3. –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
                if use_reranking and len(rows) > limit:
                    # –ü—Ä–æ—Å—Ç–æ–π —Ä–µ—Ä–∞–Ω–∫–∏–Ω–≥: —É—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Å–≤–µ–∂–µ—Å—Ç—å –∏ —Ç.–¥.
                    scored = []
                    for row in rows:
                        score = float(row['relevance_score'])
                        # –ë–æ–Ω—É—Å –∑–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –¥–ª–∏–Ω—É (–Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π/–¥–ª–∏–Ω–Ω—ã–π)
                        content_len = len(row['content'])
                        if 100 <= content_len <= 1000:
                            score *= 1.1
                        scored.append((score, row))
                    
                    scored.sort(reverse=True, key=lambda x: x[0])
                    rows = [row for _, row in scored[:limit]]
                else:
                    rows = rows[:limit]
                
                # 4. –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                context = []
                for row in rows:
                    domain = await conn.fetchval(
                        "SELECT name FROM domains WHERE id = $1",
                        row['domain_id']
                    ) if row['domain_id'] else None
                    
                    context.append({
                        "content": row['content'],
                        "confidence": float(row['confidence_score']),
                        "relevance": float(row['relevance_score']),
                        "domain": domain,
                        "metadata": row['metadata'],
                        "is_verified": row['is_verified']
                    })
                
                logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(context)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤")
                return context
                
            finally:
                await conn.close()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
            return []
    
    def build_enhanced_prompt(
        self,
        query: str,
        context: List[Dict],
        use_cot: bool = False
    ) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —É–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å
            context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ RAG
            use_cot: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ Chain-of-Thought
        
        Returns:
            –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        """
        prompt = "–¢—ã - —Ç–æ—á–Ω—ã–π –∏ –Ω–∞–¥–µ–∂–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞.\n\n"
        
        if context:
            prompt += "üìö –†–ï–õ–ï–í–ê–ù–¢–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢:\n\n"
            for i, ctx in enumerate(context, 1):
                prompt += f"[–ö–æ–Ω—Ç–µ–∫—Å—Ç {i}] (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {ctx['confidence']:.2f}, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {ctx['relevance']:.2f})\n"
                prompt += f"{ctx['content']}\n\n"
        
        if use_cot:
            prompt += "\n\n–†–ï–®–ò –ó–ê–î–ê–ß–£ –ü–û–®–ê–ì–û–í–û:\n"
            prompt += "1. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º—ã\n"
            prompt += "2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
            prompt += "3. –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞\n"
            prompt += "4. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ—á–Ω–æ—Å—Ç–∏\n\n"
        
        prompt += f"–í–û–ü–†–û–°: {query}\n\n"
        prompt += "–û–¢–í–ï–¢ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å):"
        
        return prompt


class ModelEnsemble:
    """Ensemble: –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, ollama_url: str = OLLAMA_URL):
        self.ollama_url = ollama_url
    
    async def ensemble_generate(
        self,
        prompt: str,
        models: List[str],
        strategy: str = "vote"  # vote, average, best
    ) -> Dict:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ ensemble –º–æ–¥–µ–ª–µ–π
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç
            models: –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
            strategy: –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∫–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç ensemble
        """
        import httpx
        
        async with httpx.AsyncClient(timeout=60.0) as client:
            tasks = []
            for model in models:
                task = client.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": model,
                        "prompt": prompt,
                        "stream": False
                    }
                )
                tasks.append((model, task))
            
            results = await asyncio.gather(*[t for _, t in tasks], return_exceptions=True)
            
            responses = {}
            for (model, _), result in zip(tasks, results):
                if isinstance(result, Exception):
                    continue
                if result.status_code == 200:
                    responses[model] = result.json().get('response', '')
            
            if not responses:
                return {"response": "", "error": "All models failed"}
            
            # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã
            if strategy == "vote":
                # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω
                combined = self._vote_combine(list(responses.values()))
            elif strategy == "best":
                # –í—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π (–æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã–π)
                combined = max(responses.values(), key=len)
            else:
                # Average - –±–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–∏–π –ø–æ –¥–ª–∏–Ω–µ
                avg_len = sum(len(r) for r in responses.values()) / len(responses)
                combined = min(responses.values(), key=lambda x: abs(len(x) - avg_len))
            
            return {
                "response": combined,
                "method": "ensemble",
                "models_used": list(responses.keys()),
                "num_models": len(responses),
                "strategy": strategy
            }
    
    def _vote_combine(self, responses: List[str]) -> str:
        """–ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ä–µ–∑ –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ"""
        # –ü—Ä–æ—Å—Ç–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: –≤—ã–±–∏—Ä–∞–µ–º —Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π (–æ–±—ã—á–Ω–æ –±–æ–ª–µ–µ –ø–æ–ª–Ω—ã–π)
        return max(responses, key=len)


class ModelEnhancer:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å - –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –≤—Å–µ –º–µ—Ç–æ–¥—ã —É–ª—É—á—à–µ–Ω–∏—è"""
    
    def __init__(self, db_url: str = DB_URL, ollama_url: str = OLLAMA_URL):
        self.self_consistency = SelfConsistencyEngine(ollama_url)
        self.speculative = SpeculativeDecodingEngine(ollama_url)
        self.enhanced_rag = EnhancedRAGEngine(db_url)
        self.ensemble = ModelEnsemble(ollama_url)
    
    async def enhance_response(
        self,
        query: str,
        model_name: str,
        enhancement_methods: List[str] = None,
        task_type: str = "general"
    ) -> Dict:
        """
        –£–ª—É—á—à–∏—Ç—å –æ—Ç–≤–µ—Ç –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã
        
        Args:
            query: –ó–∞–ø—Ä–æ—Å
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏
            enhancement_methods: –ú–µ—Ç–æ–¥—ã —É–ª—É—á—à–µ–Ω–∏—è (self_consistency, speculative, rag, ensemble)
            task_type: –¢–∏–ø –∑–∞–¥–∞—á–∏ (reasoning, coding, general)
        
        Returns:
            –£–ª—É—á—à–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if enhancement_methods is None:
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–µ—Ç–æ–¥–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏
            if task_type == "reasoning":
                enhancement_methods = ["self_consistency", "rag", "cot"]
            elif task_type == "coding":
                enhancement_methods = ["rag", "speculative"]
            else:
                enhancement_methods = ["rag"]
        
        result = {
            "query": query,
            "model": model_name,
            "task_type": task_type,
            "methods_used": enhancement_methods,
            "response": "",
            "metadata": {}
        }
        
        # 1. Enhanced RAG (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        context = []
        if "rag" in enhancement_methods:
            context = await self.enhanced_rag.retrieve_enhanced_context(query)
            if context:
                query = self.enhanced_rag.build_enhanced_prompt(
                    query, context, use_cot=("cot" in enhancement_methods)
                )
        
        # 2. –í—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        if "self_consistency" in enhancement_methods:
            # Self-Consistency –¥–ª—è reasoning
            consistency_result = await self.self_consistency.generate_with_consistency(
                query, model_name, num_samples=5, use_for=task_type
            )
            result["response"] = consistency_result["response"]
            result["metadata"]["consistency"] = consistency_result
        
        elif "speculative" in enhancement_methods:
            # Speculative Decoding –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            speculative_result = await self.speculative.generate_with_speculation(
                query, model_name
            )
            result["response"] = speculative_result["response"]
            result["metadata"]["speculative"] = speculative_result
        
        elif "ensemble" in enhancement_methods:
            # Ensemble –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞
            # –í—ã–±–∏—Ä–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –¥–ª—è ensemble
            if task_type == "coding":
                ensemble_models = ["qwen2.5-coder:32b", "qwen2.5:3b"]
            elif task_type == "reasoning":
                ensemble_models = ["deepseek-r1-distill-llama:70b", "llama3.3:70b"]
            else:
                ensemble_models = [model_name, "phi3.5:3.8b"]
            
            ensemble_result = await self.ensemble.ensemble_generate(query, ensemble_models)
            result["response"] = ensemble_result["response"]
            result["metadata"]["ensemble"] = ensemble_result
        
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å)
            result["response"] = "[Standard generation not implemented]"
        
        result["metadata"]["rag_context_count"] = len(context)
        result["metadata"]["rag_context"] = context[:2] if context else []  # –ü–µ—Ä–≤—ã–µ 2 –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        
        return result


async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    enhancer = ModelEnhancer()
    
    # –ü—Ä–∏–º–µ—Ä 1: Reasoning —Å Self-Consistency
    result1 = await enhancer.enhance_response(
        "–†–µ—à–∏ –∑–∞–¥–∞—á—É: –£ –ú–∞—à–∏ –±—ã–ª–æ 5 —è–±–ª–æ–∫, –æ–Ω–∞ –æ—Ç–¥–∞–ª–∞ 2. –°–∫–æ–ª—å–∫–æ –æ—Å—Ç–∞–ª–æ—Å—å?",
        "deepseek-r1-distill-llama:70b",
        enhancement_methods=["self_consistency", "rag", "cot"],
        task_type="reasoning"
    )
    print("Reasoning —Ä–µ–∑—É–ª—å—Ç–∞—Ç:", result1["response"][:200])
    
    # –ü—Ä–∏–º–µ—Ä 2: Coding —Å Speculative Decoding
    result2 = await enhancer.enhance_response(
        "–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Å–ø–∏—Å–∫–∞",
        "qwen2.5-coder:32b",
        enhancement_methods=["speculative", "rag"],
        task_type="coding"
    )
    print("Coding —Ä–µ–∑—É–ª—å—Ç–∞—Ç:", result2["response"][:200])


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
