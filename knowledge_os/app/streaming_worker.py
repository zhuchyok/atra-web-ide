"""
Streaming Worker - event-driven –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á —á–µ—Ä–µ–∑ Redis Streams.

–ó–∞–º–µ–Ω—è–µ—Ç polling-based worker –Ω–∞ —Ä–µ–∞–∫—Ç–∏–≤–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É:
- –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Ä–µ–∞–∫—Ü–∏—è –Ω–∞ –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
- Consumer Groups –¥–ª—è –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
- At-least-once –¥–æ—Å—Ç–∞–≤–∫–∞
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ —Å–±–æ—è—Ö
"""

import asyncio
import os
import sys
import json
import logging
import subprocess
from datetime import datetime
from typing import Dict, Any, Optional

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.insert(0, '/root/knowledge_os/src')

import asyncpg

from infrastructure.streaming import (
    EventConsumer,
    EventProducer,
    StreamManager,
    EventType,
    TaskEvent,
    KnowledgeEvent,
)
from infrastructure.streaming.consumer import ConsumerConfig

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'
)
logger = logging.getLogger("streaming_worker")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
REDIS_URL = os.getenv('REDIS_URL', 'redis://localhost:6379')

# Connection pool
_pool: Optional[asyncpg.Pool] = None


async def get_pool() -> asyncpg.Pool:
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ —Å–æ–∑–¥–∞—ë—Ç connection pool."""
    global _pool
    if _pool is None:
        _pool = await asyncpg.create_pool(
            DB_URL,
            min_size=1,
            max_size=5,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ –ë–î
            max_inactive_connection_lifetime=300,
            command_timeout=60
        )
    return _pool


def run_cursor_agent(prompt: str, timeout: int = 600) -> Optional[str]:
    """–í—ã–∑—ã–≤–∞–µ—Ç cursor-agent –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è AI –∑–∞–¥–∞—á."""
    try:
        env = os.environ.copy()
        result = subprocess.run(
            ['/root/.local/bin/cursor-agent', '--print', prompt],
            capture_output=True,
            text=True,
            check=True,
            timeout=timeout,
            env=env
        )
        return result.stdout.strip()
    except subprocess.TimeoutExpired:
        logger.error(f"Cursor agent timeout after {timeout}s")
        return None
    except Exception as e:
        logger.error(f"Cursor agent error: {e}")
        return None


class StreamingWorker:
    """
    Event-driven worker –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á –∏–∑ Redis Streams.
    
    –ü–æ–¥–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è –Ω–∞:
    - task_stream: –ù–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
    - knowledge_stream: –°–æ–±—ã—Ç–∏—è –∑–Ω–∞–Ω–∏–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
    """
    
    def __init__(self, worker_id: str = "worker-1"):
        self.worker_id = worker_id
        self.producer: Optional[EventProducer] = None
        self.stream_manager: Optional[StreamManager] = None
        
        # Task consumer
        self.task_consumer = EventConsumer(
            redis_url=REDIS_URL,
            config=ConsumerConfig(
                stream_name="task_stream",
                group_name="task_workers",
                consumer_name=f"task-worker-{worker_id}",
                batch_size=5,
                block_ms=5000,
                claim_idle_ms=120000,  # 2 –º–∏–Ω—É—Ç—ã –¥–ª—è claim
            )
        )
        
        # Knowledge consumer (–¥–ª—è —Ä–µ–∞–∫—Ü–∏–∏ –Ω–∞ –Ω–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è)
        self.knowledge_consumer = EventConsumer(
            redis_url=REDIS_URL,
            config=ConsumerConfig(
                stream_name="knowledge_stream",
                group_name="knowledge_processors",
                consumer_name=f"knowledge-worker-{worker_id}",
                batch_size=10,
                block_ms=3000,
            )
        )
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        self._register_handlers()
    
    def _register_handlers(self):
        """–†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ—Ç –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π."""
        
        # Task handlers
        @self.task_consumer.on_event(EventType.TASK_CREATED)
        async def handle_task_created(event: TaskEvent, raw_data: Dict) -> bool:
            return await self._process_task(event)
        
        @self.task_consumer.on_event(EventType.TASK_ASSIGNED)
        async def handle_task_assigned(event: TaskEvent, raw_data: Dict) -> bool:
            logger.info(f"üìã Task assigned: {event.title} -> {event.assignee_name}")
            return True  # Just acknowledge
        
        # Knowledge handlers
        @self.knowledge_consumer.on_event(EventType.KNOWLEDGE_CREATED)
        async def handle_knowledge_created(event: KnowledgeEvent, raw_data: Dict) -> bool:
            return await self._process_new_knowledge(event)
        
        @self.knowledge_consumer.on_event(EventType.INSIGHT_CROSS_DOMAIN)
        async def handle_insight(event: KnowledgeEvent, raw_data: Dict) -> bool:
            logger.info(f"üí° New cross-domain insight received: {event.content[:100]}...")
            return True
    
    async def _process_task(self, event: TaskEvent) -> bool:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–¥–∞—á—É –∏–∑ stream."""
        task_id = event.task_id
        logger.info(f"üîÑ Processing task: {event.title} (ID: {task_id})")
        
        try:
            pool = await get_pool()
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–∞–¥–∞—á–µ –∏–∑ –ë–î
            task = await pool.fetchrow("""
                SELECT t.id, t.title, t.description, t.status,
                       e.name as assignee, e.system_prompt
                FROM tasks t
                JOIN experts e ON t.assignee_expert_id = e.id
                WHERE t.id = $1
            """, int(task_id) if task_id else None)
            
            if not task:
                logger.warning(f"Task {task_id} not found in database")
                return True  # ACK anyway - task may have been deleted
            
            if task['status'] != 'pending':
                logger.info(f"Task {task_id} already processed (status: {task['status']})")
                return True
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –Ω–∞ in_progress
            await pool.execute(
                "UPDATE tasks SET status = 'in_progress', updated_at = NOW() WHERE id = $1",
                task['id']
            )
            
            # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
            if self.producer:
                await self.producer.publish(TaskEvent(
                    event_type=EventType.TASK_STARTED,
                    task_id=str(task['id']),
                    title=task['title'],
                    assignee_name=task['assignee']
                ))
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –∏ –≤—ã–∑—ã–≤–∞–µ–º AI
            prompt = f"""{task['system_prompt']}

–ó–ê–î–ê–ß–ê: {task['title']}
–ò–ù–°–¢–†–£–ö–¶–ò–Ø: {task['description']}

–¢–í–û–Ø –¶–ï–õ–¨: –í—ã–ø–æ–ª–Ω–∏ –∑–∞–¥–∞—á—É –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –≥–ª—É–±–æ–∫–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ. 
–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 3-5 –∫–ª—é—á–µ–≤—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤ –∏–ª–∏ —Ä–µ—à–µ–Ω–∏–π. 
–û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞.
"""
            
            logger.info(f"ü§ñ Calling AI for task {task_id}...")
            report = run_cursor_agent(prompt)
            
            if report:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                await pool.execute(
                    "UPDATE tasks SET status = 'completed', result = $2, updated_at = NOW() WHERE id = $1",
                    task['id'], report
                )
                
                # –°–æ–∑–¥–∞—ë–º knowledge node –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                domain_id = await pool.fetchval(
                    "SELECT domain_id FROM experts WHERE name = $1 LIMIT 1",
                    task['assignee']
                )
                
                if domain_id:
                    knowledge_id = await pool.fetchval("""
                        INSERT INTO knowledge_nodes 
                        (domain_id, content, metadata, confidence_score, is_verified)
                        VALUES ($1, $2, $3, 0.95, TRUE)
                        RETURNING id
                    """, domain_id,
                        f"üìä –û–¢–ß–ï–¢ –≠–ö–°–ü–ï–†–¢–ê ({task['assignee']}): {task['title']}\n\n{report}",
                        json.dumps({
                            "task_id": str(task['id']),
                            "expert": task['assignee'],
                            "source": "streaming_worker"
                        })
                    )
                    
                    # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ –æ –Ω–æ–≤–æ–º –∑–Ω–∞–Ω–∏–∏
                    if self.producer and knowledge_id:
                        await self.producer.publish_knowledge_created(
                            knowledge_id=str(knowledge_id),
                            content=report[:500],
                            domain_id=str(domain_id),
                            domain_name=task['assignee'],
                            metadata={"from_task": str(task['id'])}
                        )
                
                # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏
                if self.producer:
                    await self.producer.publish_task_completed(
                        task_id=str(task['id']),
                        title=task['title'],
                        assignee_name=task['assignee'],
                        result=report[:500]
                    )
                
                logger.info(f"‚úÖ Task {task_id} completed by {task['assignee']}")
                return True
            else:
                # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç—É—Å
                await pool.execute(
                    "UPDATE tasks SET status = 'pending', updated_at = NOW() WHERE id = $1",
                    task['id']
                )
                
                # –ü—É–±–ª–∏–∫—É–µ–º —Å–æ–±—ã—Ç–∏–µ –æ –Ω–µ—É–¥–∞—á–µ
                if self.producer:
                    await self.producer.publish(TaskEvent(
                        event_type=EventType.TASK_FAILED,
                        task_id=str(task['id']),
                        title=task['title'],
                        assignee_name=task['assignee'],
                        metadata={"reason": "AI agent returned empty response"}
                    ))
                
                logger.warning(f"‚ùå Task {task_id} failed, reverted to pending")
                return False  # –ù–µ ACK–∞–µ–º - –±—É–¥–µ—Ç retry
                
        except Exception as e:
            logger.error(f"Error processing task {task_id}: {e}")
            return False
    
    async def _process_new_knowledge(self, event: KnowledgeEvent) -> bool:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–±—ã—Ç–∏–µ —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∑–Ω–∞–Ω–∏—è."""
        logger.info(
            f"üìö New knowledge in domain '{event.domain_name}': "
            f"{event.content[:100]}..."
        )
        
        # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É:
        # - –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ vector store
        # - –£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        # - –¢—Ä–∏–≥–≥–µ—Ä cross-domain –∞–Ω–∞–ª–∏–∑–∞
        
        return True
    
    async def start(self):
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç worker."""
        logger.info(f"üöÄ StreamingWorker '{self.worker_id}' starting...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É
        self.stream_manager = StreamManager(REDIS_URL)
        await self.stream_manager.initialize()
        
        self.producer = EventProducer(REDIS_URL)
        await self.producer.connect()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º consumers –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        await asyncio.gather(
            self.task_consumer.start(),
            self.knowledge_consumer.start(),
        )
    
    async def stop(self):
        """–û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç worker."""
        logger.info("Stopping StreamingWorker...")
        
        await self.task_consumer.stop()
        await self.knowledge_consumer.stop()
        
        if self.producer:
            await self.producer.close()
        
        if self.stream_manager:
            await self.stream_manager.close()
        
        global _pool
        if _pool:
            await _pool.close()
            _pool = None
        
        logger.info("StreamingWorker stopped")


async def main():
    """Entry point."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Streaming Worker for Knowledge OS')
    parser.add_argument('--worker-id', default='worker-1', help='Unique worker ID')
    args = parser.parse_args()
    
    worker = StreamingWorker(worker_id=args.worker_id)
    
    # Graceful shutdown
    loop = asyncio.get_event_loop()
    
    def shutdown():
        logger.info("Received shutdown signal")
        asyncio.create_task(worker.stop())
    
    try:
        import signal
        for sig in (signal.SIGTERM, signal.SIGINT):
            loop.add_signal_handler(sig, shutdown)
    except NotImplementedError:
        pass  # Windows
    
    try:
        await worker.start()
    except KeyboardInterrupt:
        await worker.stop()


if __name__ == "__main__":
    asyncio.run(main())
