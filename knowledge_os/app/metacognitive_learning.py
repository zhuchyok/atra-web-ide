#!/usr/bin/env python3
"""
Metacognitive Learning - –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤.
–ü–æ–∑–≤–æ–ª—è–µ—Ç –∞–≥–µ–Ω—Ç–∞–º –æ—Ü–µ–Ω–∏–≤–∞—Ç—å, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è.

–ò—Å—Ç–æ—á–Ω–∏–∫: Research on Self-Evolving Agents (2025-2026)
–≠—Ñ—Ñ–µ–∫—Ç: +40-60% –Ω–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ—Å—Ç–∏ –∏ —Å–∞–º–æ—É–ª—É—á—à–µ–Ω–∏–∏
"""
import asyncio
import json
import logging
from typing import Dict, List, Optional, Any
from datetime import datetime, timezone
from dataclasses import dataclass, asdict

logger = logging.getLogger(__name__)


@dataclass
class MetacognitiveState:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    self_assessment: float  # –°–∞–º–æ–æ—Ü–µ–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π (0-1)
    learning_goals: List[str]  # –¶–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è
    learning_history: List[Dict]  # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
    strengths: List[str]  # –°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
    weaknesses: List[str]  # –°–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
    adaptation_strategy: str  # –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏


class MetacognitiveLearner:
    """
    –ú–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞:
    1. Self-Assessment - –æ—Ü–µ–Ω–∏–≤–∞—Ç—å —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è
    2. Metacognitive Planning - –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —á—Ç–æ –∏–∑—É—á–∞—Ç—å
    3. Metacognitive Evaluation - —Ä–µ—Ñ–ª–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –Ω–∞–¥ –æ–ø—ã—Ç–æ–º
    """
    
    def __init__(self, agent_name: str = "Victoria"):
        self.agent_name = agent_name
        self.state = MetacognitiveState(
            self_assessment=0.5,
            learning_goals=[],
            learning_history=[],
            strengths=[],
            weaknesses=[],
            adaptation_strategy="progressive"
        )
    
    async def self_assess(self, task_performance: Dict[str, Any]) -> float:
        """
        Self-Assessment - —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π
        
        Args:
            task_performance: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
                - success_rate: –ø—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–¥–∞—á
                - avg_quality: —Å—Ä–µ–¥–Ω–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ
                - feedback_scores: –æ—Ü–µ–Ω–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
        
        Returns:
            –û—Ü–µ–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π (0-1)
        """
        logger.info(f"üß† [{self.agent_name}] –í—ã–ø–æ–ª–Ω—è—é —Å–∞–º–æ–æ—Ü–µ–Ω–∫—É...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
        success_rate = task_performance.get('success_rate', 0.5)
        avg_quality = task_performance.get('avg_quality', 0.5)
        feedback_scores = task_performance.get('feedback_scores', [])
        
        # –í—ã—á–∏—Å–ª—è–µ–º –±–∞–∑–æ–≤—É—é –æ—Ü–µ–Ω–∫—É
        base_score = (success_rate + avg_quality) / 2
        
        # –£—á–∏—Ç—ã–≤–∞–µ–º –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
        if feedback_scores:
            avg_feedback = sum(feedback_scores) / len(feedback_scores)
            base_score = (base_score + avg_feedback) / 2
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        self.state.self_assessment = base_score
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
        if base_score >= 0.8:
            self.state.strengths.append("–í—ã—Å–æ–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        elif base_score < 0.5:
            self.state.weaknesses.append("–ù–∏–∑–∫–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å")
        
        logger.info(f"‚úÖ [{self.agent_name}] –°–∞–º–æ–æ—Ü–µ–Ω–∫–∞: {base_score:.2f}")
        return base_score
    
    async def plan_learning(self, current_knowledge: List[str], gaps: List[str]) -> List[str]:
        """
        Metacognitive Planning - –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á—Ç–æ –∏–∑—É—á–∞—Ç—å –¥–∞–ª—å—à–µ
        
        Args:
            current_knowledge: –¢–µ–∫—É—â–∏–µ –∑–Ω–∞–Ω–∏—è
            gaps: –ü—Ä–æ–±–µ–ª—ã –≤ –∑–Ω–∞–Ω–∏—è—Ö
        
        Returns:
            –ü–ª–∞–Ω –æ–±—É—á–µ–Ω–∏—è (–ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–µ–ª–∏)
        """
        logger.info(f"üìã [{self.agent_name}] –ü–ª–∞–Ω–∏—Ä—É—é –æ–±—É—á–µ–Ω–∏–µ...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–±–µ–ª—ã
        priority_gaps = []
        for gap in gaps:
            # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–±–µ–ª—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–π –æ—Ü–µ–Ω–∫–∏
            if self.state.self_assessment < 0.5:
                # –ï—Å–ª–∏ –æ—Ü–µ–Ω–∫–∞ –Ω–∏–∑–∫–∞—è, —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –±–∞–∑–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏—è—Ö
                if "–±–∞–∑–æ–≤—ã–π" in gap.lower() or "—Ñ—É–Ω–¥–∞–º–µ–Ω—Ç" in gap.lower():
                    priority_gaps.append(gap)
            else:
                # –ï—Å–ª–∏ –æ—Ü–µ–Ω–∫–∞ –≤—ã—Å–æ–∫–∞—è, —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è –Ω–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö
                priority_gaps.append(gap)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ü–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è
        learning_goals = []
        for gap in priority_gaps[:5]:  # –ú–∞–∫—Å–∏–º—É–º 5 —Ü–µ–ª–µ–π
            goal = f"–ò–∑—É—á–∏—Ç—å: {gap}"
            learning_goals.append(goal)
        
        self.state.learning_goals = learning_goals
        logger.info(f"‚úÖ [{self.agent_name}] –°–æ–∑–¥–∞–Ω–æ {len(learning_goals)} —Ü–µ–ª–µ–π –æ–±—É—á–µ–Ω–∏—è")
        
        return learning_goals
    
    async def evaluate_learning(self, learning_experience: Dict[str, Any]) -> Dict[str, Any]:
        """
        Metacognitive Evaluation - —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –Ω–∞–¥ –æ–ø—ã—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è
        
        Args:
            learning_experience: –û–ø—ã—Ç –æ–±—É—á–µ–Ω–∏—è
                - topic: —Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è
                - outcome: —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                - time_spent: –≤—Ä–µ–º—è –ø–æ—Ç—Ä–∞—á–µ–Ω–æ
                - effectiveness: —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        
        Returns:
            –û—Ü–µ–Ω–∫–∞ –æ–ø—ã—Ç–∞ –æ–±—É—á–µ–Ω–∏—è
        """
        logger.info(f"üîç [{self.agent_name}] –û—Ü–µ–Ω–∏–≤–∞—é –æ–ø—ã—Ç –æ–±—É—á–µ–Ω–∏—è...")
        
        topic = learning_experience.get('topic', 'Unknown')
        outcome = learning_experience.get('outcome', 'neutral')
        effectiveness = learning_experience.get('effectiveness', 0.5)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
        experience_record = {
            'topic': topic,
            'outcome': outcome,
            'effectiveness': effectiveness,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        self.state.learning_history.append(experience_record)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        evaluation = {
            'topic': topic,
            'effectiveness': effectiveness,
            'recommendation': 'continue' if effectiveness > 0.6 else 'adjust',
            'lessons_learned': []
        }
        
        if effectiveness > 0.8:
            evaluation['lessons_learned'].append(f"–£—Å–ø–µ—à–Ω–æ –∏–∑—É—á–µ–Ω–æ: {topic}")
            if topic not in self.state.strengths:
                self.state.strengths.append(topic)
        elif effectiveness < 0.4:
            evaluation['lessons_learned'].append(f"–¢—Ä–µ–±—É–µ—Ç –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞: {topic}")
            if topic not in self.state.weaknesses:
                self.state.weaknesses.append(topic)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        if effectiveness < 0.4:
            self.state.adaptation_strategy = "remedial"  # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ–ª—å–Ω–∞—è
        elif effectiveness > 0.8:
            self.state.adaptation_strategy = "accelerated"  # –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è
        else:
            self.state.adaptation_strategy = "progressive"  # –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è
        
        logger.info(f"‚úÖ [{self.agent_name}] –û—Ü–µ–Ω–∫–∞: {effectiveness:.2f}, —Å—Ç—Ä–∞—Ç–µ–≥–∏—è: {self.state.adaptation_strategy}")
        
        return evaluation
    
    async def adapt_learning_process(self) -> Dict[str, Any]:
        """
        –ê–¥–∞–ø—Ç–∞—Ü–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        """
        logger.info(f"üîÑ [{self.agent_name}] –ê–¥–∞–ø—Ç–∏—Ä—É—é –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è...")
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
        recent_experiences = self.state.learning_history[-5:] if len(self.state.learning_history) >= 5 else self.state.learning_history
        
        if not recent_experiences:
            return {
                'action': 'continue',
                'reason': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏'
            }
        
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        avg_effectiveness = sum(e.get('effectiveness', 0.5) for e in recent_experiences) / len(recent_experiences)
        
        # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ –æ–± –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        if avg_effectiveness < 0.4:
            action = 'change_approach'
            reason = '–ù–∏–∑–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è'
            # –ú–µ–Ω—è–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é
            self.state.adaptation_strategy = "remedial"
        elif avg_effectiveness > 0.8:
            action = 'accelerate'
            reason = '–í—ã—Å–æ–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å, –º–æ–∂–Ω–æ —É—Å–∫–æ—Ä–∏—Ç—å'
            self.state.adaptation_strategy = "accelerated"
        else:
            action = 'continue'
            reason = '–°—Ç–∞–±–∏–ª—å–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å'
            self.state.adaptation_strategy = "progressive"
        
        adaptation_plan = {
            'action': action,
            'reason': reason,
            'strategy': self.state.adaptation_strategy,
            'current_assessment': self.state.self_assessment,
            'learning_goals': self.state.learning_goals[:3],  # –¢–æ–ø-3 —Ü–µ–ª–∏
            'strengths': self.state.strengths[-3:],  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
            'weaknesses': self.state.weaknesses[-3:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã
        }
        
        logger.info(f"‚úÖ [{self.agent_name}] –ê–¥–∞–ø—Ç–∞—Ü–∏—è: {action} - {reason}")
        
        return adaptation_plan
    
    def get_state(self) -> Dict:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ"""
        return asdict(self.state)
    
    def save_state(self, filepath: str):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –≤ —Ñ–∞–π–ª"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.get_state(), f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {filepath}")
    
    def load_state(self, filepath: str):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∏–∑ —Ñ–∞–π–ª–∞"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.state = MetacognitiveState(**data)
        logger.info(f"üìÇ –°–æ—Å—Ç–æ—è–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {filepath}")


# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Victoria Enhanced
async def integrate_metacognitive_learning(agent_name: str = "Victoria"):
    """–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –≤ –∞–≥–µ–Ω—Ç–∞"""
    learner = MetacognitiveLearner(agent_name=agent_name)
    
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
    task_performance = {
        'success_rate': 0.85,
        'avg_quality': 0.78,
        'feedback_scores': [0.8, 0.9, 0.75]
    }
    
    # –°–∞–º–æ–æ—Ü–µ–Ω–∫–∞
    assessment = await learner.self_assess(task_performance)
    
    # –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è
    goals = await learner.plan_learning(
        current_knowledge=["Python", "AI"],
        gaps=["Advanced ML", "Distributed Systems"]
    )
    
    # –û—Ü–µ–Ω–∫–∞ –æ–ø—ã—Ç–∞
    evaluation = await learner.evaluate_learning({
        'topic': "Advanced ML",
        'outcome': 'success',
        'effectiveness': 0.85
    })
    
    # –ê–¥–∞–ø—Ç–∞—Ü–∏—è
    adaptation = await learner.adapt_learning_process()
    
    return learner, assessment, goals, evaluation, adaptation


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(integrate_metacognitive_learning())
