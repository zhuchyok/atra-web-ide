"""
Dashboard Daily Improver ‚Äî –µ–∂–µ–¥–Ω–µ–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏ (Singularity 10.0)

–≠–∫—Å–ø–µ—Ä—Ç—ã (Frontend, UX, QA, Performance, Product) –∞–Ω–∞–ª–∏–∑–∏—Ä—É—é—Ç:
- –û—à–∏–±–∫–∏: –ø—É—Å—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è, –ø–∞–¥–µ–Ω–∏—è –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö
- –ù–µ–¥–æ—á—ë—Ç—ã: –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫, –∏–∑–±—ã—Ç–æ—á–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
- –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: –∫–∞–∫–∏–µ –≤—ã–≤–æ–¥—è—Ç—Å—è, –ø–æ–ª–µ–∑–Ω—ã –ª–∏
- –°—Ç—Ä–∞–Ω–∏—Ü—ã: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –∏–µ—Ä–∞—Ä—Ö–∏—è, –Ω–∞–≤–∏–≥–∞—Ü–∏—è
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: st.cache_data max_entries, LEFT(content,N), lazy load

–ú–∏—Ä–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏: Streamlit best practices, Grafana/McKinsey dashboards
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)

_DASHBOARD_DIR = Path(__file__).resolve().parent.parent / "dashboard"
_DB_URL = os.getenv("DATABASE_URL", "postgresql://admin:secret@localhost:5432/knowledge_os")


async def _create_dashboard_improvement_tasks(conn) -> int:
    """–°–æ–∑–¥–∞—ë—Ç –∑–∞–¥–∞—á–∏ –Ω–∞ —É–ª—É—á—à–µ–Ω–∏–µ –¥–∞—à–±–æ—Ä–¥–∞ –≤ tasks."""
    try:
        import asyncpg
    except ImportError:
        return 0

    tasks_created = 0
    # –ß–µ–∫–ª–∏—Å—Ç –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –ø—Ä–æ–≥–æ–Ω–∞ (–º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —á–µ—Ä–µ–∑ LLM)
    checklist = [
        ("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å max_entries –≤ st.cache_data", "medium", "Frontend/Performance", "DASHBOARD_OPTIMIZATION_PLAN: max_entries=100"),
        ("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å LEFT(content,N) –≤ –∑–∞–ø—Ä–æ—Å–∞—Ö –∫ knowledge_nodes", "medium", "Backend", "–ò–∑–±–µ–≥–∞—Ç—å –∑–∞–≥—Ä—É–∑–∫–∏ –ø–æ–ª–Ω–æ–≥–æ content"),
        ("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å lazy load –≤–∫–ª–∞–¥–æ–∫ (st.fragment)", "low", "Frontend", "Streamlit best practices"),
        ("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—É—Å—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è –∏ fallback –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö", "high", "QA", "–û—à–∏–±–∫–∏: –ø—É—Å—Ç—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"),
        ("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–µ–∂–¥—É –≤–∫–ª–∞–¥–∫–∞–º–∏", "low", "Product", "–ù–µ–¥–æ—á—ë—Ç—ã: –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ"),
    ]

    victoria_id = await conn.fetchval("SELECT id FROM experts WHERE name ILIKE $1 LIMIT 1", "–í–∏–∫—Ç–æ—Ä–∏—è")
    if not victoria_id:
        logger.warning("Expert Victoria not found, skipping dashboard tasks")
        return 0
    domain_id = await conn.fetchval("SELECT id FROM domains WHERE name ILIKE $1 LIMIT 1", "Dashboard")
    if not domain_id:
        await conn.execute(
            "INSERT INTO domains (name, description) VALUES ($1, $2) ON CONFLICT (name) DO NOTHING",
            "Dashboard",
            "Dashboard improvements and analytics",
        )
        domain_id = await conn.fetchval("SELECT id FROM domains WHERE name ILIKE $1 LIMIT 1", "Dashboard")

    for title, priority, assignee_hint, description in checklist:
        full_title = f"üìä –î–∞—à–±–æ—Ä–¥: {title}"
        # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è: –Ω–µ —Å–æ–∑–¥–∞—ë–º –µ—Å–ª–∏ —Ç–∞–∫–∞—è –∑–∞–¥–∞—á–∞ —É–∂–µ –µ—Å—Ç—å –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24—á
        existing = await conn.fetchval("""
            SELECT 1 FROM tasks
            WHERE title = $1 AND created_at > NOW() - INTERVAL '24 hours'
            LIMIT 1
        """, full_title)
        if existing:
            continue
        metadata = json.dumps({"source": "dashboard_daily_improver", "assignee_hint": assignee_hint})
        await conn.execute("""
            INSERT INTO tasks (title, description, status, priority, creator_expert_id, domain_id, metadata)
            VALUES ($1, $2, 'pending', $3, $4, $5, $6::jsonb)
        """, full_title, description, priority, victoria_id, domain_id, metadata)
        tasks_created += 1

    return tasks_created


async def _log_improvement_to_knowledge(conn, summary: str) -> bool:
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª–æ–≥ —Ü–∏–∫–ª–∞ —É–ª—É—á—à–µ–Ω–∏–π –≤ knowledge_nodes (domain: Dashboard)."""
    try:
        domain_id = await conn.fetchval("SELECT id FROM domains WHERE name ILIKE $1 LIMIT 1", "Dashboard")
        if not domain_id:
            return False
        metadata = json.dumps({"source": "dashboard_daily_improver", "cycle": datetime.now().isoformat()})
        await conn.execute("""
            INSERT INTO knowledge_nodes (domain_id, content, metadata, confidence_score, source_ref)
            VALUES ($1, $2, $3::jsonb, 0.8, 'dashboard_improvement_cycle')
        """, domain_id, summary[:2000], metadata)
        return True
    except Exception as e:
        logger.warning("Could not log to knowledge_nodes: %s", e)
        return False


async def run_dashboard_improvement_cycle() -> Dict[str, Any]:
    """
    –ó–∞–ø—É—Å–∫–∞–µ—Ç —Ü–∏–∫–ª –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ —É–ª—É—á—à–µ–Ω–∏—è –¥–∞—à–±–æ—Ä–¥–∞.
    –°–æ–∑–¥–∞—ë—Ç –∑–∞–¥–∞—á–∏ –≤ tasks –∏ –ª–æ–≥–∏—Ä—É–µ—Ç –≤ knowledge_nodes.
    """
    try:
        import asyncpg
    except ImportError:
        logger.warning("asyncpg not available for dashboard_daily_improver")
        return {"tasks_created": 0, "logged": False}

    try:
        conn = await asyncpg.connect(_DB_URL)
        try:
            tasks_created = await _create_dashboard_improvement_tasks(conn)
            summary = f"Dashboard improvement cycle: {tasks_created} tasks created at {datetime.now().isoformat()}"
            logged = await _log_improvement_to_knowledge(conn, summary)
            logger.info("[DASHBOARD_IMPROVER] %s", summary)
            return {"tasks_created": tasks_created, "logged": logged}
        finally:
            await conn.close()
    except Exception as e:
        logger.error("dashboard_daily_improver failed: %s", e, exc_info=True)
        return {"tasks_created": 0, "logged": False}
