"""
Dashboard Daily Improver â€” ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð´Ð°ÑˆÐ±Ð¾Ñ€Ð´Ð° ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð°Ð¼Ð¸ (Singularity 10.0)

Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð´Ð°ÑˆÐ±Ð¾Ñ€Ð´ (ÐºÐ¾Ð´ Ð¸ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹), Ð·Ð°Ñ‚ÐµÐ¼ ÑÐ¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð½Ð° ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ.
Ð­ÐºÑÐ¿ÐµÑ€Ñ‚Ñ‹ (Frontend, UX, QA, Performance, Product): Ð¾ÑˆÐ¸Ð±ÐºÐ¸, Ð½ÐµÐ´Ð¾Ñ‡Ñ‘Ñ‚Ñ‹, Ð¿Ñ€Ð¾Ð¸Ð·Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ.
ÐœÐ¸Ñ€Ð¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ¸: Streamlit best practices, Grafana/McKinsey dashboards.
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

logger = logging.getLogger(__name__)

_DASHBOARD_DIR = Path(__file__).resolve().parent.parent / "dashboard"
_DB_URL = os.getenv("DATABASE_URL", "postgresql://admin:secret@localhost:5432/knowledge_os")

# ÐžÐ´Ð¸Ð½ ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚: (title_short, priority, assignee_hint, description, auto_apply_safe)
# auto_apply_safe: Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð½Ð¸Ð·ÐºÐ¾Ñ€Ð¸ÑÐºÐ¾Ð²Ñ‹Ñ… Ð¼ÐµÑ…Ð°Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð°Ð²Ð¾Ðº (max_entries)
ChecklistItem = Tuple[str, str, str, str, bool]


def _analyze_dashboard_code() -> List[ChecklistItem]:
    """
    ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ¾Ð´ Ð´Ð°ÑˆÐ±Ð¾Ñ€Ð´Ð° Ð¸ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹ Ð¿Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸ÑŽ.
    Ð¡ÐºÐ°Ð½Ð¸Ñ€ÑƒÐµÑ‚ app.py Ð½Ð° Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹: ÐºÑÑˆ Ð±ÐµÐ· max_entries, Ð·Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ð±ÐµÐ· LIMIT/LEFT(content,N),
    Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ðµ st.fragment, Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ, Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ.
    """
    findings: List[ChecklistItem] = []
    app_py = _DASHBOARD_DIR / "app.py"
    if not app_py.exists():
        logger.warning("Dashboard app.py not found at %s", app_py)
        return findings

    try:
        text = app_py.read_text(encoding="utf-8", errors="replace")
    except Exception as e:
        logger.warning("Could not read dashboard app.py: %s", e)
        return findings

    # 1) st.cache_data Ð±ÐµÐ· max_entries â€” Ñ€Ð¸ÑÐº Ð½ÐµÐ¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ Ñ€Ð¾ÑÑ‚Ð° ÐºÑÑˆÐ° (auto_apply_safe)
    if "st.cache_data" in text and "max_entries" not in text and "max_entries=" not in text:
        findings.append((
            "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ max_entries Ð² st.cache_data",
            "medium",
            "Frontend/Performance",
            "DASHBOARD_OPTIMIZATION_PLAN: Ð·Ð°Ð´Ð°Ñ‚ÑŒ max_entries (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ 100) Ð² st.cache_data, Ð¸Ð½Ð°Ñ‡Ðµ ÐºÑÑˆ Ñ€Ð°ÑÑ‚Ñ‘Ñ‚ Ð±ÐµÐ· Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ.",
            True,  # auto_apply_safe â€” Ð¼ÐµÑ…Ð°Ð½Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð·Ð°Ð¼ÐµÐ½Ð°
        ))

    # 2) Ð—Ð°Ð¿Ñ€Ð¾ÑÑ‹ Ðº knowledge_nodes Ñ Ð¿Ð¾Ð»Ð½Ñ‹Ð¼ content (Ð±ÐµÐ· LEFT/substring)
    if "knowledge_nodes" in text and ("LEFT(content" not in text and "content," in text or re.search(r"SELECT\s+[^F]*content\s+FROM\s+.*knowledge_nodes", text, re.I | re.S)):
        if "LEFT(content" not in text and "substring(content" not in text:
            findings.append((
                "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ LEFT(content,N) Ð² Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ñ… Ðº knowledge_nodes",
                "medium",
                "Backend",
                "Ð˜Ð·Ð±ÐµÐ³Ð°Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ content: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ LEFT(content, N) Ð¸Ð»Ð¸ substring Ð´Ð»Ñ Ð±Ð¾Ð»ÑŒÑˆÐ¸Ñ… Ð¿Ð¾Ð»ÐµÐ¹.",
                False,
            ))

    # 3) Lazy load Ð²ÐºÐ»Ð°Ð´Ð¾Ðº (st.fragment) â€” Streamlit best practice
    if "st.tabs" in text and "st.fragment" not in text:
        findings.append((
            "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ lazy load Ð²ÐºÐ»Ð°Ð´Ð¾Ðº (st.fragment)",
            "low",
            "Frontend",
            "Streamlit best practices: Ñ€Ð°ÑÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ st.fragment Ð´Ð»Ñ Ñ‚ÑÐ¶Ñ‘Ð»Ñ‹Ñ… Ð²ÐºÐ»Ð°Ð´Ð¾Ðº, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð²ÑÑ‘ Ð¿Ñ€Ð¸ Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚Ð¸Ð¸.",
            False,
        ))

    # 4) ÐŸÑƒÑÑ‚Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ: Ð½Ð°Ð»Ð¸Ñ‡Ð¸Ðµ try/except Ð¸ fallback Ð¿Ñ€Ð¸ Ð¿ÑƒÑÑ‚Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
    fetch_or_query = "fetch_data" in text or "get_db_connection" in text
    if fetch_or_query:
        if "st.info" not in text and "st.warning" not in text and "Ð¿ÑƒÑÑ‚" not in text.lower() and "Ð½ÐµÑ‚ Ð´Ð°Ð½" not in text.lower():
            findings.append((
                "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¸ fallback Ð¿Ñ€Ð¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…",
                "high",
                "QA",
                "ÐžÑˆÐ¸Ð±ÐºÐ¸: Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÑÐ²Ð½Ñ‹Ðµ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ (st.info/st.empty) Ð¸ fallback Ð¿Ñ€Ð¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð² Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ñ….",
                False,
            ))

    # 5) Ð”ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸Ðº: Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¾ st.metric Ñ Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ð¼Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸ÑÐ¼Ð¸
    metric_count = len(re.findall(r"st\.metric\s*\(", text))
    if metric_count > 10:
        findings.append((
            "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÐºÐ»Ð°Ð´ÐºÐ°Ð¼Ð¸",
            "low",
            "Product",
            "ÐÐµÐ´Ð¾Ñ‡Ñ‘Ñ‚Ñ‹: Ð¼Ð½Ð¾Ð³Ð¾ st.metric â€” Ð¿Ñ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÐºÐ»Ð°Ð´ÐºÐ°Ð¼Ð¸ Ð¸ Ð²Ñ‹Ð½ÐµÑÑ‚Ð¸ Ð¾Ð±Ñ‰Ð¸Ðµ Ð² Ð¿ÐµÑ€ÐµÐ¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼Ñ‹Ðµ Ð±Ð»Ð¾ÐºÐ¸.",
            False,
        ))

    return findings


def _apply_max_entries_patch(app_py_path: Path) -> bool:
    """
    Ð‘ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ð¹ Ð¿Ð°Ñ‚Ñ‡: Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ max_entries=100 Ð² st.cache_data Ð±ÐµÐ· max_entries.
    Ð¢Ð¾Ð»ÑŒÐºÐ¾ Ð´Ð»Ñ Ð´ÐµÐºÐ¾Ñ€Ð°Ñ‚Ð¾Ñ€Ð¾Ð² @st.cache_data Ð¸ @st.cache_data(...).
    ÐÐµ Ñ‚Ñ€Ð¾Ð³Ð°ÐµÑ‚ st.cache_data.clear().
    Living Organism Â§3, AUTO_APPLY_DASHBOARD.
    """
    try:
        text = app_py_path.read_text(encoding="utf-8", errors="replace")
    except Exception as e:
        logger.warning("Could not read %s for auto-apply: %s", app_py_path, e)
        return False

    if "max_entries" in text:
        return False  # ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ

    original = text

    # @st.cache_data Ð±ÐµÐ· ÑÐºÐ¾Ð±Ð¾Ðº â†’ @st.cache_data(max_entries=100)
    text = re.sub(r"@st\.cache_data\b(?!\()", "@st.cache_data(max_entries=100)", text)

    # @st.cache_data() Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐºÐ¾Ð±ÐºÐ¸ â†’ @st.cache_data(max_entries=100)
    text = re.sub(r"@st\.cache_data\s*\(\s*\)", "@st.cache_data(max_entries=100)", text)

    # @st.cache_data(...) Ñ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸ â€” Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ max_entries Ð¿ÐµÑ€ÐµÐ´ Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°ÑŽÑ‰ÐµÐ¹ )
    def _add_max_entries(match: re.Match) -> str:
        inner = match.group(1)
        if "max_entries" in inner:
            return match.group(0)
        return f"@st.cache_data({inner}, max_entries=100)"

    text = re.sub(r"@st\.cache_data\s*\(([^)]*)\)", _add_max_entries, text)

    if text != original:
        try:
            app_py_path.write_text(text, encoding="utf-8")
            logger.info("[DASHBOARD_IMPROVER] auto-applied max_entries to %s", app_py_path)
            return True
        except Exception as e:
            logger.warning("Could not write %s after auto-apply: %s", app_py_path, e)
            return False
    return False


def _get_fallback_checklist() -> List[ChecklistItem]:
    """Ð§ÐµÐºÐ»Ð¸ÑÑ‚ Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ, ÐµÑÐ»Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· Ð½Ðµ Ð²ÐµÑ€Ð½ÑƒÐ» Ð¿Ñ€ÐµÐ´Ð»Ð¾Ð¶ÐµÐ½Ð¸Ð¹."""
    return [
        ("ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ max_entries Ð² st.cache_data", "medium", "Frontend/Performance", "DASHBOARD_OPTIMIZATION_PLAN: max_entries=100", True),
        ("ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ LEFT(content,N) Ð² Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°Ñ… Ðº knowledge_nodes", "medium", "Backend", "Ð˜Ð·Ð±ÐµÐ³Ð°Ñ‚ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ Ð¿Ð¾Ð»Ð½Ð¾Ð³Ð¾ content", False),
        ("ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ lazy load Ð²ÐºÐ»Ð°Ð´Ð¾Ðº (st.fragment)", "low", "Frontend", "Streamlit best practices", False),
        ("ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ Ð¸ fallback Ð¿Ñ€Ð¸ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²Ð¸Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ñ…", "high", "QA", "ÐžÑˆÐ¸Ð±ÐºÐ¸: Ð¿ÑƒÑÑ‚Ñ‹Ðµ ÑÐ¾ÑÑ‚Ð¾ÑÐ½Ð¸Ñ", False),
        ("ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÑ‚Ñ€Ð¸Ðº Ð¼ÐµÐ¶Ð´Ñƒ Ð²ÐºÐ»Ð°Ð´ÐºÐ°Ð¼Ð¸", "low", "Product", "ÐÐµÐ´Ð¾Ñ‡Ñ‘Ñ‚Ñ‹: Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ", False),
    ]


async def _create_dashboard_improvement_tasks(conn, checklist: List[ChecklistItem]) -> int:
    """Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð½Ð° ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ Ð´Ð°ÑˆÐ±Ð¾Ñ€Ð´Ð° Ð² tasks Ð¿Ð¾ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½Ð½Ð¾Ð¼Ñƒ Ñ‡ÐµÐºÐ»Ð¸ÑÑ‚Ñƒ (Ð¸Ð· Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¸Ð»Ð¸ fallback)."""
    try:
        import asyncpg
    except ImportError:
        return 0

    tasks_created = 0
    if not checklist:
        return 0

    victoria_id = await conn.fetchval("SELECT id FROM experts WHERE name ILIKE $1 LIMIT 1", "Ð’Ð¸ÐºÑ‚Ð¾Ñ€Ð¸Ñ")
    if not victoria_id:
        logger.warning("Expert Victoria not found, skipping dashboard tasks")
        return 0
    domain_id = await conn.fetchval("SELECT id FROM domains WHERE name ILIKE $1 LIMIT 1", "Dashboard")
    if not domain_id:
        await conn.execute(
            "INSERT INTO domains (name, description) VALUES ($1, $2) ON CONFLICT (name) DO NOTHING",
            "Dashboard",
            "Dashboard improvements and analytics",
        )
        domain_id = await conn.fetchval("SELECT id FROM domains WHERE name ILIKE $1 LIMIT 1", "Dashboard")

    for item in checklist:
        title, priority, assignee_hint, description = item[0], item[1], item[2], item[3]
        full_title = f"ðŸ“Š Ð”Ð°ÑˆÐ±Ð¾Ñ€Ð´: {title}"
        # Ð˜Ð·Ð±ÐµÐ³Ð°ÐµÐ¼ Ð´ÑƒÐ±Ð»Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ: Ð½Ðµ ÑÐ¾Ð·Ð´Ð°Ñ‘Ð¼ ÐµÑÐ»Ð¸ Ñ‚Ð°ÐºÐ°Ñ Ð·Ð°Ð´Ð°Ñ‡Ð° ÑƒÐ¶Ðµ ÐµÑÑ‚ÑŒ Ð·Ð° Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ 24Ñ‡
        existing = await conn.fetchval("""
            SELECT 1 FROM tasks
            WHERE title = $1 AND created_at > NOW() - INTERVAL '24 hours'
            LIMIT 1
        """, full_title)
        if existing:
            continue
        metadata = json.dumps({"source": "dashboard_daily_improver", "assignee_hint": assignee_hint})
        await conn.execute("""
            INSERT INTO tasks (title, description, status, priority, creator_expert_id, domain_id, metadata)
            VALUES ($1, $2, 'pending', $3, $4, $5, $6::jsonb)
        """, full_title, description, priority, victoria_id, domain_id, metadata)
        tasks_created += 1

    return tasks_created


async def _log_improvement_to_knowledge(conn, summary: str) -> bool:
    """Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÑ‚ Ð»Ð¾Ð³ Ñ†Ð¸ÐºÐ»Ð° ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ð¹ Ð² knowledge_nodes (domain: Dashboard)."""
    try:
        domain_id = await conn.fetchval("SELECT id FROM domains WHERE name ILIKE $1 LIMIT 1", "Dashboard")
        if not domain_id:
            return False
        metadata = json.dumps({"source": "dashboard_daily_improver", "cycle": datetime.now().isoformat()})
        content_kn = summary[:2000]
        embedding = None
        try:
            from semantic_cache import get_embedding
            embedding = await get_embedding(content_kn[:8000])
        except Exception:
            pass
        if embedding is not None:
            await conn.execute("""
                INSERT INTO knowledge_nodes (domain_id, content, metadata, confidence_score, source_ref, embedding)
                VALUES ($1, $2, $3::jsonb, 0.8, 'dashboard_improvement_cycle', $4::vector)
            """, domain_id, content_kn, metadata, str(embedding))
        else:
            await conn.execute("""
                INSERT INTO knowledge_nodes (domain_id, content, metadata, confidence_score, source_ref)
                VALUES ($1, $2, $3::jsonb, 0.8, 'dashboard_improvement_cycle')
            """, domain_id, content_kn, metadata)
        return True
    except Exception as e:
        logger.warning("Could not log to knowledge_nodes: %s", e)
        return False


async def run_dashboard_improvement_cycle() -> Dict[str, Any]:
    """
    Ð—Ð°Ð¿ÑƒÑÐºÐ°ÐµÑ‚ Ñ†Ð¸ÐºÐ» ÐµÐ¶ÐµÐ´Ð½ÐµÐ²Ð½Ð¾Ð³Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ Ð´Ð°ÑˆÐ±Ð¾Ñ€Ð´Ð°.
    1) ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ ÐºÐ¾Ð´ Ð´Ð°ÑˆÐ±Ð¾Ñ€Ð´Ð° (_analyze_dashboard_code).
    2) Ð•ÑÐ»Ð¸ AUTO_APPLY_DASHBOARD=true â€” Ð´Ð»Ñ Ð±ÐµÐ·Ð¾Ð¿Ð°ÑÐ½Ñ‹Ñ… Ð¿Ñ€Ð°Ð²Ð¾Ðº (max_entries) Ð¿Ñ€Ð¸Ð¼ÐµÐ½ÑÐµÑ‚ Ð¿Ð°Ñ‚Ñ‡.
    3) Ð¡Ð¾Ð·Ð´Ð°Ñ‘Ñ‚ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¿Ð¾ Ð¾ÑÑ‚Ð°Ð²ÑˆÐ¸Ð¼ÑÑ Ð¿ÑƒÐ½ÐºÑ‚Ð°Ð¼ (ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¸Ð»Ð¸ Ð¿Ð¾ÑÐ»Ðµ Ð½ÐµÑƒÑÐ¿ÐµÑˆÐ½Ð¾Ð³Ð¾ auto-apply).
    4) Ð›Ð¾Ð³Ð¸Ñ€ÑƒÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ Ð² knowledge_nodes.
    """
    try:
        import asyncpg
    except ImportError:
        logger.warning("asyncpg not available for dashboard_daily_improver")
        return {"tasks_created": 0, "logged": False, "from_analysis": False, "auto_applied": False}

    auto_applied = False
    try:
        analysis_findings = _analyze_dashboard_code()
        from_analysis = len(analysis_findings) > 0
        if not analysis_findings:
            checklist = _get_fallback_checklist()
            analysis_note = "used fallback checklist (analysis returned empty or file not found)"
        else:
            checklist = analysis_findings
            analysis_note = f"analysis found {len(checklist)} improvement(s)"

        # Auto-apply safe patches (Living Organism Â§3)
        if os.getenv("AUTO_APPLY_DASHBOARD", "").lower() in ("1", "true", "yes"):
            for item in checklist:
                if len(item) >= 5 and item[4] and item[0] == "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ max_entries Ð² st.cache_data":
                    app_py = _DASHBOARD_DIR / "app.py"
                    if app_py.exists():
                        if _apply_max_entries_patch(app_py):
                            auto_applied = True
                            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ ÑÑ‚Ð¾Ñ‚ Ð¿ÑƒÐ½ÐºÑ‚ Ð¸Ð· Ñ‡ÐµÐºÐ»Ð¸ÑÑ‚Ð° â€” Ð·Ð°Ð´Ð°Ñ‡Ð° Ð½Ðµ Ð½ÑƒÐ¶Ð½Ð°
                            checklist = [c for c in checklist if not (len(c) >= 5 and c[4] and c[0] == "ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ max_entries Ð² st.cache_data")]
                        break

        logger.info("[DASHBOARD_IMPROVER] %s", analysis_note)

        conn = await asyncpg.connect(_DB_URL)
        try:
            tasks_created = await _create_dashboard_improvement_tasks(conn, checklist)
            summary = f"Dashboard improvement cycle: {analysis_note}; {tasks_created} tasks created; auto_applied={auto_applied} at {datetime.now().isoformat()}"
            logged = await _log_improvement_to_knowledge(conn, summary)
            logger.info("[DASHBOARD_IMPROVER] %s", summary)
            return {"tasks_created": tasks_created, "logged": logged, "from_analysis": from_analysis, "auto_applied": auto_applied}
        finally:
            await conn.close()
    except Exception as e:
        logger.error("dashboard_daily_improver failed: %s", e, exc_info=True)
        return {"tasks_created": 0, "logged": False, "from_analysis": False, "auto_applied": False}
