import os
import logging
import json
from typing import List, Dict, Optional
import asyncpg
try:
    from semantic_cache import get_embedding
    from db_pool import get_pool
except ImportError:
    from app.semantic_cache import get_embedding
    from app.db_pool import get_pool

logger = logging.getLogger(__name__)

class KnowledgeService:
    """
    –ú–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π (RAG, —ç–º–±–µ–¥–¥–∏–Ω–≥–∏).
    –ò–∑–æ–ª–∏—Ä—É–µ—Ç —Ç—è–∂–µ–ª—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –ë–î –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏.
    """
    def __init__(self):
        self.db_url = os.getenv("DATABASE_URL")

    async def _get_knowledge_context(self, query: str) -> str:
        """Retrieve relevant knowledge nodes (RAG) - –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ + AI Research (Singularity 10.0)."""
        try:
            embedding = await get_embedding(query)
            if not embedding: return ""
            pool = await get_pool()
            
            async with pool.acquire() as conn:
                # –ü–æ–∏—Å–∫ –ø–æ –¥–≤—É–º –æ—Å–Ω–æ–≤–Ω—ã–º –¥–æ–º–µ–Ω–∞–º: –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏ AI Research
                rows = await conn.fetch("""
                    SELECT content, metadata, (1 - (embedding <=> $1::vector)) as similarity
                    FROM knowledge_nodes
                    WHERE embedding IS NOT NULL
                    AND (
                        domain_id = (SELECT id FROM domains WHERE name = 'AI Research' LIMIT 1)
                        OR domain_id = (SELECT id FROM domains WHERE name = 'victoria_tasks' LIMIT 1)
                        OR metadata->>'source' = 'external_docs_indexer'
                    )
                    AND confidence_score >= 0.3
                    ORDER BY similarity DESC LIMIT 7
                """, embedding)
                
                if not rows: return ""
                
                context = "\nüìö [KNOWLEDGE CONTEXT (AI Research & Corp)]:\n"
                for row in rows:
                    if row['similarity'] >= 0.6:
                        meta = row['metadata'] or {}
                        source = meta.get('source', 'unknown')
                        file_path = meta.get('file_path', 'N/A')
                        
                        if source == 'external_docs_indexer':
                            context += f"\n[AI RESEARCH: {file_path}] (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {row['similarity']:.2f}):\n"
                        else:
                            context += f"\n[–ö–û–†–ü–û–†–ê–¶–ò–Ø] (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {row['similarity']:.2f}):\n"
                        
                        context += f"{row['content'][:1000]}\n"
                return context
        except Exception as exc:
            logger.error(f"Knowledge retrieval error: {exc}")
            return ""

    async def save_insight(self, content: str, expert_name: str, metadata: Dict = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –Ω–æ–≤—ã–π –∏–Ω—Å–∞–π—Ç –≤ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π."""
        embedding = await get_embedding(content[:1000])
        
        # –°–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å 10.0: –ï—Å–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥ –Ω–µ –ø–æ–ª—É—á–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –Ω—É–∂–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
        if not embedding:
            logger.warning(f"‚ö†Ô∏è [KNOWLEDGE SERVICE] –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∏–Ω—Å–∞–π—Ç–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä")
            embedding = [0.0] * 768

        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–ø–∏—Å–æ–∫ –≤ —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ '[1.2, 3.4, ...]' –¥–ª—è pgvector, –µ—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
        if isinstance(embedding, list):
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ (pgvector —Ç—Ä–µ–±—É–µ—Ç —Å—Ç—Ä–æ–≥–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è)
            if len(embedding) != 768:
                logger.warning(f"‚ö†Ô∏è [KNOWLEDGE SERVICE] –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ {len(embedding)} != 768. –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º.")
                if len(embedding) > 768:
                    embedding = embedding[:768]
                else:
                    embedding = embedding + [0.0] * (768 - len(embedding))
            embedding_str = "[" + ",".join(map(str, embedding)) + "]"
        else:
            embedding_str = embedding

        pool = await get_pool()
        async with pool.acquire() as conn:
            # –°–∏–Ω–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å 10.0: –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ metadata ‚Äî —ç—Ç–æ JSON-—Å—Ç—Ä–æ–∫–∞ –¥–ª—è PostgreSQL
            metadata_json = json.dumps(metadata or {})
            await conn.execute("""
                INSERT INTO knowledge_nodes (content, domain_id, confidence_score, embedding, is_verified, metadata)
                VALUES ($1, (SELECT id FROM domains WHERE name = 'victoria_tasks' LIMIT 1), 0.9, $2, TRUE, $3::jsonb)
            """, content, embedding_str, metadata_json)
            logger.info(f"üìö [KNOWLEDGE SERVICE] –°–æ—Ö—Ä–∞–Ω–µ–Ω –∏–Ω—Å–∞–π—Ç –æ—Ç {expert_name}")

knowledge_service = KnowledgeService()
