"""
MLX API Server –¥–ª—è Mac Studio M4 Max
FastAPI —Å–µ—Ä–≤–µ—Ä –¥–ª—è –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –æ—Ç –∞–≥–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ MLX –º–æ–¥–µ–ª–∏
–£—Å—Ç–æ–π—á–∏–≤—ã–π —Å–µ—Ä–≤–µ—Ä —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –ø–∞–º—è—Ç–∏ –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ–º
"""

import asyncio
import logging
from contextlib import asynccontextmanager
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict
import json
import os
import time
import threading
from datetime import datetime, timedelta
from collections import defaultdict
from mlx_lm import load, generate
import sys
import gc

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ mlx_router –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞
sys.path.insert(0, os.path.dirname(__file__))

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ —Ñ–∞–π–ª
log_dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "logs")
os.makedirs(log_dir, exist_ok=True)
log_file = os.path.join(log_dir, "mlx_api_server.log")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è: –∏ –≤ –∫–æ–Ω—Å–æ–ª—å, –∏ –≤ —Ñ–∞–π–ª
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(name)s | %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)
logger.info(f"üìù –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ MLX API Server: {log_file}")

# –ò–º–ø–æ—Ä—Ç psutil —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ (–ø–æ—Å–ª–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ logger)
try:
    import psutil
except ImportError:
    psutil = None
    logger.warning("‚ö†Ô∏è psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –±—É–¥–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω")


@asynccontextmanager
async def _mlx_lifespan(app: FastAPI):
    """Startup: –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞. Shutdown: –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ (–º–∏—Ä–æ–≤–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞: lifespan –≤–º–µ—Å—Ç–æ on_event)."""
    asyncio.create_task(preload_models())
    if _cache_cleanup_interval_sec > 0:
        asyncio.create_task(periodic_cache_cleanup())
        logger.info(
            "üîÑ –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π –∫–∞–∂–¥—ã–µ %ds (–º–∞–∫—Å %d –≤ –∫—ç—à–µ)",
            _cache_cleanup_interval_sec,
            _max_cached_models,
        )
    yield
    _models_cache.clear()
    logger.info("‚úÖ –ö—ç—à –º–æ–¥–µ–ª–µ–π –æ—á–∏—â–µ–Ω (shutdown)")


app = FastAPI(title="MLX Model Server", version="2.0.0", lifespan=_mlx_lifespan)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # –í –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ —É–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –¥–æ–º–µ–Ω—ã
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
# –°—Ç—Ä—É–∫—Ç—É—Ä–∞: {model_key: {"model": model, "tokenizer": tokenizer, "loaded_at": datetime, "last_used": datetime, "use_count": int}}
_models_cache = {}

# –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ (–Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ env ‚Äî –º–µ–Ω—å—à–µ 429, —á–∞—â–µ —É—Å–ø–µ—à–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã)
_active_requests = 0
_max_concurrent_requests = int(os.getenv("MLX_MAX_CONCURRENT", "5"))
# –°–µ–º–∞—Ñ–æ—Ä: –∑–∞–ø—Ä–æ—Å—ã –∂–¥—É—Ç —Å–ª–æ—Ç –≤–º–µ—Å—Ç–æ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ 503 (–¥–æ–∂–∏–¥–∞—é—Ç—Å—è –æ—á–µ—Ä–µ–¥–∏)
_concurrent_semaphore = asyncio.Semaphore(_max_concurrent_requests)
# –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Å–ª–æ—Ç–∞: –∏–∑ env –∏–ª–∏ –º–∞–∫—Å –ø–æ –æ—Ü–µ–Ω–∫–∞–º –º–æ–¥–µ–ª–µ–π (–∑–∞–≥—Ä—É–∑–∫–∞ + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å + –∑–∞–ø–∞—Å)
_queue_wait_timeout = None  # –∑–∞–¥–∞—ë—Ç—Å—è —á–µ—Ä–µ–∑ _max_queue_wait_timeout() –ø–æ—Å–ª–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è MODEL_TIME_ESTIMATES
_request_lock = threading.Lock()
_request_times = defaultdict(list)  # –î–ª—è rate limiting
_rate_limit_window = int(os.getenv("MLX_RATE_LIMIT_WINDOW", "90"))  # —Å–µ–∫—É–Ω–¥ (—É–≤–µ–ª–∏—á–µ–Ω–æ –æ–∫–Ω–æ ‚Äî —Ä–µ–∂–µ —É–ø–∏—Ä–∞–µ–º—Å—è –≤ –ª–∏–º–∏—Ç)
_rate_limit_max = int(os.getenv("MLX_RATE_LIMIT_MAX", "150"))  # –º–∞–∫—Å –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ –æ–∫–Ω–µ (—É–≤–µ–ª–∏—á–µ–Ω–æ ‚Äî –º–µ–Ω—å—à–µ 429)

# –û—á–µ—Ä–µ–¥—å –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
try:
    from app.mlx_request_queue import get_request_queue, RequestPriority
    REQUEST_QUEUE_AVAILABLE = True
except ImportError:
    REQUEST_QUEUE_AVAILABLE = False
    logger.debug("‚ÑπÔ∏è MLX Request Queue –Ω–µ –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É")

# –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –º–æ–¥–µ–ª—è–º (–∑–∞—â–∏—Ç–∞ –æ—Ç –≤—ã–≥—Ä—É–∑–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π)
_active_model_requests = defaultdict(int)  # {model_key: count} - —Å–∫–æ–ª—å–∫–æ –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
_model_locks = defaultdict(threading.Lock)  # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
_loading_models = set()  # –ú–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–µ–π—á–∞—Å –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è

# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ (–º–æ–∂–Ω–æ –∑–∞–¥–∞—Ç—å —á–µ—Ä–µ–∑ env, –µ—Å–ª–∏ 95% ‚Äî –Ω–æ—Ä–º–∞ –¥–ª—è –≤–∞—à–µ–π –Ω–∞–≥—Ä—É–∑–∫–∏)
_memory_warning_threshold = float(os.getenv("MLX_MEMORY_WARNING_PERCENT", "85")) / 100.0
_memory_critical_threshold = float(os.getenv("MLX_MEMORY_CRITICAL_PERCENT", "95")) / 100.0
_last_memory_check = 0
_memory_check_interval = 10  # —Å–µ–∫—É–Ω–¥

# –ú–∞–∫—Å–∏–º—É–º –º–æ–¥–µ–ª–µ–π –≤ –∫—ç—à–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (–æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤—ã–≥—Ä—É–∂–∞—é—Ç—Å—è –ø–æ LRU) ‚Äî —Å–Ω–∏–∂–∞–µ—Ç –ø–∏–∫–æ–≤–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ RAM
# –û–¥–Ω–∞ –±–æ–ª—å—à–∞—è –º–æ–¥–µ–ª—å (70b) ~40‚Äì50GB, 32b ~20GB; –ø—Ä–∏ 2 –º–æ–¥–µ–ª—è—Ö –≤ –∫—ç—à–µ –ø–∏–∫ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –Ω–∏–∂–µ
_max_cached_models = int(os.getenv("MLX_MAX_CACHED_MODELS", "2"))
# –ò–Ω—Ç–µ—Ä–≤–∞–ª —Ñ–æ–Ω–æ–≤–æ–π –æ—á–∏—Å—Ç–∫–∏ –∫—ç—à–∞ –ø–æ LRU (—Å–µ–∫—É–Ω–¥—ã); 0 = –æ—Ç–∫–ª—é—á–∏—Ç—å
_cache_cleanup_interval_sec = int(os.getenv("MLX_CACHE_CLEANUP_INTERVAL_SEC", "600"))

# –ú–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ (–º–æ–∂–Ω–æ –∏–∑–º–µ–Ω–∏—Ç—å —á–µ—Ä–µ–∑ MLX_PRELOAD_MODELS)
# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Ç–æ–ª—å–∫–æ "fast" (~2.5GB), —á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–∂–∞—Ç—å 32b (~20GB) –≤ –ø–∞–º—è—Ç–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ
# –ü—É—Å—Ç–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ = –±–µ–∑ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏; "default,fast" = –∫–∞–∫ —Ä–∞–Ω—å—à–µ (–±–æ–ª—å—à–µ RAM)
_preload_models_env = os.getenv("MLX_PRELOAD_MODELS", "fast")
_preload_models = [m.strip() for m in _preload_models_env.split(",") if m.strip()]

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (–ø—É—Ç–∏ –∫ MLX –º–æ–¥–µ–ª—è–º)
# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –∏–∑ ~/mlx-models/
MLX_BASE = os.path.expanduser("~/mlx-models")
MODEL_PATHS = {
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ (–¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
    "reasoning": os.path.join(MLX_BASE, "deepseek-r1-distill-llama-70b"),
    "coding": os.path.join(MLX_BASE, "qwen2.5-coder-32b"),
    "fast": os.path.join(MLX_BASE, "phi3.5-mini-4k"),
    "tiny": os.path.join(MLX_BASE, "tinyllama-1.1b-chat"),
    "qwen_3b": os.path.join(MLX_BASE, "qwen2.5-3b"),
    "phi3_mini": os.path.join(MLX_BASE, "phi3-mini-4k"),
    "default": os.path.join(MLX_BASE, "qwen2.5-coder-32b"),
    # –ú–æ–¥–µ–ª–∏ –∏–∑ PLAN.md (Ollama –∏–º–µ–Ω–∞ ‚Üí —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏)
    "command-r-plus:104b": os.path.join(MLX_BASE, "command-r-plus"),
    "deepseek-r1-distill-llama:70b": os.path.join(MLX_BASE, "deepseek-r1-distill-llama-70b"),
    "llama3.3:70b": os.path.join(MLX_BASE, "llama3.3-70b"),
    "qwen2.5-coder:32b": os.path.join(MLX_BASE, "qwen2.5-coder-32b"),
    "phi3.5:3.8b": os.path.join(MLX_BASE, "phi3.5-mini-4k"),
    "phi3:mini-4k": os.path.join(MLX_BASE, "phi3-mini-4k"),
    "qwen2.5:3b": os.path.join(MLX_BASE, "qwen2.5-3b"),
    "tinyllama:1.1b-chat": os.path.join(MLX_BASE, "tinyllama-1.1b-chat"),
}

# –ú–æ–∂–Ω–æ —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
# –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º ~/mlx-models/ (–Ω–∞–π–¥–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏)
MLX_MODELS_DIR = os.getenv('MLX_MODELS_DIR', os.path.expanduser("~/mlx-models"))

# Mapping –∫–∞—Ç–µ–≥–æ—Ä–∏–π –∫ –º–æ–¥–µ–ª—è–º
CATEGORY_TO_MODEL = {
    "reasoning": "reasoning",
    "coding": "coding",
    "code": "coding",
    "fast": "fast",
    "tiny": "tiny",
    "default": "default"
}

# –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏
PRELOAD_MODEL_MAP = {
    "default": "qwen2.5-coder:32b",  # –û—Å–Ω–æ–≤–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    "fast": "phi3.5:3.8b",  # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—â–∏—Ö –∑–∞–¥–∞—á
    # "tiny": "tinyllama:1.1b-chat",  # –ò—Å–∫–ª—é—á–µ–Ω–∞ - —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
    "coding": "qwen2.5-coder:32b",  # –î–ª—è –∫–æ–¥–∏–Ω–≥–∞
}

# –ú–∞–ø–ø–∏–Ω–≥ –∏–º–µ–Ω –º–æ–¥–µ–ª–µ–π –∏–∑ PLAN.md (Ollama —Ñ–æ—Ä–º–∞—Ç) –≤ MLX –∫–ª—é—á–∏
OLLAMA_TO_MLX_MAP = {
    "command-r-plus:104b": "command-r-plus:104b",
    "deepseek-r1-distill-llama:70b": "deepseek-r1-distill-llama:70b",
    "llama3.3:70b": "llama3.3:70b",
    "qwen2.5-coder:32b": "qwen2.5-coder:32b",
    "phi3.5:3.8b": "phi3.5:3.8b",
    "phi3:mini-4k": "phi3:mini-4k",
    "qwen2.5:3b": "qwen2.5:3b",
    "tinyllama:1.1b-chat": "tinyllama:1.1b-chat",
}

# –û—Ü–µ–Ω–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ –º–æ–¥–µ–ª—è–º: –∑–∞–≥—Ä—É–∑–∫–∞ (—Å–µ–∫), –∏–Ω—Ñ–µ—Ä–µ–Ω—Å (—Å–µ–∫ –Ω–∞ 1k —Ç–æ–∫–µ–Ω–æ–≤), –∑–∞–ø–∞—Å (—Å–µ–∫).
# –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ = load_sec + (max_tokens/1000 * inference_sec_per_1k) + margin_sec.
# –î–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏ ‚Äî fallback –ø–æ —Ä–∞–∑–º–µ—Ä—É (104b/70b/32b/3b/1b) –∏–ª–∏ default.
MODEL_TIME_ESTIMATES = {
    "default": {"load_sec": 60, "inference_sec_per_1k": 40, "margin_sec": 60},
    "command-r-plus:104b": {"load_sec": 180, "inference_sec_per_1k": 180, "margin_sec": 120},
    "deepseek-r1-distill-llama:70b": {"load_sec": 120, "inference_sec_per_1k": 120, "margin_sec": 120},
    "llama3.3:70b": {"load_sec": 120, "inference_sec_per_1k": 120, "margin_sec": 120},
    "reasoning": {"load_sec": 120, "inference_sec_per_1k": 120, "margin_sec": 120},
    "qwen2.5-coder:32b": {"load_sec": 60, "inference_sec_per_1k": 40, "margin_sec": 60},
    "coding": {"load_sec": 60, "inference_sec_per_1k": 40, "margin_sec": 60},
    "phi3.5:3.8b": {"load_sec": 25, "inference_sec_per_1k": 15, "margin_sec": 30},
    "phi3:mini-4k": {"load_sec": 25, "inference_sec_per_1k": 15, "margin_sec": 30},
    "fast": {"load_sec": 25, "inference_sec_per_1k": 15, "margin_sec": 30},
    "qwen2.5:3b": {"load_sec": 20, "inference_sec_per_1k": 12, "margin_sec": 25},
    "qwen_3b": {"load_sec": 20, "inference_sec_per_1k": 12, "margin_sec": 25},
    "tinyllama:1.1b-chat": {"load_sec": 10, "inference_sec_per_1k": 5, "margin_sec": 20},
    "tiny": {"load_sec": 10, "inference_sec_per_1k": 5, "margin_sec": 20},
}


def _get_estimates_for_model(model_key: str) -> dict:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤—Ä–µ–º–µ–Ω–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ (exact –∏–ª–∏ –ø–æ —Ä–∞–∑–º–µ—Ä—É 104b/70b/32b/3b/1b)."""
    if model_key in MODEL_TIME_ESTIMATES:
        return MODEL_TIME_ESTIMATES[model_key].copy()
    # Fallback –ø–æ —Ä–∞–∑–º–µ—Ä—É –∏–∑ –∏–º–µ–Ω–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä llama3.3:70b —É–∂–µ –µ—Å—Ç—å, –Ω–æ –Ω–∞ —Å–ª—É—á–∞–π –Ω–æ–≤—ã—Ö 70b)
    key_lower = model_key.lower()
    if "104b" in key_lower or "104" in key_lower:
        return {"load_sec": 180, "inference_sec_per_1k": 180, "margin_sec": 120}
    if "70b" in key_lower or "70" in key_lower:
        return {"load_sec": 120, "inference_sec_per_1k": 120, "margin_sec": 120}
    if "32b" in key_lower or "32" in key_lower:
        return {"load_sec": 60, "inference_sec_per_1k": 40, "margin_sec": 60}
    if "3b" in key_lower or "3.8" in key_lower or "4k" in key_lower:
        return {"load_sec": 25, "inference_sec_per_1k": 15, "margin_sec": 30}
    if "1b" in key_lower or "1.1" in key_lower:
        return {"load_sec": 10, "inference_sec_per_1k": 5, "margin_sec": 20}
    return MODEL_TIME_ESTIMATES["default"].copy()


def get_model_timeout_estimate(
    model_key: str,
    max_tokens: int,
    load_time_actual: Optional[float] = None,
) -> float:
    """
    –û—Ü–µ–Ω–∫–∞ –ø–æ–ª–Ω–æ–≥–æ —Ç–∞–π–º–∞—É—Ç–∞ –∑–∞–ø—Ä–æ—Å–∞: –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å + –∑–∞–ø–∞—Å.
    load_time_actual ‚Äî —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –∑–∞–≥—Ä—É–∑–∫–∏ (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –≤ –∫—ç—à–µ).
    """
    est = _get_estimates_for_model(model_key)
    load = load_time_actual if load_time_actual is not None else est["load_sec"]
    inference = (max_tokens / 1000.0) * est["inference_sec_per_1k"]
    total = load + inference + est["margin_sec"]
    return max(60.0, total)  # –º–∏–Ω–∏–º—É–º 1 –º–∏–Ω—É—Ç–∞


def _max_queue_wait_timeout() -> float:
    """–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ç–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Å–ª–æ—Ç–∞: –∏–∑ env –∏–ª–∏ –º–∞–∫—Å –ø–æ –≤—Å–µ–º –º–æ–¥–µ–ª—è–º (–∑–∞–≥—Ä—É–∑–∫–∞ + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å 2k + –∑–∞–ø–∞—Å)."""
    env_val = os.getenv("MLX_QUEUE_WAIT_TIMEOUT")
    if env_val is not None:
        try:
            return float(env_val)
        except ValueError:
            pass
    max_sec = 300.0
    for key in MODEL_TIME_ESTIMATES:
        t = get_model_timeout_estimate(key, max_tokens=2048, load_time_actual=None)
        if t > max_sec:
            max_sec = t
    return max_sec


def _get_queue_wait_timeout() -> float:
    """–¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Å–ª–æ—Ç–∞ –≤ middleware (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)."""
    global _queue_wait_timeout
    if _queue_wait_timeout is None:
        _queue_wait_timeout = _max_queue_wait_timeout()
        logger.info("‚è±Ô∏è –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Å–ª–æ—Ç–∞: %s —Å (–º–æ–¥–µ–ª–∏: –∑–∞–≥—Ä—É–∑–∫–∞ + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å + –∑–∞–ø–∞—Å)", _queue_wait_timeout)
    return _queue_wait_timeout


class GenerateRequest(BaseModel):
    prompt: str
    model: Optional[str] = None
    category: Optional[str] = None
    max_tokens: int = 512
    temperature: float = 0.7
    stream: bool = False


# Anthropic-compatible API models
class AnthropicMessage(BaseModel):
    role: str  # "user", "assistant", "system"
    content: str


class AnthropicMessagesRequest(BaseModel):
    model: str
    messages: List[AnthropicMessage]
    max_tokens: Optional[int] = 1024
    temperature: Optional[float] = 0.7
    stream: Optional[bool] = False


class ChatMessage(BaseModel):
    """–°–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è Ollama Chat API (/api/chat)"""
    role: str  # "user", "assistant", "system"
    content: str


class ChatRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –¥–ª—è Ollama Chat API (/api/chat)"""
    model: str
    messages: List[ChatMessage]
    stream: Optional[bool] = False
    options: Optional[Dict] = None  # temperature, num_predict –∏ –¥—Ä.


def check_memory() -> Dict[str, float]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏"""
    if psutil is None:
        return {"used_percent": 0.0, "available_percent": 1.0, "total_gb": 0.0, "available_gb": 0.0}
    try:
        memory = psutil.virtual_memory()
        return {
            "total_gb": memory.total / (1024**3),
            "available_gb": memory.available / (1024**3),
            "used_percent": memory.percent / 100,
            "available_percent": memory.available / memory.total
        }
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–∞–º—è—Ç–∏: {e}")
        return {"used_percent": 0.0, "available_percent": 1.0, "total_gb": 0.0, "available_gb": 0.0}


def evict_lru_to_limit(keep_max: int):
    """–í—ã–≥—Ä—É–∂–∞–µ—Ç –Ω–∞–∏–º–µ–Ω–µ–µ –Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏, –ø–æ–∫–∞ –≤ –∫—ç—à–µ –Ω–µ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –Ω–µ –±–æ–ª–µ–µ keep_max.
    –ù–µ —Ç—Ä–æ–≥–∞–µ—Ç –∞–∫—Ç–∏–≤–Ω—ã–µ –∏ –Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ (30 —Å). –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏.
    """
    if keep_max < 1:
        keep_max = 1
    try:
        cache_keys = list(_models_cache.keys())
    except (RuntimeError, AttributeError):
        cache_keys = []
    if len(cache_keys) <= keep_max:
        return
    with _request_lock:
        active_models = {k for k, v in _active_model_requests.items() if v > 0}
        loading_models = _loading_models.copy()
    protected = active_models | loading_models
    now = datetime.now()
    for key in cache_keys:
        if key in protected:
            continue
        model_data = _models_cache.get(key)
        if not model_data:
            continue
        last_used = model_data.get("last_used")
        if isinstance(last_used, datetime) and (now - last_used).total_seconds() < 30:
            protected.add(key)
    candidates = [
        (k, _models_cache.get(k, {}).get("last_used") or datetime.min)
        for k in cache_keys
        if k not in protected and k in _models_cache
    ]
    candidates.sort(key=lambda x: x[1])
    evicted = 0
    while len(_models_cache) > keep_max and candidates:
        key = candidates.pop(0)[0]
        if key not in _models_cache:
            continue
        with _request_lock:
            if _active_model_requests.get(key, 0) > 0:
                continue
        del _models_cache[key]
        evicted += 1
        logger.info(f"üóëÔ∏è LRU –≤—ã–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {key} (–ª–∏–º–∏—Ç –∫—ç—à–∞ {keep_max})")
    if evicted:
        gc.collect()


def cleanup_unused_models(aggressive: bool = False, keep_count: int = 1):
    """–û—á–∏—Å—Ç–∫–∞ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ –Ω–µ—Ö–≤–∞—Ç–∫–µ –ø–∞–º—è—Ç–∏ (LRU —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
    –ù–ï –≤—ã–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å!
    
    Args:
        aggressive: –ï—Å–ª–∏ True, –≤—ã–≥—Ä—É–∂–∞–µ—Ç –í–°–ï –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ (—ç–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞)
        keep_count: –°–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –æ—Å—Ç–∞–≤–∏—Ç—å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1 - —Å–∞–º–∞—è –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è)
    """
    memory_info = check_memory()
    initial_used = memory_info["used_percent"]
    
    if memory_info["used_percent"] > _memory_critical_threshold or aggressive:
        logger.warning(f"üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏: {memory_info['used_percent']*100:.1f}%")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å
        with _request_lock:
            active_models = {k for k, v in _active_model_requests.items() if v > 0}
            loading_models = _loading_models.copy()
        
        protected_models = active_models | loading_models
        
        # –ö–†–ò–¢–ò–ß–ù–û: –¢–∞–∫–∂–µ –∑–∞—â–∏—â–∞–µ–º –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å (–≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 —Å–µ–∫—É–Ω–¥)
        # –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤—ã–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–∫–æ–Ω—Ñ–ª–∏–∫—Ç Metal)
        now = datetime.now()
        # –ö–†–ò–¢–ò–ß–ù–û: –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é —Å–ø–∏—Å–∫–∞ –∫–ª—é—á–µ–π, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å RuntimeError –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Å–ª–æ–≤–∞—Ä—è –≤–æ –≤—Ä–µ–º—è –∏—Ç–µ—Ä–∞—Ü–∏–∏
        try:
            cache_keys = list(_models_cache.keys())
        except (RuntimeError, AttributeError) as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∫–ª—é—á–µ–π –∫—ç—à–∞: {e}")
            cache_keys = []
        
        for key in cache_keys:
            try:
                model_data = _models_cache.get(key)
                if not model_data:
                    continue
                last_used = model_data.get("last_used")
                if last_used and isinstance(last_used, datetime):
                    time_since_use = (now - last_used).total_seconds()
                    if time_since_use < 30:  # –ó–∞—â–∏—â–∞–µ–º –º–æ–¥–µ–ª–∏, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 —Å–µ–∫—É–Ω–¥
                        protected_models.add(key)
            except (KeyError, AttributeError, TypeError) as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–æ–¥–µ–ª–∏ {key}: {e}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                continue
        
        if protected_models:
            logger.info(f"üõ°Ô∏è –ó–∞—â–∏—â–µ–Ω—ã –æ—Ç –≤—ã–≥—Ä—É–∑–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è/–Ω–µ–¥–∞–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å): {protected_models}")
        
        if aggressive or memory_info["used_percent"] > 0.98:  # 98% - —ç–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è —Å–∏—Ç—É–∞—Ü–∏—è
            # –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞: –≤—ã–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ù–ï–ò–°–ü–û–õ–¨–ó–£–ï–ú–´–ï –º–æ–¥–µ–ª–∏
            logger.error(f"üö® –≠–ö–°–¢–†–ï–ù–ù–ê–Ø –û–ß–ò–°–¢–ö–ê: –≤—ã–≥—Ä—É–∂–∞–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–º—è—Ç–∏")
            keys_to_remove = [k for k in _models_cache.keys() if k not in protected_models]
            
            if not keys_to_remove and protected_models:
                logger.warning(f"‚ö†Ô∏è –í—Å–µ –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è! –ù–µ–ª—å–∑—è –≤—ã–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å. –ê–∫—Ç–∏–≤–Ω—ã–µ: {protected_models}")
                return  # –ù–µ –º–æ–∂–µ–º –Ω–∏—á–µ–≥–æ –≤—ã–≥—Ä—É–∑–∏—Ç—å
            
            for key in keys_to_remove:
                # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
                try:
                    with _request_lock:
                        active_count = _active_model_requests.get(key, 0)
                        if active_count > 0:
                            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—ã–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ {key} - –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({active_count})")
                            continue
                    
                    # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –≤—Å–µ –µ—â–µ –≤ –∫—ç—à–µ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
                    if key in _models_cache:
                        del _models_cache[key]
                        logger.info(f"üóëÔ∏è –ú–æ–¥–µ–ª—å {key} –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏ (—ç–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞)")
                except (KeyError, RuntimeError) as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {key}: {e}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
        elif len(_models_cache) > keep_count:
            # LRU –æ—á–∏—Å—Ç–∫–∞: –æ—Å—Ç–∞–≤–ª—è–µ–º —Å–∞–º—ã–µ —á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ + –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ use_count (–ø–æ —É–±—ã–≤–∞–Ω–∏—é), –∑–∞—Ç–µ–º –ø–æ last_used (–ø–æ —É–±—ã–≤–∞–Ω–∏—é)
            # –ö–†–ò–¢–ò–ß–ù–û: –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é items() –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏
            try:
                cache_items = list(_models_cache.items())
            except (RuntimeError, AttributeError) as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ items –∫—ç—à–∞: {e}")
                cache_items = []
            
            sorted_models = sorted(
                cache_items,
                key=lambda x: (x[1].get("use_count", 0) if x[1] else 0, x[1].get("last_used", datetime.min) if x[1] else datetime.min),
                reverse=True
            )
            
            # –û—Å—Ç–∞–≤–ª—è–µ–º keep_count —Å–∞–º—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö + –≤—Å–µ –∑–∞—â–∏—â–µ–Ω–Ω—ã–µ
            models_to_keep_keys = set()
            for k, v in sorted_models:
                if k in protected_models:
                    models_to_keep_keys.add(k)
                elif len(models_to_keep_keys) < keep_count:
                    models_to_keep_keys.add(k)
            
            keys_to_remove = [k for k in _models_cache.keys() if k not in models_to_keep_keys]
            
            if not keys_to_remove:
                logger.info(f"‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞—â–∏—â–µ–Ω—ã –∏–ª–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã, –æ—á–∏—Å—Ç–∫–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
                return
            
            for key in keys_to_remove:
                # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏
                try:
                    with _request_lock:
                        active_count = _active_model_requests.get(key, 0)
                        if active_count > 0:
                            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≤—ã–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ {key} - –µ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ({active_count})")
                            continue
                    
                    # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –≤—Å–µ –µ—â–µ –≤ –∫—ç—à–µ –ø–µ—Ä–µ–¥ —É–¥–∞–ª–µ–Ω–∏–µ–º
                    if key in _models_cache:
                        model_data = _models_cache[key]
                        use_count = model_data.get('use_count', 0)
                        use_info = f" (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ {use_count} —Ä–∞–∑)" if use_count > 0 else ""
                        del _models_cache[key]
                        logger.info(f"üóëÔ∏è –ú–æ–¥–µ–ª—å {key} –≤—ã–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ –ø–∞–º—è—Ç–∏{use_info}")
                except (KeyError, RuntimeError) as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {key}: {e}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
            
            logger.info(f"‚úÖ –û—Å—Ç–∞–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏: {list(models_to_keep_keys)} (–∑–∞—â–∏—â–µ–Ω—ã: {list(protected_models)})")
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å–±–æ—Ä–∫–∞ –º—É—Å–æ—Ä–∞
        gc.collect()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        memory_info_after = check_memory()
        freed_percent = (initial_used - memory_info_after["used_percent"]) * 100
        logger.info(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞: –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–æ {freed_percent:.1f}%, —Ç–µ–ø–µ—Ä—å {memory_info_after['used_percent']*100:.1f}%")


def get_model(model_key: str):
    """–ü–æ–ª—É—á–∞–µ—Ç –∏–ª–∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç OOM –∏ –∑–∞—â–∏—Ç–æ–π –æ—Ç –≤—ã–≥—Ä—É–∑–∫–∏"""
    model_lock = _model_locks[model_key]
    
    # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –ü–ï–†–ï–î –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π (–±—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
    if model_key in _models_cache:
        with model_lock:
            # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ —Å—á–µ—Ç—á–∏–∫
            _models_cache[model_key]["last_used"] = datetime.now()
            _models_cache[model_key]["use_count"] = _models_cache[model_key].get("use_count", 0) + 1
            logger.debug(f"üì¶ –ú–æ–¥–µ–ª—å {model_key} —É–∂–µ –≤ –∫—ç—à–µ (–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ {_models_cache[model_key]['use_count']} —Ä–∞–∑)")
            return _models_cache[model_key]
    
    with model_lock:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –µ—â–µ —Ä–∞–∑ –ø–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ (–Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –ø–æ–∫–∞ –∂–¥–∞–ª–∏)
        if model_key in _models_cache:
            _models_cache[model_key]["last_used"] = datetime.now()
            _models_cache[model_key]["use_count"] = _models_cache[model_key].get("use_count", 0) + 1
            logger.debug(f"üì¶ –ú–æ–¥–µ–ª—å {model_key} –ø–æ—è–≤–∏–ª–∞—Å—å –≤ –∫—ç—à–µ –ø–æ–∫–∞ –∂–¥–∞–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É")
            return _models_cache[model_key]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –¥—Ä—É–≥–∏–º –∑–∞–ø—Ä–æ—Å–æ–º
        if model_key in _loading_models:
            # –ú–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¥—Ä—É–≥–∏–º –∑–∞–ø—Ä–æ—Å–æ–º - –∂–¥–µ–º
            logger.warning(f"‚è≥ –ú–æ–¥–µ–ª—å {model_key} —É–∂–µ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –¥—Ä—É–≥–∏–º –∑–∞–ø—Ä–æ—Å–æ–º, –æ–∂–∏–¥–∞–Ω–∏–µ...")
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –∏ –∂–¥–µ–º
            max_wait = 60  # –ú–∞–∫—Å–∏–º—É–º 60 —Å–µ–∫—É–Ω–¥ –æ–∂–∏–¥–∞–Ω–∏—è
            waited = 0
            while model_key in _loading_models and waited < max_wait:
                model_lock.release()
                time.sleep(0.5)  # time —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω –≤ –Ω–∞—á–∞–ª–µ —Ñ–∞–π–ª–∞
                waited += 0.5
                model_lock.acquire()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—è–≤–∏–ª–∞—Å—å –ª–∏ –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ
                if model_key in _models_cache:
                    _models_cache[model_key]["last_used"] = datetime.now()
                    _models_cache[model_key]["use_count"] = _models_cache[model_key].get("use_count", 0) + 1
                    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_key} –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –¥—Ä—É–≥–∏–º –∑–∞–ø—Ä–æ—Å–æ–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–∑ –∫—ç—à–∞")
                    return _models_cache[model_key]
            
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—Å–µ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ—â–µ —Ä–∞–∑
            if model_key in _models_cache:
                _models_cache[model_key]["last_used"] = datetime.now()
                _models_cache[model_key]["use_count"] = _models_cache[model_key].get("use_count", 0) + 1
                return _models_cache[model_key]
            
            if model_key in _loading_models:
                raise RuntimeError(f"–ú–æ–¥–µ–ª—å {model_key} –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å –∑–∞ {max_wait} —Å–µ–∫—É–Ω–¥")
        
        # –ö–†–ò–¢–ò–ß–ù–û: Metal (Apple GPU) –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ—Ç –ª–∏ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –î–†–£–ì–ò–• –º–æ–¥–µ–ª–µ–π (–Ω–µ —Ç–æ–π, –∫–æ—Ç–æ—Ä—É—é –∑–∞–≥—Ä—É–∂–∞–µ–º)
        # –ï—Å–ª–∏ –∞–∫—Ç–∏–≤–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç—É –∂–µ –º–æ–¥–µ–ª—å - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ (–º–æ–¥–µ–ª—å —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞)
        with _request_lock:
            # –ê–∫—Ç–∏–≤–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –î–†–£–ì–ò–• –º–æ–¥–µ–ª–µ–π (–Ω–µ —Ç–æ–π, –∫–æ—Ç–æ—Ä—É—é –∑–∞–≥—Ä—É–∂–∞–µ–º)
            other_models_active = {
                k: v for k, v in _active_model_requests.items() 
                if k != model_key and v > 0
            }
            active_other_count = sum(other_models_active.values())
            
            if active_other_count > 0:
                # –ï—Å—Ç—å –∞–∫—Ç–∏–≤–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –î–†–£–ì–ò–• –º–æ–¥–µ–ª–µ–π - –æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
                other_models = list(other_models_active.keys())
                logger.warning(
                    f"‚è≥ –ê–∫—Ç–∏–≤–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π ({active_other_count}: {other_models}), "
                    f"–æ—Ç–∫–ª–∞–¥—ã–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏ {model_key} –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞ Metal"
                )
                # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –∏ –∂–¥–µ–º
                max_wait_metal = 120  # –ú–∞–∫—Å–∏–º—É–º 2 –º–∏–Ω—É—Ç—ã –æ–∂–∏–¥–∞–Ω–∏—è
                waited_metal = 0
                while active_other_count > 0 and waited_metal < max_wait_metal:
                    model_lock.release()
                    time.sleep(1.0)
                    waited_metal += 1.0
                    model_lock.acquire()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –µ—â–µ —Ä–∞–∑
                    if model_key in _models_cache:
                        _models_cache[model_key]["last_used"] = datetime.now()
                        _models_cache[model_key]["use_count"] = _models_cache[model_key].get("use_count", 0) + 1
                        logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_key} –ø–æ—è–≤–∏–ª–∞—Å—å –≤ –∫—ç—à–µ –ø–æ–∫–∞ –∂–¥–∞–ª–∏")
                        return _models_cache[model_key]
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –∞–∫—Ç–∏–≤–Ω—ã—Ö –≥–µ–Ω–µ—Ä–∞—Ü–∏–π –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
                    other_models_active = {
                        k: v for k, v in _active_model_requests.items() 
                        if k != model_key and v > 0
                    }
                    active_other_count = sum(other_models_active.values())
                
                if active_other_count > 0:
                    other_models = list(other_models_active.keys())
                    raise RuntimeError(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å {model_key}: –∞–∫—Ç–∏–≤–Ω—ã–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π "
                        f"({other_models}) –Ω–µ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å –∑–∞ {max_wait_metal}—Å"
                    )
        
        # –ü–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏ ‚Äî –≤—ã–≥—Ä—É–∂–∞–µ–º –ø–æ LRU, —á—Ç–æ–±—ã –≤ –∫—ç—à–µ –±—ã–ª–æ –Ω–µ –±–æ–ª—å—à–µ MLX_MAX_CACHED_MODELS
        evict_lru_to_limit(_max_cached_models - 1)
        
        # –û—Ç–º–µ—á–∞–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è (–∑–∞—â–∏—Ç–∞ –æ—Ç –≤—ã–≥—Ä—É–∑–∫–∏ –∏ –æ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏)
        _loading_models.add(model_key)
        
        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            memory_info = check_memory()
            if memory_info["used_percent"] > _memory_warning_threshold:
                logger.warning(f"‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory_info['used_percent']*100:.1f}%")
                cleanup_unused_models()
            
            model_path = MODEL_PATHS.get(model_key)
            if not model_path:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ MLX_MODELS_DIR
                model_path = os.path.join(MLX_MODELS_DIR, model_key)
            
            if not model_path or not os.path.exists(model_path):
                logger.error(f"‚ùå –ú–æ–¥–µ–ª—å {model_key} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏ {model_path}")
                raise ValueError(f"Model {model_key} not found at {model_path}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏
            memory_info = check_memory()
            if memory_info["used_percent"] > _memory_critical_threshold:
                cleanup_unused_models()
                memory_info = check_memory()
                if memory_info["used_percent"] > _memory_critical_threshold:
                    raise RuntimeError(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–∞–º—è—Ç–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {memory_info['used_percent']*100:.1f}% –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è")
            
            logger.info(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_key} –∏–∑ {model_path} (–ø–∞–º—è—Ç—å: {memory_info['used_percent']*100:.1f}%)")
            load_start = time.time()
            model, tokenizer = load(model_path)
            load_duration = time.time() - load_start
            
            _models_cache[model_key] = {
                "model": model,
                "tokenizer": tokenizer,
                "loaded_at": datetime.now(),
                "last_used": datetime.now(),
                "use_count": 1,
                "load_time_seconds": load_duration
            }
            
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_key} (–∑–∞–≥—Ä—É–∑–∫–∞ –∑–∞–Ω—è–ª–∞ {load_duration:.2f}—Å)")
            return _models_cache[model_key]
        except MemoryError as e:
            logger.error(f"‚ùå –ù–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏ {model_key}: {e}")
            # –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ MemoryError
            cleanup_unused_models(aggressive=True)
            memory_info = check_memory()
            raise HTTPException(
                status_code=503, 
                detail=f"Insufficient memory to load model: {str(e)}. Memory usage: {memory_info['used_percent']*100:.1f}%"
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ {model_key}: {e}", exc_info=True)
            raise
        finally:
            # –£–±–∏—Ä–∞–µ–º –∏–∑ —Å–ø–∏—Å–∫–∞ –∑–∞–≥—Ä—É–∂–∞—é—â–∏—Ö—Å—è
            _loading_models.discard(model_key)


# Middleware –¥–ª—è rate limiting –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
@app.middleware("http")
async def rate_limit_middleware(request: Request, call_next):
    """Rate limiting –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    global _active_requests, _request_times
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ rate limit
    now = time.time()
    client_ip = request.client.host if request.client else "unknown"
    
    with _request_lock:
        # –û—á–∏—Å—Ç–∫–∞ —Å—Ç–∞—Ä—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        _request_times[client_ip] = [
            t for t in _request_times[client_ip] 
            if now - t < _rate_limit_window
        ]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–∏–º–∏—Ç–∞
        if len(_request_times[client_ip]) >= _rate_limit_max:
            logger.warning(f"‚ö†Ô∏è Rate limit –ø—Ä–µ–≤—ã—à–µ–Ω –¥–ª—è {client_ip}")
            return JSONResponse(
                status_code=429,
                content={"error": f"Rate limit exceeded. Max {_rate_limit_max} requests per {_rate_limit_window} seconds"}
            )
        
        _request_times[client_ip].append(now)
    
    # Health/read-only –Ω–µ –∑–∞–Ω–∏–º–∞—é—Ç —Å–ª–æ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ ‚Äî –∏–Ω–∞—á–µ –ø—Ä–∏ –æ–¥–Ω–æ–π –¥–æ–ª–≥–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ 70b –≤—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∂–¥—É—Ç –∏ –ø–∞–¥–∞—é—Ç –ø–æ —Ç–∞–π–º–∞—É—Ç—É
    path = request.url.path or ""
    skip_semaphore = path in ("/", "/health", "/api/tags") or path.rstrip("/") in ("", "/health", "/api/tags")
    
    if skip_semaphore:
        return await call_next(request)
    
    queue_wait = _get_queue_wait_timeout()
    try:
        await asyncio.wait_for(_concurrent_semaphore.acquire(), timeout=queue_wait)
    except asyncio.TimeoutError:
        logger.warning(f"‚ö†Ô∏è –û–∂–∏–¥–∞–Ω–∏–µ —Å–ª–æ—Ç–∞ –ø—Ä–µ–≤—ã—Å–∏–ª–æ {queue_wait}—Å, –æ—Ç–∫–ª–æ–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å")
        return JSONResponse(
            status_code=503,
            content={"error": f"Server overloaded. Waited {int(queue_wait)}s for slot. Max {_max_concurrent_requests} concurrent requests"}
        )
    with _request_lock:
        _active_requests += 1
    try:
        response = await call_next(request)
        return response
    finally:
        with _request_lock:
            _active_requests -= 1
        _concurrent_semaphore.release()


@app.get("/")
async def root():
    """Health check —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π"""
    memory_info = check_memory()
    return {
        "status": "online",
        "server": "MLX Model Server",
        "version": "2.0.0",
        "device": "Mac Studio M4 Max",
        "models_loaded": len(_models_cache),
        "cached_models": [
            {
                "name": k,
                "use_count": v.get("use_count", 0),
                "last_used": v.get("last_used", v.get("loaded_at")).isoformat() if v.get("last_used") or v.get("loaded_at") else None,
                "load_time_seconds": v.get("load_time_seconds", 0)
            }
            for k, v in _models_cache.items()
        ],
        "available_models": list(MODEL_PATHS.keys()),
        "active_requests": _active_requests,
        "max_concurrent": _max_concurrent_requests,
        "memory": {
            "used_percent": round(memory_info["used_percent"] * 100, 1),
            "available_gb": round(memory_info["available_gb"], 2)
        }
    }


@app.get("/api/tags")
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Ollama API)"""
    try:
        return {
            "models": [
                {
                    "name": name,
                    "model": name,
                    "size": 0,
                    "format": "mlx",
                    "exists": os.path.exists(MODEL_PATHS.get(name, ""))
                }
                for name in MODEL_PATHS.keys()
            ]
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to list models: {str(e)}")


@app.post("/api/generate")
async def generate_text(
    request: GenerateRequest,
    http_request: Request
):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å Ollama API) —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –æ—à–∏–±–æ–∫
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—á–µ—Ä–µ–¥—å —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏:
    - high: –ß–∞—Ç —Å –í–∏–∫—Ç–æ—Ä–∏–µ–π (–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –ø–µ—Ä–≤—ã–º) - –∑–∞–≥–æ–ª–æ–≤–æ–∫ X-Request-Priority: high
    - medium: Task Distribution (–º–æ–∂–µ—Ç –ø–æ–¥–æ–∂–¥–∞—Ç—å) - –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    - low: –§–æ–Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏ - –∑–∞–≥–æ–ª–æ–≤–æ–∫ X-Request-Priority: low
    """
    start_time = time.time()
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ X-Request-Priority
    priority_header = http_request.headers.get("X-Request-Priority", "medium").lower()
    priority_map = {
        "high": RequestPriority.HIGH,
        "medium": RequestPriority.MEDIUM,
        "low": RequestPriority.LOW
    }
    request_priority = priority_map.get(priority_header, RequestPriority.MEDIUM)
    
    logger.debug(f"üì• –ó–∞–ø—Ä–æ—Å –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {request_priority.name}, –º–æ–¥–µ–ª—å: {request.model})")
    
    # –û—Ü–µ–Ω–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞ –ø–æ –º–æ–¥–µ–ª–∏ (–∑–∞–≥—Ä—É–∑–∫–∞ + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å + –∑–∞–ø–∞—Å) –¥–ª—è –æ—á–µ—Ä–µ–¥–∏ –∏ –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    if request.model:
        _queue_model_key = OLLAMA_TO_MLX_MAP.get(request.model, request.model)
        if _queue_model_key not in MODEL_PATHS:
            _queue_model_key = CATEGORY_TO_MODEL.get(request.model, "default")
    elif request.category:
        _queue_model_key = CATEGORY_TO_MODEL.get(request.category, "default")
    else:
        _queue_model_key = "default"
    timeout_estimate = get_model_timeout_estimate(_queue_model_key, request.max_tokens, load_time_actual=None)
    
    # –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
    if REQUEST_QUEUE_AVAILABLE:
        queue = get_request_queue()
        
        # –°–æ–∑–¥–∞–µ–º Future –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        result_future = asyncio.Future()
        
        async def _execute_generation():
            try:
                result = await _generate_text_internal(request, start_time)
                if not result_future.done():
                    result_future.set_result(result)
                return result
            except Exception as e:
                if not result_future.done():
                    result_future.set_exception(e)
                raise
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å (—Ç–∞–π–º–∞—É—Ç –ø–æ –º–æ–¥–µ–ª–∏: –∑–∞–≥—Ä—É–∑–∫–∞ + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å + –∑–∞–ø–∞—Å)
        success, request_id, queue_position = await queue.add_request(
            priority=request_priority,
            callback=_execute_generation,
            timeout=timeout_estimate,
            metadata={"model": request.model, "category": request.category}
        )
        
        if not success:
            raise HTTPException(
                status_code=503,
                detail="Queue is full. Please try again later."
            )
        
        # –ï—Å–ª–∏ –∑–∞–ø—Ä–æ—Å –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å—Ä–∞–∑—É (queue_position = 0), –∂–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        # –ï—Å–ª–∏ –≤ –æ—á–µ—Ä–µ–¥–∏ (queue_position > 0), —Ç–∞–∫–∂–µ –∂–¥–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        try:
            result = await asyncio.wait_for(result_future, timeout=timeout_estimate)
            return result
        except asyncio.TimeoutError:
            logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –æ–∂–∏–¥–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ {request_id} (–ª–∏–º–∏—Ç {timeout_estimate:.0f}—Å)")
            raise HTTPException(
                status_code=504,
                detail=f"Request timeout while waiting in queue (limit {timeout_estimate:.0f}s for this model)"
            )
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ {request_id}: {e}")
            raise
    
    # Fallback: –ø—Ä—è–º–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Å—Ç–∞—Ä—ã–π —Å–ø–æ—Å–æ–±)
    return await _generate_text_internal(request, start_time)


@app.post("/api/chat")
async def chat(
    request: ChatRequest,
    http_request: Request
):
    """
    Ollama Chat API (/api/chat) - —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å LocalAIRouter
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç messages –≤ prompt –∏ –≤—ã–∑—ã–≤–∞–µ—Ç /api/generate
    """
    # –§–æ—Ä–º–∏—Ä—É–µ–º prompt –∏–∑ messages
    system_parts = []
    user_parts = []
    for msg in request.messages:
        if msg.role == "system":
            system_parts.append(msg.content)
        elif msg.role == "user":
            user_parts.append(msg.content)
        elif msg.role == "assistant":
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º assistant messages –ø—Ä–∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–∏ –ø—Ä–æ–º–ø—Ç–∞
            pass
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º system –∏ user –≤ –æ–¥–∏–Ω prompt
    prompt_parts = []
    if system_parts:
        prompt_parts.append("\n".join(system_parts))
    if user_parts:
        prompt_parts.append("\n".join(user_parts))
    
    prompt = "\n\n".join(prompt_parts)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ options
    temperature = 0.7
    max_tokens = 512
    if request.options:
        temperature = request.options.get("temperature", 0.7)
        max_tokens = request.options.get("num_predict", 512)
    
    # –°–æ–∑–¥–∞—ë–º GenerateRequest
    gen_request = GenerateRequest(
        prompt=prompt,
        model=request.model,
        max_tokens=max_tokens,
        temperature=temperature,
        stream=request.stream
    )
    
    # –í—ã–∑—ã–≤–∞–µ–º /api/generate
    response = await generate_text(gen_request, http_request)
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç /api/chat
    if isinstance(response, dict) and "response" in response:
        return {
            "model": request.model,
            "created_at": response.get("created_at", datetime.now().isoformat()),
            "message": {
                "role": "assistant",
                "content": response["response"]
            },
            "done": True
        }
    elif isinstance(response, StreamingResponse):
        # –î–ª—è stream=true –≤–æ–∑–≤—Ä–∞—â–∞–µ–º StreamingResponse –∫–∞–∫ –µ—Å—Ç—å
        return response
    else:
        # Fallback
        return response


async def _generate_text_internal(request: GenerateRequest, start_time: float):
    """–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞"""
    try:
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        memory_info = check_memory()
        if memory_info["used_percent"] > _memory_critical_threshold:
            cleanup_unused_models()
            memory_info = check_memory()
            if memory_info["used_percent"] > _memory_critical_threshold:
                raise HTTPException(
                    status_code=503,
                    detail=f"Insufficient memory: {memory_info['used_percent']*100:.1f}% used"
                )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å
        if request.model:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –º–∞–ø–ø–∏–Ω–≥ –∏–∑ Ollama –∏–º–µ–Ω–∏ –≤ MLX
            model_key = OLLAMA_TO_MLX_MAP.get(request.model, request.model)
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ –º–∞–ø–ø–∏–Ω–≥–µ, –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∫ –µ—Å—Ç—å
            if model_key not in MODEL_PATHS:
                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å default
                model_key = CATEGORY_TO_MODEL.get(request.model, "default")
        elif request.category:
            model_key = CATEGORY_TO_MODEL.get(request.category, "default")
        else:
            model_key = "default"
        
        # –û—Ç–º–µ—á–∞–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è (–∑–∞—â–∏—Ç–∞ –æ—Ç –≤—ã–≥—Ä—É–∑–∫–∏)
        with _request_lock:
            _active_model_requests[model_key] += 1
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª—å (—Å –∑–∞—â–∏—Ç–æ–π –æ—Ç OOM)
            try:
                model_data = get_model(model_key)
                model = model_data["model"]
                tokenizer = model_data["tokenizer"]
            except (MemoryError, RuntimeError) as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_key}: {e}")
                raise HTTPException(status_code=503, detail=f"Model loading failed: {str(e)}")
            
            # –¢–∞–π–º–∞—É—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: –∑–∞–≥—Ä—É–∑–∫–∞ (—Ñ–∞–∫—Ç –∏–ª–∏ –æ—Ü–µ–Ω–∫–∞) + –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –ø–æ max_tokens + –∑–∞–ø–∞—Å
            gen_timeout = get_model_timeout_estimate(
                model_key,
                request.max_tokens,
                load_time_actual=model_data.get("load_time_seconds"),
            )
            
            # –ö–†–ò–¢–ò–ß–ù–û: Metal –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –æ–¥–Ω–∏–º command buffer
            # –°–µ—Ä–∏–∞–ª–∏–∑—É–µ–º –≤—Å–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
            # –†–∞–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ, –Ω–æ –æ–¥–Ω–∞ –º–æ–¥–µ–ª—å - –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
            model_lock = _model_locks[model_key]
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å —Ç–∞–π–º–∞—É—Ç–æ–º
            if request.stream:
                return StreamingResponse(
                    generate_stream(model, tokenizer, request.prompt, request.max_tokens, model_lock),
                    media_type="application/json"
                )
            else:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º executor –¥–ª—è async —Å —Ç–∞–π–º–∞—É—Ç–æ–º
                # –ö–†–ò–¢–ò–ß–ù–û: –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –º–æ–¥–µ–ª–∏ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
                # –¥–ª—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (–∑–∞—â–∏—Ç–∞ –æ—Ç Metal –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤)
                loop = asyncio.get_event_loop()
                try:
                    def generate_with_lock():
                        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç Metal –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤"""
                        with model_lock:
                            return generate(model, tokenizer, prompt=request.prompt, max_tokens=request.max_tokens)
                    
                    response_text = await asyncio.wait_for(
                        loop.run_in_executor(None, generate_with_lock),
                        timeout=gen_timeout
                    )
                    
                    duration = time.time() - start_time
                    logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {duration:.2f}—Å (–º–æ–¥–µ–ª—å: {model_key}, —Ç–æ–∫–µ–Ω–æ–≤: {request.max_tokens})")
                    
                    return {
                        "model": model_key,
                        "response": response_text,
                        "done": True
                    }
                except asyncio.TimeoutError:
                    logger.error(f"‚ùå –¢–∞–π–º–∞—É—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–∏ {model_key} (–ª–∏–º–∏—Ç {gen_timeout:.0f}—Å)")
                    raise HTTPException(
                        status_code=504,
                        detail=f"Generation timeout (limit {gen_timeout:.0f}s for this model and max_tokens)"
                    )
                except MemoryError as e:
                    logger.error(f"‚ùå –ù–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                    # –≠–∫—Å—Ç—Ä–µ–Ω–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ MemoryError
                    cleanup_unused_models(aggressive=True)
                    memory_info = check_memory()
                    raise HTTPException(
                        status_code=503, 
                        detail=f"Out of memory during generation: {str(e)}. Memory usage: {memory_info['used_percent']*100:.1f}%"
                    )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—å—é {model_key}: {e}", exc_info=True)
            raise
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")
    finally:
        # –£–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        try:
            with _request_lock:
                current_count = _active_model_requests.get(model_key, 0)
                if current_count > 0:
                    _active_model_requests[model_key] = current_count - 1
                    if _active_model_requests[model_key] == 0:
                        _active_model_requests.pop(model_key, None)
                else:
                    # –ï—Å–ª–∏ —Å—á–µ—Ç—á–∏–∫ —É–∂–µ 0 –∏–ª–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ—Å—Ç–æ —É–¥–∞–ª—è–µ–º –∫–ª—é—á
                    _active_model_requests.pop(model_key, None)
        except (KeyError, RuntimeError) as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—á–µ—Ç—á–∏–∫–∞ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è {model_key}: {e}")


async def generate_stream(model, tokenizer, prompt: str, max_tokens: int, model_lock: threading.Lock = None):
    """Streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç Metal –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤"""
    # MLX –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç streaming –Ω–∞–ø—Ä—è–º—É—é, —ç–º—É–ª–∏—Ä—É–µ–º
    # –ö–†–ò–¢–ò–ß–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–ª–æ–∫–∏—Ä–æ–≤–∫—É –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞—â–∏—Ç—ã –æ—Ç Metal –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
    if model_lock is None:
        # –ï—Å–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞, —Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é (–Ω–µ –¥–æ–ª–∂–Ω–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç—å)
        logger.warning("‚ö†Ô∏è generate_stream –≤—ã–∑–≤–∞–Ω –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –º–æ–¥–µ–ª–∏, —Å–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—É—é")
        model_lock = threading.Lock()
    
    loop = asyncio.get_event_loop()
    
    def generate_with_lock():
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π –º–æ–¥–µ–ª–∏"""
        with model_lock:
            return generate(model, tokenizer, prompt=prompt, max_tokens=max_tokens)
    
    response = await loop.run_in_executor(None, generate_with_lock)
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–æ–∫–µ–Ω—ã –¥–ª—è —ç–º—É–ª—è—Ü–∏–∏ streaming
    for char in response:
        yield json.dumps({"response": char, "done": False}) + "\n"
    
    yield json.dumps({"response": "", "done": True}) + "\n"


@app.get("/api/models/{model_name}")
async def get_model_info(model_name: str):
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏"""
    if model_name not in MODEL_PATHS:
        raise HTTPException(status_code=404, detail="Model not found")
    
    model_path = MODEL_PATHS[model_name]
    exists = os.path.exists(model_path)
    
    return {
        "name": model_name,
        "path": model_path,
        "exists": exists,
        "loaded": model_name in _models_cache
    }


@app.get("/queue/stats")
async def queue_stats():
    """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–µ—Ä–µ–¥–∏ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    if REQUEST_QUEUE_AVAILABLE:
        queue = get_request_queue()
        stats = queue.get_stats()
        return stats
    else:
        return {
            "status": "queue_not_available",
            "message": "Request queue is not available, using direct processing"
        }


@app.post("/v1/messages")
async def anthropic_messages(request: AnthropicMessagesRequest):
    """
    Anthropic-compatible API endpoint –¥–ª—è Claude Code –∏ –¥—Ä—É–≥–∏—Ö Anthropic-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö –∫–ª–∏–µ–Ω—Ç–æ–≤
    
    –≠–º—É–ª–∏—Ä—É–µ—Ç Anthropic Messages API, –ø–æ–∑–≤–æ–ª—è—è Claude Code —Ä–∞–±–æ—Ç–∞—Ç—å —Å MLX –º–æ–¥–µ–ª—è–º–∏
    """
    try:
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º Anthropic —Ñ–æ—Ä–º–∞—Ç –≤ Ollama —Ñ–æ—Ä–º–∞—Ç
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –æ–¥–∏–Ω –ø—Ä–æ–º–ø—Ç
        prompt_parts = []
        for msg in request.messages:
            if msg.role == "system":
                prompt_parts.append(f"System: {msg.content}")
            elif msg.role == "user":
                prompt_parts.append(f"User: {msg.content}")
            elif msg.role == "assistant":
                prompt_parts.append(f"Assistant: {msg.content}")
        
        combined_prompt = "\n".join(prompt_parts)
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å (–º–∞–ø–ø–∏–Ω–≥ –∏–∑ Anthropic –∏–º–µ–Ω –≤ MLX)
        model_key = request.model
        if model_key in OLLAMA_TO_MLX_MAP:
            model_key = OLLAMA_TO_MLX_MAP[model_key]
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å –≤ —Ñ–æ—Ä–º–∞—Ç–µ Ollama –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Ñ—É–Ω–∫—Ü–∏–∏
        internal_request = GenerateRequest(
            prompt=combined_prompt,
            model=model_key,
            max_tokens=request.max_tokens or 1024,
            temperature=request.temperature or 0.7,
            stream=request.stream or False
        )
        
        if request.stream:
            # Streaming response (—ç–º—É–ª–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ —Ä–∞–∑–±–∏–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞)
            async def generate_stream_response():
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω—é—é —Ñ—É–Ω–∫—Ü–∏—é –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                    result = await _generate_text_internal(internal_request, time.time())
                    response_text = result.get("response", "")
                    
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ Anthropic streaming format
                    for char in response_text:
                        yield f"data: {json.dumps({'type': 'content_block_delta', 'delta': {'type': 'text_delta', 'text': char}})}\n\n"
                    
                    yield f"data: {json.dumps({'type': 'message_delta', 'delta': {'stop_reason': 'end_turn'}})}\n\n"
                    yield f"data: {json.dumps({'type': 'message_stop'})}\n\n"
                except Exception as e:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ streaming –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}", exc_info=True)
                    yield f"data: {json.dumps({'type': 'error', 'error': {'message': str(e)}})}\n\n"
            
            return StreamingResponse(generate_stream_response(), media_type="text/event-stream")
        else:
            # Non-streaming response
            result = await _generate_text_internal(internal_request, time.time())
            response_text = result.get("response", "")
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –≤ Anthropic format
            return {
                "id": f"msg_{int(time.time() * 1000)}",
                "type": "message",
                "role": "assistant",
                "content": [
                    {
                        "type": "text",
                        "text": response_text
                    }
                ],
                "model": request.model,
                "stop_reason": "end_turn",
                "stop_sequence": None,
                "usage": {
                    "input_tokens": len(combined_prompt.split()),  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                    "output_tokens": len(response_text.split())   # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
                }
            }
    
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ Anthropic API: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Anthropic API error: {str(e)}")


@app.get("/health")
async def health_check():
    """Health check endpoint —Å –¥–µ—Ç–∞–ª—å–Ω–æ–π –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–æ–π –ø–∞–º—è—Ç–∏"""
    try:
        memory_info = check_memory()
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—á–∏—Å—Ç–∫–∞ –ø—Ä–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –Ω–µ—Ö–≤–∞—Ç–∫–µ –ø–∞–º—è—Ç–∏
        if memory_info["used_percent"] > _memory_critical_threshold:
            logger.warning(f"‚ö†Ô∏è Health check: –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏ ({memory_info['used_percent']*100:.1f}%), –∑–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É...")
            cleanup_unused_models(aggressive=memory_info["used_percent"] > 0.98)
            memory_info = check_memory()  # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
        
        is_healthy = (
            memory_info["used_percent"] < _memory_critical_threshold and
            _active_requests < _max_concurrent_requests
        )
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
        status = "healthy"
        if memory_info["used_percent"] > _memory_critical_threshold:
            status = "critical"
        elif memory_info["used_percent"] > _memory_warning_threshold:
            status = "warning"
        elif not is_healthy:
            status = "degraded"
        
        warnings = []
        if memory_info["used_percent"] > _memory_warning_threshold:
            warnings.append(f"High memory usage: {memory_info['used_percent']*100:.1f}%")
        if _active_requests >= _max_concurrent_requests:
            warnings.append(f"Too many concurrent requests: {_active_requests}/{_max_concurrent_requests}")
        
        return {
            "status": status,
            "service": "MLX API Server",
            "version": "2.0.0",
            "models_cached": len(_models_cache),
            "cached_models": [
                {
                    "name": k,
                    "use_count": v.get("use_count", 0),
                    "last_used": v.get("last_used", v.get("loaded_at")).isoformat() if v.get("last_used") or v.get("loaded_at") else None,
                    "load_time_seconds": round(v.get("load_time_seconds", 0), 2),
                    "active_requests": _active_model_requests.get(k, 0),
                    "is_loading": k in _loading_models
                }
                for k, v in _models_cache.items()
            ],
            "active_model_requests": dict(_active_model_requests),
            "loading_models": list(_loading_models),
            "total_models": len(MODEL_PATHS),
            "active_requests": _active_requests,
            "max_concurrent": _max_concurrent_requests,
            "memory": {
                "used_percent": round(memory_info["used_percent"] * 100, 1),
                "available_gb": round(memory_info["available_gb"], 2),
                "total_gb": round(memory_info["total_gb"], 2),
                "warning_threshold": round(_memory_warning_threshold * 100, 1),
                "critical_threshold": round(_memory_critical_threshold * 100, 1)
            },
            "rate_limit": {
                "max_per_window": _rate_limit_max,
                "window_seconds": _rate_limit_window
            },
            "warnings": warnings,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ health check: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")


async def periodic_cache_cleanup():
    """–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –≤—ã–≥—Ä—É–∂–∞–µ—Ç –ª–∏—à–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ LRU (–Ω–µ –±–æ–ª–µ–µ MLX_MAX_CACHED_MODELS –≤ –∫—ç—à–µ)."""
    if _cache_cleanup_interval_sec <= 0:
        return
    while True:
        try:
            await asyncio.sleep(_cache_cleanup_interval_sec)
            evict_lru_to_limit(_max_cached_models)
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—á–∏—Å—Ç–∫–µ –∫—ç—à–∞: {e}")


async def preload_models():
    """–ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –æ—Å–Ω–æ–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ —Å–µ—Ä–≤–µ—Ä–∞"""
    if not _preload_models:
        logger.info("‚ÑπÔ∏è –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –æ—Ç–∫–ª—é—á–µ–Ω–∞ (MLX_PRELOAD_MODELS –ø—É—Å—Ç–æ)")
        return
    
    logger.info(f"üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π: {_preload_models}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–æ–π
    memory_info = check_memory()
    if memory_info["used_percent"] > 0.7:  # 70% - —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
        logger.warning(f"‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ ({memory_info['used_percent']*100:.1f}%), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É")
        return
    
    preloaded = []
    failed = []
    
    for model_key in _preload_models:
        # –ú–∞–ø–ø–∏–Ω–≥ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
        actual_model = PRELOAD_MODEL_MAP.get(model_key, model_key)
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –≤ –∫—ç—à–µ
        if actual_model in _models_cache:
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {actual_model} —É–∂–µ –≤ –∫—ç—à–µ, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
            preloaded.append(actual_model)
            continue
        
        try:
            logger.info(f"üîÑ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {actual_model}...")
            start_time = time.time()
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            model_data = get_model(actual_model)
            
            duration = time.time() - start_time
            logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {actual_model} –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {duration:.2f}—Å")
            preloaded.append(actual_model)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            memory_info = check_memory()
            if memory_info["used_percent"] > 0.75:  # 75% - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É
                logger.warning(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –¥–æ—Å—Ç–∏–≥–ª–æ {memory_info['used_percent']*100:.1f}%, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫—É")
                break
                
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {actual_model}: {e}")
            failed.append(actual_model)
    
    if preloaded:
        logger.info(f"‚úÖ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(preloaded)} ({', '.join(preloaded)})")
    if failed:
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∏—Ç—å: {len(failed)} ({', '.join(failed)})")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
    memory_info = check_memory()
    logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∏: {memory_info['used_percent']*100:.1f}% ({memory_info['available_gb']:.1f}GB —Å–≤–æ–±–æ–¥–Ω–æ)")


if __name__ == "__main__":
    import uvicorn
    import signal
    import sys
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è graceful shutdown
    def signal_handler(sig, frame):
        logger.info("üõë –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è, –≤—ã–ø–æ–ª–Ω—è—é graceful shutdown...")
        # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –º–æ–¥–µ–ª–µ–π
        _models_cache.clear()
        logger.info("‚úÖ –ö—ç—à –º–æ–¥–µ–ª–µ–π –æ—á–∏—â–µ–Ω")
        sys.exit(0)
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ—Ä—Ç–∞: 11435 –≤–º–µ—Å—Ç–æ 11434 (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞!)
        PORT = int(os.getenv("MLX_API_PORT", 11435))
        WORKERS = int(os.getenv("MLX_API_WORKERS", 1))  # MLX –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç multiprocessing
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ MLX API Server –Ω–∞ –ø–æ—Ä—Ç—É {PORT} (workers: {WORKERS})")
        logger.info(f"üìä –õ–∏–º–∏—Ç—ã: {_max_concurrent_requests} –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤, {_rate_limit_max} –∑–∞–ø—Ä–æ—Å–æ–≤/{_rate_limit_window}—Å")
        logger.info(f"üì¶ –ü—Ä–µ–¥–∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π: {_preload_models if _preload_models else '–æ—Ç–∫–ª—é—á–µ–Ω–∞'}")
        
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=PORT,
            workers=WORKERS,
            timeout_keep_alive=120,  # Keep-alive –¥–ª—è connection pooling
            limit_concurrency=_max_concurrent_requests + 5,  # –ù–µ–±–æ–ª—å—à–æ–π –∑–∞–ø–∞—Å
            log_level="info"
        )
    except KeyboardInterrupt:
        logger.info("üõë –ü–æ–ª—É—á–µ–Ω KeyboardInterrupt, –≤—ã–ø–æ–ª–Ω—è—é graceful shutdown...")
        _models_cache.clear()
        sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞: {e}", exc_info=True)
        sys.exit(1)

