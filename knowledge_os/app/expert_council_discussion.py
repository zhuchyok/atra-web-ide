#!/usr/bin/env python3
"""
–°–æ–≤–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ - –æ–±—Å—É–∂–¥–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–∏—Ä–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ —Å 58 —Å–æ—Ç—Ä—É–¥–Ω–∏–∫–∞–º–∏.
–í—ã–¥–≤–∏–∂–µ–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ.
"""
import asyncio
import os
import json
import asyncpg
from datetime import datetime, timezone
from typing import List, Dict, Optional
import logging

logger = logging.getLogger(__name__)

DB_URL = os.getenv("DATABASE_URL", "postgresql://admin:secret@localhost:5432/knowledge_os")

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –º–æ–¥–µ–ª—è–º–∏
try:
    from ai_core import run_smart_agent_async
    AI_CORE_AVAILABLE = True
except ImportError:
    AI_CORE_AVAILABLE = False
    logger.warning("ai_core not available, using fallback")

try:
    from local_router import LocalAIRouter
    LOCAL_ROUTER_AVAILABLE = True
except ImportError:
    LOCAL_ROUTER_AVAILABLE = False


class ExpertCouncil:
    """–°–æ–≤–µ—Ç —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫"""
    
    def __init__(self, db_url: str = None):
        self.db_url = db_url or DB_URL
        self.router = LocalAIRouter() if LOCAL_ROUTER_AVAILABLE else None
    
    async def get_experts_by_department(self, department: str = None, limit: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –ø–æ –¥–µ–ø–∞—Ä—Ç–∞–º–µ–Ω—Ç—É"""
        pool = await asyncpg.create_pool(self.db_url, min_size=1, max_size=3)
        async with pool.acquire() as conn:
            query = """
                SELECT id, name, role, department, system_prompt
                FROM experts
                WHERE is_active = TRUE OR is_active IS NULL
            """
            params = []
            if department:
                query += " AND department = $1"
                params.append(department)
            query += " ORDER BY RANDOM() LIMIT $2"
            params.append(limit)
            
            rows = await conn.fetch(query, *params)
            return [dict(row) for row in rows]
    
    async def get_all_experts(self) -> List[Dict]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤"""
        pool = await asyncpg.create_pool(self.db_url, min_size=1, max_size=3)
        async with pool.acquire() as conn:
            rows = await conn.fetch("""
                SELECT id, name, role, department, system_prompt
                FROM experts
                WHERE is_active = TRUE OR is_active IS NULL
                ORDER BY name
            """)
            return [dict(row) for row in rows]
    
    async def get_relevant_experts(self, topic: str, count: int = 5) -> List[Dict]:
        """–í—ã–±—Ä–∞—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è —Ç–µ–º—ã"""
        all_experts = await self.get_all_experts()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        topic_lower = topic.lower()
        
        relevant = []
        for expert in all_experts:
            role = (expert.get('role') or '').lower()
            dept = (expert.get('department') or '').lower()
            prompt = (expert.get('system_prompt') or '').lower()
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            score = 0
            if 'ai' in topic_lower or 'agent' in topic_lower or 'neural' in topic_lower:
                if 'ml' in role or 'ai' in role or 'engineer' in role or 'developer' in role:
                    score += 2
            if 'security' in topic_lower:
                if 'security' in role or 'security' in dept:
                    score += 2
            if 'devops' in topic_lower or 'infrastructure' in topic_lower:
                if 'devops' in role or 'devops' in dept:
                    score += 2
            if 'learning' in topic_lower or 'self' in topic_lower:
                if 'ml' in role or 'ai' in role or 'researcher' in role:
                    score += 2
            if 'architecture' in topic_lower:
                if 'architect' in role or 'engineer' in role:
                    score += 1
            
            if score > 0:
                expert['relevance_score'] = score
                relevant.append(expert)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ score –∏ –±–µ—Ä–µ–º —Ç–æ–ø
        relevant.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        return relevant[:count]
    
    async def ask_expert(self, expert: Dict, question: str) -> Optional[str]:
        """–°–ø—Ä–æ—Å–∏—Ç—å —ç–∫—Å–ø–µ—Ä—Ç–∞"""
        name = expert.get('name', '–≠–∫—Å–ø–µ—Ä—Ç')
        role = expert.get('role', '')
        system_prompt = expert.get('system_prompt', '')
        
        full_prompt = f"""–í—ã {name}, {role}.

{system_prompt}

–í–û–ü–†–û–° –î–õ–Ø –û–ë–°–£–ñ–î–ï–ù–ò–Ø:
{question}

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–π—Ç–µ –≤–∞—à–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–µ –º–Ω–µ–Ω–∏–µ:
1. –ß—Ç–æ –≤—ã –¥—É–º–∞–µ—Ç–µ –æ–± —ç—Ç–æ–º?
2. –ö–∞–∫–∏–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ —Ä–∏—Å–∫–∏?
3. –ö–∞–∫ —ç—Ç–æ –º–æ–∂–Ω–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤ –Ω–∞—à–µ–π –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏?
4. –ß—Ç–æ –Ω–∞–º –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏?

–û—Ç–≤–µ—Ç—å—Ç–µ –∫—Ä–∞—Ç–∫–æ, –Ω–æ –ø–æ –¥–µ–ª—É (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)."""

        try:
            if LOCAL_ROUTER_AVAILABLE and self.router:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                response = await self.router.run_local_llm_async(
                    full_prompt,
                    category="reasoning",
                    model="phi3.5:3.8b"
                )
                if response:
                    return response
            # Fallback –∫ –æ–±–ª–∞—á–Ω–æ–π –º–æ–¥–µ–ª–∏
            if AI_CORE_AVAILABLE:
                response = await run_smart_agent_async(
                    full_prompt,
                    expert_name=name,
                    category="reasoning"
                )
                return response
        except Exception as e:
            logger.error(f"Error asking expert {name}: {e}")
        
        return None
    
    async def conduct_discussion(self, topic: str, question: str, expert_count: int = 5) -> Dict:
        """–ü—Ä–æ–≤–µ—Å—Ç–∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Å —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏"""
        logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–µ —Ç–µ–º—ã: {topic}")
        
        # –í—ã–±–∏—Ä–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        experts = await self.get_relevant_experts(topic, count=expert_count)
        if not experts:
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö, –±–µ—Ä–µ–º —Å–ª—É—á–∞–π–Ω—ã—Ö
            experts = await self.get_experts_by_department(limit=expert_count)
        
        logger.info(f"üë• –í—ã–±—Ä–∞–Ω–æ {len(experts)} —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è")
        
        # –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å –∫–∞–∂–¥–æ–º—É —ç–∫—Å–ø–µ—Ä—Ç—É
        discussions = []
        for expert in experts:
            name = expert.get('name', 'Unknown')
            logger.info(f"   üí¨ –°–ø—Ä–∞—à–∏–≤–∞–µ–º {name}...")
            
            response = await self.ask_expert(expert, question)
            if response:
                discussions.append({
                    'expert': name,
                    'role': expert.get('role', ''),
                    'department': expert.get('department', ''),
                    'response': response
                })
                logger.info(f"   ‚úÖ {name} –æ—Ç–≤–µ—Ç–∏–ª ({len(response)} —Å–∏–º–≤–æ–ª–æ–≤)")
            else:
                logger.warning(f"   ‚ö†Ô∏è {name} –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–≤–æ–¥–∫—É
        summary = {
            'topic': topic,
            'question': question,
            'experts_count': len(experts),
            'responses_count': len(discussions),
            'discussions': discussions,
            'timestamp': datetime.now(timezone.utc).isoformat()
        }
        
        return summary
    
    async def generate_hypotheses(self, discussion_summary: Dict) -> List[Dict]:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è"""
        logger.info("üí° –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥–∏–ø–æ—Ç–µ–∑—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑
        discussions_text = "\n\n".join([
            f"**{d['expert']}** ({d['role']}):\n{d['response']}"
            for d in discussion_summary['discussions']
        ])
        
        hypothesis_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è.

–¢–ï–ú–ê: {discussion_summary['topic']}

–û–ë–°–£–ñ–î–ï–ù–ò–ï –≠–ö–°–ü–ï–†–¢–û–í:
{discussions_text}

–ó–ê–î–ê–ß–ê:
–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ 3-5 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è. –ö–∞–∂–¥–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –¥–æ–ª–∂–Ω–∞:
1. –ë—ã—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∏ –∏–∑–º–µ—Ä–∏–º–æ–π
2. –û–ø–∏—Å—ã–≤–∞—Ç—å —á—Ç–æ –≤–Ω–µ–¥—Ä–∏—Ç—å
3. –£–∫–∞–∑—ã–≤–∞—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç
4. –ë—ã—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ (high/medium/low)

–í–µ—Ä–Ω–∏—Ç–µ JSON –º–∞—Å—Å–∏–≤:
[
  {{
    "title": "–ù–∞–∑–≤–∞–Ω–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã",
    "description": "–û–ø–∏—Å–∞–Ω–∏–µ —á—Ç–æ –≤–Ω–µ–¥—Ä–∏—Ç—å",
    "expected_effect": "–û–∂–∏–¥–∞–µ–º—ã–π —ç—Ñ—Ñ–µ–∫—Ç",
    "priority": "high|medium|low",
    "components_needed": ["–∫–æ–º–ø–æ–Ω–µ–Ω—Ç1", "–∫–æ–º–ø–æ–Ω–µ–Ω—Ç2"]
  }}
]"""

        try:
            if AI_CORE_AVAILABLE:
                response = await run_smart_agent_async(
                    hypothesis_prompt,
                    expert_name="–í–∏–∫—Ç–æ—Ä–∏—è",
                    category="reasoning",
                    require_cot=True
                )
                
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
                if response:
                    # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
                    import re
                    json_match = re.search(r'\[.*\]', response, re.DOTALL)
                    if json_match:
                        try:
                            hypotheses = json.loads(json_match.group())
                            logger.info(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(hypotheses)} –≥–∏–ø–æ—Ç–µ–∑")
                            return hypotheses
                        except json.JSONDecodeError:
                            logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑: {e}")
        
        # Fallback: —Å–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –≥–∏–ø–æ—Ç–µ–∑—ã
        return [
            {
                "title": f"–í–Ω–µ–¥—Ä–∏—Ç—å –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–∑ {discussion_summary['topic']}",
                "description": "–í–Ω–µ–¥—Ä–∏—Ç—å –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏",
                "expected_effect": "–£–ª—É—á—à–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã",
                "priority": "medium",
                "components_needed": []
            }
        ]
    
    async def save_hypotheses(self, hypotheses: List[Dict], discussion_id: str = None):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã –≤ –ë–î –∏ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –≤ –¥–µ–±–∞—Ç—ã –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è."""
        pool = await asyncpg.create_pool(self.db_url, min_size=1, max_size=3)
        async with pool.acquire() as conn:
            for hyp in hypotheses:
                content = hyp.get('description', '')
                domain_id = hyp.get('domain_id')
                meta_kn = json.dumps({
                    'type': 'hypothesis',
                    'title': hyp.get('title'),
                    'priority': hyp.get('priority', 'medium'),
                    'expected_effect': hyp.get('expected_effect'),
                    'components_needed': hyp.get('components_needed', []),
                    'discussion_id': discussion_id,
                    'source': 'expert_council'
                })
                created = datetime.now(timezone.utc)
                embedding = None
                try:
                    from semantic_cache import get_embedding
                    embedding = await get_embedding(content[:8000])
                except Exception:
                    pass
                if embedding is not None:
                    kn_id = await conn.fetchval("""
                        INSERT INTO knowledge_nodes (content, domain_id, confidence_score, metadata, created_at, embedding)
                        VALUES ($1, $2, 0.8, $3, $4, $5::vector)
                        RETURNING id
                    """, content, domain_id, meta_kn, created, str(embedding))
                else:
                    kn_id = await conn.fetchval("""
                        INSERT INTO knowledge_nodes (content, domain_id, confidence_score, metadata, created_at)
                        VALUES ($1, $2, 0.8, $3, $4)
                        RETURNING id
                    """, content, domain_id, meta_kn, created)
                # –û—Ç–ø—Ä–∞–≤–∫–∞ –≥–∏–ø–æ—Ç–µ–∑—ã –≤ –¥–µ–±–∞—Ç—ã –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏
                if kn_id and content:
                    try:
                        from nightly_learner import create_debate_for_hypothesis
                        await create_debate_for_hypothesis(conn, kn_id, content[:800], domain_id)
                    except Exception as e:
                        logger.debug("Hypothesis debate skip: %s", e)
        logger.info(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(hypotheses)} –≥–∏–ø–æ—Ç–µ–∑ –≤ –ë–î")


async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫"""
    
    # –ù–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏–∑ –≤–µ–±-–ø–æ–∏—Å–∫–∞
    new_practices = [
        {
            "topic": "Self-Evolving Agents —Å –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º",
            "question": """–û–±—Å—É–¥–∏—Ç–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–æ–Ω–∏—Ä—É—é—â–∏—Ö –∞–≥–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º:
            
1. Intrinsic Metacognitive Learning - —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∞–≥–µ–Ω—Ç–∞ –æ—Ü–µ–Ω–∏–≤–∞—Ç—å, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–π –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è
2. Self-Assessment - —Å–∞–º–æ–æ—Ü–µ–Ω–∫–∞ –∑–Ω–∞–Ω–∏–π –∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–µ–π
3. Metacognitive Planning - –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á—Ç–æ –∏–∑—É—á–∞—Ç—å –¥–∞–ª—å—à–µ
4. Metacognitive Evaluation - —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –Ω–∞–¥ –æ–ø—ã—Ç–æ–º –æ–±—É—á–µ–Ω–∏—è

–ß—Ç–æ –Ω–∞–º –Ω—É–∂–Ω–æ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è? –ö–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–æ–∑–¥–∞—Ç—å?"""
        },
        {
            "topic": "Agent Lifecycle Governance –∏ Registration",
            "question": """–û–±—Å—É–¥–∏—Ç–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∂–∏–∑–Ω–µ–Ω–Ω—ã–º —Ü–∏–∫–ª–æ–º –∞–≥–µ–Ω—Ç–æ–≤:
            
1. Agent Registration - —è–≤–Ω–∞—è —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∏ –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–æ–≤
2. Lifecycle Governance - —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–µ—Ä—Å–∏—è–º–∏, –≤–∞–ª–∏–¥–∞—Ü–∏—è –ø–µ—Ä–µ–¥ –¥–µ–ø–ª–æ–µ–º
3. Agent Versioning - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –≤–µ—Ä—Å–∏–π –∞–≥–µ–Ω—Ç–æ–≤
4. Pre-deployment Validation - –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–¥ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–æ–º

–ö–∞–∫ —ç—Ç–æ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤ –Ω–∞—à–µ–π –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏? –ß—Ç–æ –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å?"""
        },
        {
            "topic": "Separation of Concerns –∏ Secure by Design",
            "question": """–û–±—Å—É–¥–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏:
            
1. Separation of Concerns - —á–µ—Ç–∫–æ–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–æ–≤
2. Secure by Design - –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Å —Å–∞–º–æ–≥–æ –Ω–∞—á–∞–ª–∞
3. Context Management Policies - –ø–æ–ª–∏—Ç–∏–∫–∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –º–µ–∂–¥—É –∞–≥–µ–Ω—Ç–∞–º–∏
4. Failure Isolation - –∏–∑–æ–ª—è—Ü–∏—è —Å–±–æ–µ–≤, graceful degradation

–ö–∞–∫ —É–ª—É—á—à–∏—Ç—å –Ω–∞—à—É –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É? –ö–∞–∫–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–æ–±–∞–≤–∏—Ç—å?"""
        },
        {
            "topic": "AgentEvolver: Self-Questioning, Self-Navigating, Self-Attributing",
            "question": """–û–±—Å—É–¥–∏—Ç–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã —Å–∞–º–æ—ç–≤–æ–ª—é—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤:
            
1. Self-Questioning - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ª—é–±–æ–ø—ã—Ç—Å—Ç–≤–∞
2. Self-Navigating - —É–ª—É—á—à–µ–Ω–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á
3. Self-Attributing - —É–ª—É—á—à–µ–Ω–Ω–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∫–∏

–ö–∞–∫ –≤–Ω–µ–¥—Ä–∏—Ç—å —ç—Ç–∏ –º–µ—Ö–∞–Ω–∏–∑–º—ã? –ß—Ç–æ —Å–æ–∑–¥–∞—Ç—å?"""
        },
        {
            "topic": "Multi-Agent Reference Architecture (Microsoft)",
            "question": """–û–±—Å—É–¥–∏—Ç–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å–Ω—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω—ã—Ö —Å–∏—Å—Ç–µ–º:
            
1. Workflow Agents - —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è
2. Multi-Agent Collaboration - –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏—è
3. Observability & Traceability - –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π
4. Distributed Token Processing - –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

–ö–∞–∫ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –≤ –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º–µ? –ß—Ç–æ —É–ª—É—á—à–∏—Ç—å?"""
        }
    ]
    
    council = ExpertCouncil()
    
    all_hypotheses = []
    
    for practice in new_practices:
        logger.info(f"\n{'='*60}")
        logger.info(f"–û–±—Å—É–∂–¥–∞–µ–º: {practice['topic']}")
        logger.info(f"{'='*60}\n")
        
        # –ü—Ä–æ–≤–æ–¥–∏–º –æ–±—Å—É–∂–¥–µ–Ω–∏–µ
        discussion = await council.conduct_discussion(
            topic=practice['topic'],
            question=practice['question'],
            expert_count=5  # 5 —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –Ω–∞ —Ç–µ–º—É
        )
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≥–∏–ø–æ—Ç–µ–∑—ã
        hypotheses = await council.generate_hypotheses(discussion)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥–∏–ø–æ—Ç–µ–∑—ã
        await council.save_hypotheses(hypotheses)
        
        all_hypotheses.extend(hypotheses)
        
        logger.info(f"\n‚úÖ –û–±—Å—É–∂–¥–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –°–æ–∑–¥–∞–Ω–æ {len(hypotheses)} –≥–∏–ø–æ—Ç–µ–∑\n")
    
    logger.info(f"\nüéâ –í—Å–µ–≥–æ —Å–æ–∑–¥–∞–Ω–æ {len(all_hypotheses)} –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è –≤–Ω–µ–¥—Ä–µ–Ω–∏—è!")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É
    summary_file = "expert_council_discussion_summary.json"
    with open(summary_file, 'w', encoding='utf-8') as f:
        json.dump({
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'practices_discussed': len(new_practices),
            'total_hypotheses': len(all_hypotheses),
            'hypotheses': all_hypotheses
        }, f, ensure_ascii=False, indent=2)
    
    logger.info(f"üìÑ –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {summary_file}")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
