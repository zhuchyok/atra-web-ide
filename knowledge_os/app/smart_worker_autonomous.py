
import asyncio
import os
import json
import sys
import logging
from datetime import datetime
from functools import partial
from typing import Optional

logger = logging.getLogger(__name__)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# 12-Factor: –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–∑ requirements.txt, —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–∏ setup, –Ω–µ –≤ —Ä–∞–Ω—Ç–∞–π–º–µ
try:
    import asyncpg
except ImportError:
    print("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: bash knowledge_os/scripts/setup_knowledge_os.sh (–∏–ª–∏ pip install -r knowledge_os/requirements.txt)", file=sys.stderr)
    sys.exit(1)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ—Ç –∂–µ —Ñ–æ—Ä–º–∞—Ç, —á—Ç–æ –∏ –¥—Ä—É–≥–∏–µ –º–æ–¥—É–ª–∏
DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')

# –ú–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏; –ø–æ—Å–ª–µ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è ‚Äî —ç—Å–∫–∞–ª–∞—Ü–∏—è –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤
MAX_ATTEMPTS = int(os.getenv('SMART_WORKER_MAX_ATTEMPTS', '3'))

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –ø—É–ª —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π (singleton)
_pool = None

# –ö—ç—à —Å–∫–∞–Ω–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π –≤ –≥–ª–∞–≤–Ω–æ–º —Ü–∏–∫–ª–µ –≤–æ—Ä–∫–µ—Ä–∞ (TTL 120 —Å–µ–∫) ‚Äî –º–µ–Ω—å—à–µ –≤—ã–∑–æ–≤–æ–≤ –∫ Ollama/MLX
_scanner_cache_time = 0.0
_scanner_cache_mlx = None
_scanner_cache_ollama = None

async def get_pool():
    global _pool
    if _pool is None:
        # –ü—É–ª –¥–æ–ª–∂–µ–Ω –ø–æ–∫—Ä—ã–≤–∞—Ç—å: –¥–æ MAX_CONCURRENT_TASKS –æ–±—Ä–∞–±–æ—Ç–æ–∫ + —Å—Ç–æ–ª—å–∫–æ –∂–µ heartbeats (–∫–∞–∂–¥—ã–µ 15 —Å–µ–∫ acquire)
        # –†–∞–Ω—å—à–µ max_size=5 –ø—Ä–∏ 10 –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö ‚Üí heartbeats –Ω–µ –ø–æ–ª—É—á–∞–ª–∏ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ ‚Üí updated_at –Ω–µ –æ–±–Ω–æ–≤–ª—è–ª—Å—è ‚Üí –∑–∞–¥–∞—á–∏ —Å—á–∏—Ç–∞–ª–∏—Å—å –∑–∞–≤–∏—Å—à–∏–º–∏
        max_concurrent = int(os.getenv('SMART_WORKER_MAX_CONCURRENT', '10'))
        pool_size = max(15, max_concurrent + 8)
        _pool = await asyncpg.create_pool(
            DB_URL,
            min_size=1,
            max_size=pool_size,
            max_inactive_connection_lifetime=300,
            command_timeout=60
        )
    return _pool

try:
    from ai_core import run_smart_agent_async
except ImportError:
    # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ —Å –ø–æ–ª–Ω—ã–º –ø—É—Ç–µ–º
    import importlib.util
    ai_core_path = os.path.join(os.path.dirname(__file__), 'ai_core.py')
    spec = importlib.util.spec_from_file_location("ai_core", ai_core_path)
    ai_core = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(ai_core)
    run_smart_agent_async = ai_core.run_smart_agent_async

async def run_cursor_agent_smart(prompt: str, expert_name: str, router=None):
    """Smart replacement for the old cursor-agent call. router ‚Äî —Ä–æ—É—Ç–µ—Ä —Å _preferred_source (mlx/ollama), —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –≥–æ–Ω–∫–∏ –ø—Ä–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á–∞—Ö."""
    return await run_smart_agent_async(prompt, expert_name=expert_name, category="autonomous_worker", local_router=router)

def _parse_batch_response(text: str, n: int) -> list:
    """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç LLM –¥–ª—è –±–∞—Ç—á–∞ –∏–∑ N –∑–∞–¥–∞—á. –§–æ—Ä–º–∞—Ç: [RESULT_1]...[/RESULT_1] [RESULT_2]...[/RESULT_2]
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ."""
    import re
    if not text or n < 1:
        return None
    parts = re.findall(r'\[RESULT_\d+\]\s*(.*?)\s*\[/RESULT_\d+\]', text, re.DOTALL)
    if len(parts) >= n:
        return [p.strip() if p else "" for p in parts[:n]]
    # Fallback: split by |||BATCH_SEP|||
    if "|||BATCH_SEP|||" in text:
        parts = text.split("|||BATCH_SEP|||")
        if len(parts) >= n:
            return [p.strip() if p else "" for p in parts[:n]]
    return None


async def escalate_task_to_board(
    pool,
    task_id: int,
    task_title: str,
    task_description: str,
    last_error: str,
    attempt_count: int,
) -> Optional[str]:
    """
    –≠—Å–∫–∞–ª–∞—Ü–∏—è –∑–∞–¥–∞—á–∏ –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ—Å–ª–µ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è –ø–æ–ø—ã—Ç–æ–∫ (MAX_ATTEMPTS).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –¥–∏—Ä–µ–∫—Ç–∏–≤—ã –°–æ–≤–µ—Ç–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ.
    """
    question = (
        f"–ó–∞–¥–∞—á–∞ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –ø–æ—Å–ª–µ {attempt_count} –ø–æ–ø—ã—Ç–æ–∫. –¢—Ä–µ–±—É–µ—Ç—Å—è –≤—ã—è—Å–Ω–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω –∏ —Ä–µ—à–µ–Ω–∏–µ.\n\n"
        f"–ó–∞–¥–∞—á–∞: {task_title}\n\n–û–ø–∏—Å–∞–Ω–∏–µ: {(task_description or '')[:1500]}\n\n"
        f"–ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞/–æ—Ç–≤–µ—Ç: {(last_error or '')[:800]}"
    )
    context = {
        "task_id": task_id,
        "attempt_count": attempt_count,
        "source": "smart_worker_escalation",
    }
    try:
        from strategic_board import consult_board
        result = await consult_board(
            question=question,
            context=context,
            correlation_id=f"task_{task_id}",
            source="task_escalation",
            session_id=None,
            user_id=None,
        )
        if result and isinstance(result, dict):
            return result.get("directive_text") or result.get("directive") or None
        return None
    except Exception as e:
        logger.warning(f"Board escalation failed for task {task_id}: {e}")
        return None


async def process_batch_tasks(pool, tasks: list):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –∑–∞–¥–∞—á –æ–¥–Ω–∏–º –≤—ã–∑–æ–≤–æ–º LLM (ARCHITECTURE_IMPROVEMENTS ¬ß2.5).
    –¢–æ–ª—å–∫–æ –¥–ª—è –∑–∞–¥–∞—á —Å metadata.batch_group. –ü—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ ‚Äî fallback –Ω–∞ –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É."""
    if len(tasks) < 2 or len(tasks) > int(os.getenv("SMART_WORKER_BATCH_GROUP_MAX", "3")):
        return False
    bg = (tasks[0].get("metadata") or {})
    if isinstance(bg, str):
        try:
            bg = json.loads(bg) if bg else {}
        except Exception:
            bg = {}
    if not bg.get("batch_group"):
        return False
    expert_name = tasks[0].get("assignee", "–í–∏–∫—Ç–æ—Ä–∏—è")
    src = tasks[0].get("preferred_source")
    model = tasks[0].get("preferred_model")
    if any((t.get("preferred_source") != src or t.get("preferred_model") != model) for t in tasks):
        return False

    prompt_parts = [
        f"You are {expert_name}. Process these {len(tasks)} short tasks. "
        "Return answers in EXACT format: [RESULT_1]answer for task 1[/RESULT_1] [RESULT_2]answer for task 2[/RESULT_2] etc.",
        ""
    ]
    for i, t in enumerate(tasks, 1):
        prompt_parts.append(f"--- Task {i}: {t.get('title', '')} ---")
        prompt_parts.append(str(t.get("description", ""))[:500])
        prompt_parts.append("")
    combined_prompt = "\n".join(prompt_parts)
    router_instance = None
    if src or model:
        try:
            from local_router import LocalAIRouter
            router_instance = LocalAIRouter()
            if src:
                router_instance._preferred_source = src
            if model:
                router_instance._preferred_model = model
            import ai_core
            if hasattr(ai_core, "_current_router"):
                ai_core._current_router = router_instance
        except Exception:
            pass

    try:
        llm_timeout = float(os.getenv("SMART_WORKER_LLM_TIMEOUT", "300"))
        report = await asyncio.wait_for(
            run_cursor_agent_smart(combined_prompt, expert_name, router=router_instance),
            timeout=llm_timeout,
        )
        if router_instance:
            router_instance._preferred_source = None
            router_instance._preferred_model = None

        if isinstance(report, tuple):
            report = report[0] if report[0] else (report[1] if len(report) > 1 else None)
        elif isinstance(report, dict):
            report = report.get("response", report.get("text", str(report)))
        else:
            report = str(report) if report else ""

        parsed = _parse_batch_response(report, len(tasks))
        if parsed and all(len(p) > 10 for p in parsed):
            async with pool.acquire() as conn:
                for t, result in zip(tasks, parsed):
                    await conn.execute(
                        "UPDATE tasks SET status = 'completed', result = $2, updated_at = NOW() WHERE id = $1",
                        t["id"], result,
                    )
                print(f"[{datetime.now()}] ‚úÖ Batch completed: {len(tasks)} tasks (batch_group)")
                return True
    except Exception as e:
        logger.debug("Batch LLM failed, falling back to individual: %s", e)
    if router_instance:
        router_instance._preferred_source = None
        router_instance._preferred_model = None
    return False


async def process_task(pool, task):
    task_id = task['id']
    expert_name = task['assignee']
    task_title = task['title']
    preferred_source = task.get('preferred_source')  # MLX –∏–ª–∏ Ollama
    print(f'[{datetime.now()}] Expert {expert_name} processing: {task_title} [Source: {preferred_source or "auto"}]')
    
    # Heartbeat –º–µ—Ö–∞–Ω–∏–∑–º - –æ–±–Ω–æ–≤–ª—è–µ–º updated_at –∫–∞–∂–¥—ã–µ 30 —Å–µ–∫—É–Ω–¥, —á—Ç–æ–±—ã –∑–∞–¥–∞—á–∞ –Ω–µ —Å—á–∏—Ç–∞–ª–∞—Å—å –∑–∞—Å—Ç—Ä—è–≤—à–µ–π
    heartbeat_task = None
    heartbeat_stopped = False
    
    async def update_heartbeat():
        """Heartbeat —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –ø–∞–¥–µ–Ω–∏–π - –æ–±–Ω–æ–≤–ª—è–µ—Ç updated_at –∫–∞–∂–¥—ã–µ 15 —Å–µ–∫—É–Ω–¥"""
        nonlocal heartbeat_stopped
        while not heartbeat_stopped:
            try:
                async with pool.acquire() as conn:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–¥–∞—á–∞ –≤—Å–µ –µ—â–µ –≤ in_progress (–Ω–µ –±—ã–ª–∞ —Å–±—Ä–æ—à–µ–Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–æ–º)
                    status = await conn.fetchval("SELECT status FROM tasks WHERE id = $1", task_id)
                    if status != 'in_progress':
                        heartbeat_stopped = True
                        break
                    # –û–±–Ω–æ–≤–ª—è–µ–º updated_at - —ç—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏—è
                    await conn.execute("UPDATE tasks SET updated_at = NOW() WHERE id = $1 AND status = 'in_progress'", task_id)
                await asyncio.sleep(15)  # Heartbeat –∫–∞–∂–¥—ã–µ 15 —Å–µ–∫—É–Ω–¥ (–±—ã—Å—Ç—Ä–µ–µ –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏)
            except asyncio.CancelledError:
                heartbeat_stopped = True
                break
            except Exception as e:
                logger.debug(f"Heartbeat error for task {task_id}: {e}")
                # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ heartbeat
                try:
                    await asyncio.sleep(15)
                except asyncio.CancelledError:
                    heartbeat_stopped = True
                    break
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏
    async with pool.acquire() as conn:
        async with conn.transaction():
            # –ü–æ—Ä–æ–≥ ¬´–∑–∞–≤–∏—Å—à–∞—è in_progress¬ª ‚Äî —Ç–æ—Ç –∂–µ, —á—Ç–æ –∏ –≤ –≥–ª–∞–≤–Ω–æ–º —Ü–∏–∫–ª–µ (STUCK_MINUTES), –∏–Ω–∞—á–µ —Å–ª–æ—Ç—ã –∑–∞–Ω—è—Ç—ã –¥–æ 1 —á
            stuck_mins = int(os.getenv('SMART_WORKER_STUCK_MINUTES', '15'))
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π, —á—Ç–æ –∑–∞–¥–∞—á–∞ –Ω–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è –¥—Ä—É–≥–∏–º worker'–æ–º
            result = await conn.execute("""
                UPDATE tasks 
                SET status = 'in_progress', updated_at = NOW() 
                WHERE id = $1 
                AND (status = 'pending' OR (status = 'in_progress' AND updated_at < NOW() - make_interval(mins => $2::int)))
            """, task_id, stuck_mins)
    
            # –ï—Å–ª–∏ –∑–∞–¥–∞—á–∞ —É–∂–µ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è (–Ω–µ –æ–±–Ω–æ–≤–∏–ª–∞—Å—å), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if result == "UPDATE 0":
                print(f'[{datetime.now()}] Task {task_id} already being processed or recently updated, skipping...')
                return
            
            try:
                from app.expert_aliases import resolve_expert_name_for_db
                resolved_name = resolve_expert_name_for_db(expert_name)
            except ImportError:
                resolved_name = expert_name
            expert_config = await conn.fetchrow("SELECT id, system_prompt, role, department FROM experts WHERE name = $1", resolved_name)
            if not expert_config:
                await conn.execute("UPDATE tasks SET status = 'failed', result = 'Expert not found', updated_at = NOW() WHERE id = $1", task_id)
                return

            # üåü –ú–ò–†–û–í–´–ï –ü–†–ê–ö–¢–ò–ö–ò: –û–±–æ–≥–∞—â–∞–µ–º –∑–∞–¥–∞—á—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ñ–∞–π–ª–æ–≤
            task_description = task['description']
            task_metadata = task.get('metadata', {})
            if isinstance(task_metadata, str):
                try:
                    task_metadata = json.loads(task_metadata)
                except:
                    task_metadata = {}
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —á–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –∏–∑ metadata (–≤ executor ‚Äî sync I/O –Ω–µ –¥–æ–ª–∂–µ–Ω –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop –∏ heartbeats)
            try:
                from file_context_enricher import get_file_enricher
                enricher = get_file_enricher()
                loop = asyncio.get_event_loop()
                file_path = task_metadata.get('file_path') or task_metadata.get('file')
                keywords = task_metadata.get('keywords', [])
                if file_path:
                    task_description = await loop.run_in_executor(
                        None,
                        partial(
                            enricher.enrich_task_with_file_context,
                            task_description,
                            file_path=file_path,
                            metadata=task_metadata,
                            keywords=keywords,
                        ),
                    )
                    logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –æ–±–æ–≥–∞—â–µ–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ñ–∞–π–ª–∞: {file_path}")
                elif task_metadata.get('file_paths'):
                    file_paths = task_metadata.get('file_paths', [])
                    task_description = await loop.run_in_executor(
                        None,
                        partial(
                            enricher.enrich_task_with_multiple_files,
                            task_description,
                            file_paths,
                            task_metadata,
                        ),
                    )
                    logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ {task_id} –æ–±–æ–≥–∞—â–µ–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º {len(file_paths)} —Ñ–∞–π–ª–æ–≤")
            except ImportError:
                logger.debug("file_context_enricher –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –æ–±–æ–≥–∞—â–µ–Ω–∏—è –∑–∞–¥–∞—á–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ")

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç —Å –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º
            # üåü –ú–ò–†–û–í–´–ï –ü–†–ê–ö–¢–ò–ö–ò: –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –æ —Ä–∞–±–æ—Ç–µ —Å –∫–æ–¥–æ–º
            file_access_instructions = ""
            if task_metadata.get('file_path') or task_metadata.get('file_paths'):
                file_access_instructions = """
üìÅ –†–ê–ë–û–¢–ê –° –ö–û–î–û–ú (–ú–ò–†–û–í–´–ï –ü–†–ê–ö–¢–ò–ö–ò):
1. –í –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—ã—à–µ –µ—Å—Ç—å –†–ï–ê–õ–¨–ù–´–ô –ö–û–î —Ñ–∞–π–ª–∞(–æ–≤) - –∏—Å–ø–æ–ª—å–∑—É–π –ï–ì–û –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
2. –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–¥–µ
3. –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç read_file (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω —á–µ—Ä–µ–∑ –∞–≥–µ–Ω—Ç–∞)
4. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –¢–û–õ–¨–ö–û —Ç–æ, —á—Ç–æ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –≤ –∫–æ–¥–µ
5. –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û —Ç–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –≤ –∫–æ–¥–µ
"""
            
            # üåü –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: –ó–∞–¥–∞—á–∏ —Ä–∞–∑–≤–µ–¥–∫–∏ (–¥–æ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–º–ø—Ç–∞)
            if task_metadata.get('source') in ('scout_orchestrator', 'dashboard_scout', 'enhanced_scout_orchestrator'):
                try:
                    sys.path.insert(0, os.path.dirname(__file__))
                    from scout_task_processor import process_scout_task
                    logger.info(f"üïµÔ∏è –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–¥–∞—á–∏ —Ä–∞–∑–≤–µ–¥–∫–∏: {task['title']}")
                    scout_result = await process_scout_task(task_metadata, task_description)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                    async with pool.acquire() as conn:
                        async with conn.transaction():
                            await conn.execute(
                                "UPDATE tasks SET status = 'completed', result = $2, updated_at = NOW() WHERE id = $1",
                                task_id, scout_result
                            )
                    logger.info(f"‚úÖ –ó–∞–¥–∞—á–∞ —Ä–∞–∑–≤–µ–¥–∫–∏ {task_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {scout_result[:100]}...")
                    return  # –í—ã—Ö–æ–¥–∏–º, –Ω–µ –≤—ã–∑—ã–≤–∞—è LLM
                except ImportError as e:
                    logger.warning(f"scout_task_processor –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ LLM")
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á–∏ —Ä–∞–∑–≤–µ–¥–∫–∏: {e}, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ LLM")
                    import traceback
                    traceback.print_exc()

            # üåü –°–ü–ï–¶–ò–ê–õ–¨–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê: –°–∏–º—É–ª—è—Ü–∏—è –±–∏–∑–Ω–µ—Å-–∏–¥–µ–∏ (–¥–∞—à–±–æ—Ä–¥)
            if task_metadata.get('source') == 'dashboard_simulator':
                sim_id = task_metadata.get('simulation_id')
                if sim_id is not None:
                    try:
                        from simulator import run_simulation as run_sim
                        logger.info(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ –±–∏–∑–Ω–µ—Å-–∏–¥–µ–∏ #{sim_id}: {task['title']}")
                        await run_sim(int(sim_id))
                        async with pool.acquire() as conn:
                            result_text = await conn.fetchval("SELECT result FROM simulations WHERE id = $1", int(sim_id))
                            if result_text:
                                await conn.execute(
                                    "UPDATE tasks SET status = 'completed', result = $2, updated_at = NOW() WHERE id = $1",
                                    task_id, result_text
                                )
                                logger.info(f"‚úÖ –°–∏–º—É–ª—è—Ü–∏—è #{sim_id} –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –∑–∞–¥–∞—á–∞ {task_id} –æ—Ç–º–µ—á–µ–Ω–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π.")
                            else:
                                await conn.execute(
                                    "UPDATE tasks SET status = 'failed', result = '–°–∏–º—É–ª—è—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞, –Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –∑–∞–ø–∏—Å–∞–Ω', updated_at = NOW() WHERE id = $1",
                                    task_id
                                )
                        return
                    except ImportError as e:
                        logger.warning(f"simulator –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ LLM")
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ —Å–∏–º—É–ª—è—Ü–∏–∏ #{sim_id}: {e}", exc_info=True)
                        async with pool.acquire() as conn:
                            await conn.execute(
                                "UPDATE tasks SET status = 'failed', result = $2, updated_at = NOW() WHERE id = $1",
                                task_id, f"–û—à–∏–±–∫–∞ —Å–∏–º—É–ª—è—Ü–∏–∏: {str(e)}"
                            )
                        return
            
            prompt = f"""{expert_config['system_prompt']}

Role: {expert_config['role']}
Dept: {expert_config['department']}

TASK: {task['title']}

DESC: {task_description}
{file_access_instructions}

üí° –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: 
- –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –≤—ã—à–µ –µ—Å—Ç—å –∫–æ–¥ —Ñ–∞–π–ª–∞, –∏—Å–ø–æ–ª—å–∑—É–π –ï–ì–û –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–¥–µ!
- –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ—á–∏—Ç–∞—Ç—å –¥—Ä—É–≥–∏–µ —Ñ–∞–π–ª—ã, –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç read_file (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
"""
    
    # –ö–†–ò–¢–ò–ß–ù–û: –ó–∞–ø—É—Å–∫–∞–µ–º heartbeat –°–†–ê–ó–£ –ø–æ—Å–ª–µ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤ in_progress, –î–û –≤—ã–∑–æ–≤–∞ run_cursor_agent_smart
    # –≠—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ updated_at –±—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª—è—Ç—å—Å—è –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–∏—Å–Ω–µ—Ç
    heartbeat_task = asyncio.create_task(update_heartbeat())
    
    # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞, —á—Ç–æ–±—ã –ø–µ—Ä–≤—ã–π heartbeat —É—Å–ø–µ–ª –≤—ã–ø–æ–ª–Ω–∏—Ç—å—Å—è
    await asyncio.sleep(0.1)
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ –º–æ–¥–µ–ª—å –¥–ª—è router (–±–∞—Ç—á–∏ –ø–æ –º–æ–¥–µ–ª–∏ ‚Äî –º–µ–Ω—å—à–µ load/unload –Ω–∞ MLX/Ollama)
    router_instance = None
    preferred_model = task.get('preferred_model')
    if preferred_source or preferred_model:
        try:
            from local_router import LocalAIRouter
            router_instance = LocalAIRouter()
            if preferred_source:
                router_instance._preferred_source = preferred_source
            if preferred_model:
                router_instance._preferred_model = preferred_model
            import ai_core
            if hasattr(ai_core, '_current_router'):
                ai_core._current_router = router_instance
        except Exception as e:
            logger.debug(f"Could not set preferred source/model: {e}")
    
    if router_instance:
        router_instance._current_task_id = task_id
    
    # –ü—Ä–∏—á–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–±–æ—è (—Ç–∞–π–º–∞—É—Ç/–∏—Å–∫–ª—é—á–µ–Ω–∏–µ) ‚Äî —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ last_error –∏ –ø–µ—Ä–µ–¥–∞—ë–º –≤ –°–æ–≤–µ—Ç –ø—Ä–∏ —ç—Å–∫–∞–ª–∞—Ü–∏–∏
    _last_failure_reason = None
    # –í—ã–ø–æ–ª–Ω—è–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–Ω–µ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –¥–æ–ª–≥–æ–π)
    try:
        try:
            # –¢–∞–π–º–∞—É—Ç –∏–∑ env (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 300 —Å–µ–∫ = 5 –º–∏–Ω)
            llm_timeout = float(os.getenv('SMART_WORKER_LLM_TIMEOUT', '300'))
            # –î–ª—è —Ç—è–∂—ë–ª—ã—Ö –º–æ–¥–µ–ª–µ–π: —É—á–µ—Å—Ç—å –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏ (30-90 —Å–µ–∫); –∏–Ω–∞—á–µ ReadTimeout –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ
            if preferred_model:
                try:
                    from adaptive_concurrency import is_model_heavy
                    if is_model_heavy(preferred_model):
                        mult = float(os.getenv('SMART_WORKER_HEAVY_MODEL_TIMEOUT_MULTIPLIER', '1.5'))
                        llm_timeout = max(llm_timeout, llm_timeout * mult)
                        llm_timeout = min(llm_timeout, 600)  # –Ω–µ –±–æ–ª—å—à–µ 10 –º–∏–Ω
                except ImportError:
                    pass
            report = await asyncio.wait_for(
                run_cursor_agent_smart(prompt, expert_name, router=router_instance),
                timeout=llm_timeout
            )
        except asyncio.TimeoutError:
            _last_failure_reason = "timeout"
            print(f'[{datetime.now()}] ‚è±Ô∏è Task {task_id} timed out after {llm_timeout}s')
            report = None
        except Exception as e:
            _last_failure_reason = str(e)[:500]
            print(f'[{datetime.now()}] Error calling agent for task {task_id}: {e}')
            import traceback
            traceback.print_exc()
            report = None

        # –ü–æ–ª—É—á–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏–∑ router'–∞
        used_model = None
        if router_instance and hasattr(router_instance, '_used_model'):
            used_model = router_instance._used_model
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ metadata –∑–∞–¥–∞—á–∏
            async with pool.acquire() as conn:
                await conn.execute("""
                    UPDATE tasks 
                    SET metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object('used_model', $2::text)
                    WHERE id = $1
                """, task_id, str(used_model) if used_model else '')
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –æ—Ç–≤–µ—Ç–æ–≤
        if report is None:
            report = None
        elif isinstance(report, tuple):
            # –ï—Å–ª–∏ –∫–æ—Ä—Ç–µ–∂ - –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç (–æ—Ç–≤–µ—Ç)
            report = report[0] if report[0] else (report[1] if len(report) > 1 else None)
        elif isinstance(report, dict):
            report = report.get('response', report.get('text', str(report)))
        elif not isinstance(report, str):
            report = str(report)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        print(f'[{datetime.now()}] Agent response for task {task_id} (length: {len(report) if report else 0}): {report[:100] if report else "None"}...')

        # –ë–æ–ª–µ–µ –º—è–≥–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ - –ø—Ä–∏–Ω–∏–º–∞–µ–º –ª—é–±–æ–π –æ—Ç–≤–µ—Ç –¥–ª–∏–Ω–Ω–µ–µ 5 —Å–∏–º–≤–æ–ª–æ–≤
        if report and isinstance(report, str) and len(report.strip()) > 5:
            # –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏
            try:
                from model_performance_tracker import get_performance_tracker
                tracker = get_performance_tracker()
                
                # –í—ã—á–∏—Å–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –æ—Ç–≤–µ—Ç–∞
                quality_score = tracker.calculate_quality_score(report)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–∏–∑ metadata –∑–∞–¥–∞—á–∏ –∏–ª–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                used_model = 'phi3.5:3.8b'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
                try:
                    async with pool.acquire() as conn:
                        metadata = await conn.fetchval("SELECT metadata FROM tasks WHERE id = $1", task_id)
                        if metadata and metadata.get('used_model'):
                            used_model = metadata['used_model']
                except:
                    pass
                
                # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –ø–æ–ø—ã—Ç–∫—É
                await tracker.record_attempt(
                    task_id=task_id,
                    model=used_model,
                    category='autonomous_worker',
                    success=True,
                    response_length=len(report),
                    latency_ms=0,  # TODO: –¥–æ–±–∞–≤–∏—Ç—å –∏–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
                    quality_score=quality_score
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å
                should_upgrade, next_model = await tracker.should_upgrade_model(
                    task_id=task_id,
                    current_model=used_model,
                    category='autonomous_worker',
                    response=report
                )
                
                if should_upgrade and next_model:
                    logger.info(f"üîÑ [MODEL UPGRADE] –ó–∞–¥–∞—á–∞ {task_id} —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å: {next_model}")
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∞–ø–≥—Ä–µ–π–¥–∞
                    async with pool.acquire() as conn:
                        await conn.execute("""
                            UPDATE tasks 
                            SET metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object('model_upgrade_needed', true, 'recommended_model', $2::text)
                            WHERE id = $1
                        """, task_id, str(next_model) if next_model else '')
            except Exception as e:
                logger.debug(f"Model performance tracking failed: {e}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–æ–æ–±—â–µ–Ω–∏–µ–º –æ–± –æ—à–∏–±–∫–µ
            error_indicators = ['‚ö†Ô∏è', '‚ùå', '‚åõ', 'Error', 'failed', '–Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω', '–Ω–µ –º–æ–≥—É', '–í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã', '–û—à–∏–±–∫–∞ —Å–≤—è–∑–∏']
            is_error = any(indicator in report for indicator in error_indicators)
            # LLM unavailable ‚Äî —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —è–≤–Ω—ã—Ö –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö –æ–± –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ (–Ω–µ –¥–ª–∏–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —Å–ª–æ–≤–æ–º "–Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            report_lower = (report or '').lower()
            report_len = len((report or '').strip())
            _unavailable_phrases = (
                '–≤—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã', '–º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã', '—Å–∏—Å—Ç–µ–º–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞',
                'all sources unavailable', 'models unavailable', 'connection refused'
            )
            is_llm_unavailable = (
                (report_len < 350 and ('–Ω–µ–¥–æ—Å—Ç—É–ø–Ω' in report_lower or 'unavailable' in report_lower))
                or any(phrase in report_lower for phrase in _unavailable_phrases)
            )
            
            if is_error:
                print(f'[{datetime.now()}] ‚ö†Ô∏è Agent returned error for task {task_id}: {report[:150]}...')
                
                attempt_count = 0
                try:
                    async with pool.acquire() as conn:
                        metadata = await conn.fetchval("SELECT metadata FROM tasks WHERE id = $1", task_id)
                        # metadata –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π JSON –∏–ª–∏ dict (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç asyncpg)
                        if metadata:
                            if isinstance(metadata, str):
                                metadata = json.loads(metadata)
                            if isinstance(metadata, dict) and metadata.get('attempt_count'):
                                attempt_count = int(metadata.get('attempt_count', 0))
                except (asyncpg.PostgresError, ValueError, TypeError, json.JSONDecodeError) as e:
                    logger.debug(f"Error reading attempt_count for task {task_id}: {e}, using default 0")
                    attempt_count = 0
                attempt_count += 1
                
                # –ü–æ—Å–ª–µ MAX_ATTEMPTS: rule ‚Üí —ç—Å–∫–∞–ª–∞—Ü–∏—è –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ ‚Üí complete —Å –¥–∏—Ä–µ–∫—Ç–∏–≤–æ–π –∏–ª–∏ deferred
                should_try_rule_or_escalate = (is_llm_unavailable and attempt_count >= 2) or attempt_count >= MAX_ATTEMPTS
                if should_try_rule_or_escalate:
                    rule_result = None
                    try:
                        from task_rule_executor import execute_fallback as rule_execute, can_handle as rule_can_handle
                        task_dict = dict(task) if not isinstance(task, dict) else task
                        if rule_can_handle(task_dict):
                            rule_result = await rule_execute(task_dict)
                    except Exception as e:
                        logger.debug("Rule executor failed for task %s: %s", task_id, e)
                    if rule_result:
                        async with pool.acquire() as conn:
                            await conn.execute("""
                                UPDATE tasks SET status = 'completed', result = $2, updated_at = NOW(),
                                    metadata = COALESCE(metadata, '{}'::jsonb) || '{"execution_mode": "rule_based", "llm_unavailable_fallback": true}"::jsonb
                                WHERE id = $1
                            """, task_id, rule_result)
                        print(f'[{datetime.now()}] ‚úÖ Task {task_id} completed via rule_executor (LLM unavailable)')
                        return
                    # rule –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª ‚Äî —ç—Å–∫–∞–ª–∞—Ü–∏—è –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤, –∑–∞—Ç–µ–º complete
                    board_directive = await escalate_task_to_board(
                        pool, task_id, task_title, task_description or "", report[:500] if report else "", attempt_count
                    )
                    final_result = f"""–ó–∞–¥–∞—á–∞: {task_title}
–°—Ç–∞—Ç—É—Å: AI –∞–≥–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ—Å–ª–µ {attempt_count} –ø–æ–ø—ã—Ç–æ–∫. –ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤.
–û—à–∏–±–∫–∞: {(report or '')[:300]}
[deferred_to_human: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞]"""
                    if board_directive:
                        final_result += f"\n\n--- –†–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ ---\n{board_directive[:2000]}"
                    meta_escalation = json.dumps({
                        'attempt_count': attempt_count,
                        'deferred_to_human': True,
                        'execution_mode': 'minimal_response',
                        'board_escalated': True,
                    })
                    async with pool.acquire() as conn:
                        await conn.execute("""
                            UPDATE tasks 
                            SET status = 'completed', result = $2, updated_at = NOW(),
                                metadata = COALESCE(metadata, '{}'::jsonb) || $3::jsonb
                            WHERE id = $1
                        """, task_id, final_result, meta_escalation)
                    print(f'[{datetime.now()}] ‚úÖ Task {task_id} completed with board escalation (attempt {attempt_count})')
                    return
                # attempt_count < 3 –∏ –Ω–µ LLM unavailable: retry
                else:
                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –Ω–µ—É–¥–∞—á–Ω—É—é –ø–æ–ø—ã—Ç–∫—É
                    try:
                        from model_performance_tracker import get_performance_tracker
                        tracker = get_performance_tracker()
                        used_model = 'phi3.5:3.8b'
                        try:
                            async with pool.acquire() as conn:
                                metadata = await conn.fetchval("SELECT metadata FROM tasks WHERE id = $1", task_id)
                                if metadata and metadata.get('used_model'):
                                    used_model = metadata['used_model']
                        except:
                            pass
                        
                        await tracker.record_attempt(
                            task_id=task_id,
                            model=used_model,
                            category='autonomous_worker',
                            success=False,
                            response_length=len(report) if report else 0,
                            quality_score=0.0
                        )
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å—Å—è –Ω–∞ –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å
                        should_upgrade, next_model = await tracker.should_upgrade_model(
                            task_id=task_id,
                            current_model=used_model,
                            category='autonomous_worker',
                            response=report
                        )
                        
                        if should_upgrade and next_model:
                            logger.info(f"üîÑ [AUTO UPGRADE] –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ {next_model} –¥–ª—è –∑–∞–¥–∞—á–∏ {task_id}")
                            # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–¥–∞—á—É —Å —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é
                            async with pool.acquire() as conn:
                                await conn.execute("""
                                    UPDATE tasks 
                                    SET status = 'pending', 
                                        updated_at = NOW(), 
                                        metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object(
                                            'last_attempt_failed', true, 
                                            'attempt_count', $2::int, 
                                            'last_error', $3::text,
                                            'model_upgrade_needed', true,
                                            'recommended_model', $4::text
                                        )
                                    WHERE id = $1
                                """, task_id, attempt_count, str(report[:500]), str(next_model))
                            print(f'[{datetime.now()}] üîÑ Task {task_id} upgraded to model {next_model} for retry')
                            return
                    except Exception as e:
                        logger.debug(f"Model upgrade check failed: {e}")
                    
                    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤ pending –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –ø–æ–ø—ã—Ç–∫–∏
                    async with pool.acquire() as conn:
                        await conn.execute("""
                            UPDATE tasks 
                            SET status = 'pending', 
                                updated_at = NOW(), 
                                metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object('last_attempt_failed', true, 'attempt_count', $2::int, 'last_error', $3::text)
                            WHERE id = $1
                        """, task_id, attempt_count, str(report[:500]))
                    print(f'[{datetime.now()}] ‚ö†Ô∏è Task {task_id} reverted to PENDING (attempt {attempt_count}/{MAX_ATTEMPTS}). Will retry later.')
                return  # –ù–ï –ø–æ–º–µ—á–∞–µ–º –∫–∞–∫ completed!
            
            # –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø–µ—Ä–µ–¥ –æ—Ç–º–µ—Ç–∫–æ–π completed (–∞–Ω–∞–ª–æ–≥ manager_review –≤ —Ü–µ–ø–æ—á–∫–µ –ë–î)
            # –ù–µ—É—Å–ø–µ—à–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ–ø—ã—Ç–∫–æ–π; –ø–æ—Å–ª–µ MAX_ATTEMPTS ‚Äî —ç—Å–∫–∞–ª–∞—Ü–∏—è –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤
            try:
                try:
                    from task_result_validator import validate_task_result
                except ImportError:
                    from app.task_result_validator import validate_task_result
                req_text = (task.get('title') or '') + ' ' + (task_description or '')
                is_valid, score = validate_task_result(req_text, report or '')
                if not is_valid or score < 0.5:
                    v_attempt_count = 0
                    try:
                        async with pool.acquire() as conn:
                            meta = await conn.fetchval("SELECT metadata FROM tasks WHERE id = $1", task_id)
                            if meta and (isinstance(meta, dict) and meta.get('attempt_count') is not None):
                                v_attempt_count = int(meta.get('attempt_count', 0))
                            elif meta and isinstance(meta, str):
                                import json as _j
                                m = _j.loads(meta) if meta else {}
                                v_attempt_count = int(m.get('attempt_count', 0))
                    except Exception:
                        pass
                    v_attempt_count += 1
                    if v_attempt_count >= MAX_ATTEMPTS:
                        last_err = f"–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –ø—Ä–æ–π–¥–µ–Ω–∞ (score={score:.2f}); –ø–æ–ø—ã—Ç–æ–∫: {v_attempt_count}"
                        board_directive = await escalate_task_to_board(
                            pool, task_id, task_title, task_description or "", last_err, v_attempt_count
                        )
                        final_result = f"""–ó–∞–¥–∞—á–∞: {task_title}
–°—Ç–∞—Ç—É—Å: –†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –ø—Ä–æ—à—ë–ª –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ—Å–ª–µ {v_attempt_count} –ø–æ–ø—ã—Ç–æ–∫. –ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤.
–ü—Ä–∏—á–∏–Ω–∞: {last_err}
[deferred_to_human: —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞]"""
                        if board_directive:
                            final_result += f"\n\n--- –†–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ ---\n{board_directive[:2000]}"
                        meta_v = json.dumps({
                            'attempt_count': v_attempt_count, 'validation_failed': True,
                            'validation_score': float(score), 'board_escalated': True, 'deferred_to_human': True,
                        })
                        async with pool.acquire() as conn:
                            await conn.execute("""
                                UPDATE tasks SET status = 'completed', result = $2, updated_at = NOW(),
                                    metadata = COALESCE(metadata, '{}'::jsonb) || $3::jsonb
                                WHERE id = $1
                            """, task_id, final_result, meta_v)
                        print(f'[{datetime.now()}] ‚úÖ Task {task_id} completed with board escalation after validation failure (attempt {v_attempt_count})')
                    else:
                        async with pool.acquire() as conn:
                            await conn.execute("""
                                UPDATE tasks SET status = 'pending', updated_at = NOW(),
                                    metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object(
                                        'validation_failed', true, 'validation_score', $2::float, 'attempt_count', $3::int
                                    )
                                WHERE id = $1
                            """, task_id, float(score), v_attempt_count)
                        print(f'[{datetime.now()}] ‚ö†Ô∏è Task {task_id} validation failed (attempt {v_attempt_count}/{MAX_ATTEMPTS}), reverted to pending')
                    return
            except ImportError:
                pass
            except Exception as e:
                logger.debug(f"Validation skip for task {task_id}: {e}")
    
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï –æ—à–∏–±–∫–∞)
            async with pool.acquire() as conn:
                async with conn.transaction():
                    await conn.execute("UPDATE tasks SET status = 'completed', result = $2, updated_at = NOW() WHERE id = $1", task_id, report)
                    logger.info("Task %s marked completed in DB (updated_at=NOW()).", task_id)
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ knowledge_nodes —Å embedding (–¥–ª—è RAG/search) ‚Äî –∑–Ω–∞–Ω–∏—è –≤–Ω–µ–¥—Ä—è—é—Ç—Å—è –≤ —Å–∏—Å—Ç–µ–º—É
                    try:
                        content_for_kn = f"üìä REPORT BY {expert_name}: {task_title}\n\n{report}"
                        embedding = None
                        try:
                            from semantic_cache import get_embedding
                            embedding = await get_embedding(content_for_kn[:8000])  # –ª–∏–º–∏—Ç –¥–ª—è embedding
                        except Exception as emb_err:
                            logger.debug("Embedding skip for knowledge_node: %s", emb_err)
                        meta_kn = json.dumps({
                            'task_id': str(task_id), 'expert': expert_name,
                            'fallback_used': is_error, 'department': expert_config['department']
                        })
                        if embedding:
                            await conn.execute("""
                                INSERT INTO knowledge_nodes (content, metadata, confidence_score, source_ref, embedding)
                                VALUES ($1, $2, 0.85, $3, $4::vector)
                            """, content_for_kn, meta_kn, 'autonomous_worker', str(embedding))
                        else:
                            await conn.execute("""
                                INSERT INTO knowledge_nodes (content, metadata, confidence_score, source_ref)
                                VALUES ($1, $2, 0.85, $3)
                            """, content_for_kn, meta_kn, 'autonomous_worker')
                        print(f'[{datetime.now()}] ‚úÖ Knowledge saved for task {task_id}' + (' (with embedding)' if embedding else ''))
                    except Exception as e:
                        print(f'[{datetime.now()}] ‚ö†Ô∏è Error saving to knowledge_nodes: {e}')
                        import traceback
                        traceback.print_exc()
        print(f'[{datetime.now()}] ‚úÖ Task {task_id} COMPLETED.')
    except Exception as e:
        _last_failure_reason = str(e)[:500]
        print(f'[{datetime.now()}] ‚ùå Error processing task {task_id}: {e}')
        import traceback
        traceback.print_exc()
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∑–∞–¥–∞—á—É –≤ pending –ø—Ä–∏ –æ—à–∏–±–∫–µ
        async with pool.acquire() as conn:
            await conn.execute("""
                UPDATE tasks 
                SET status = 'pending', 
                    updated_at = NOW(),
                    metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object('processing_error', $2::text)
                WHERE id = $1
            """, task_id, _last_failure_reason)
    finally:
        # –û—á–∏—â–∞–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ –∏ –º–æ–¥–µ–ª—å
        if router_instance:
            if hasattr(router_instance, '_preferred_source'):
                router_instance._preferred_source = None
            if hasattr(router_instance, '_preferred_model'):
                router_instance._preferred_model = None
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º heartbeat –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
        heartbeat_stopped = True
        if heartbeat_task and not heartbeat_task.done():
            heartbeat_task.cancel()
            try:
                await asyncio.wait_for(heartbeat_task, timeout=1.0)
            except (asyncio.CancelledError, asyncio.TimeoutError):
                pass
    
    if not (report and isinstance(report, str) and len(report.strip()) > 5):
        # –ï—Å–ª–∏ –∞–≥–µ–Ω—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç, —Å–æ–∑–¥–∞–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏ –∑–∞–≤–µ—Ä—à–∞–µ–º –∑–∞–¥–∞—á—É
        attempt_count = 0
        metadata = None
        try:
            async with pool.acquire() as conn:
                metadata = await conn.fetchval("SELECT metadata FROM tasks WHERE id = $1", task_id)
                if metadata and (isinstance(metadata, dict) and metadata.get('attempt_count') is not None):
                    attempt_count = int(metadata.get('attempt_count', 0))
                elif metadata and isinstance(metadata, str):
                    try:
                        m = json.loads(metadata)
                        attempt_count = int(m.get('attempt_count', 0))
                    except (TypeError, ValueError, KeyError):
                        pass
        except Exception:
            pass
        meta_dict = metadata if isinstance(metadata, dict) else {}
        if not isinstance(meta_dict, dict):
            try:
                meta_dict = json.loads(metadata) if metadata else {}
            except (TypeError, ValueError, json.JSONDecodeError):
                meta_dict = {}
        last_error_text = (
            _last_failure_reason
            or meta_dict.get('processing_error')
            or meta_dict.get('last_error')
            or 'empty_or_short_response'
        )
        attempt_count += 1

        # –ü–æ—Å–ª–µ MAX_ATTEMPTS: rule ‚Üí —ç—Å–∫–∞–ª–∞—Ü–∏—è –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ ‚Üí complete
        if attempt_count >= MAX_ATTEMPTS:
            rule_result = None
            try:
                from task_rule_executor import execute_fallback as rule_execute, can_handle as rule_can_handle
                task_dict = dict(task) if not isinstance(task, dict) else task
                if rule_can_handle(task_dict):
                    rule_result = await rule_execute(task_dict)
            except ImportError:
                pass
            except Exception as e:
                logger.debug("Rule executor failed for task %s: %s", task_id, e)

            final_result = rule_result
            exec_mode = "rule_based" if rule_result else "minimal_response"
            deferred = not rule_result

            if not final_result:
                # –≠—Å–∫–∞–ª–∞—Ü–∏—è –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ (–ø–µ—Ä–µ–¥–∞—ë–º –ø—Ä–∏—á–∏–Ω—É —Å–±–æ—è –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
                task_title = task.get('title', '')
                task_description = task.get('description', '')
                board_directive = await escalate_task_to_board(
                    pool, task_id, task_title, task_description or "", last_error_text, attempt_count
                )
                print(f'[{datetime.now()}] ‚ö†Ô∏è Task {task_id} failed after {attempt_count} attempts, escalated to board (reason: {last_error_text[:80]}...)')
                final_result = f"""–ó–∞–¥–∞—á–∞: {task_title}

–°—Ç–∞—Ç—É—Å: –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ {attempt_count} –Ω–µ—É–¥–∞—á–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫. –ó–∞–¥–∞—á–∞ –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ –°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –≤—ã—è—Å–Ω–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω.
–ü—Ä–∏—á–∏–Ω–∞: {last_error_text[:500]}

[deferred_to_human: true ‚Äî —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä—É—á–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞]"""
                if board_directive:
                    final_result += f"\n\n--- –†–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ ---\n{board_directive[:2000]}"

            assignee_id = task.get('assignee_expert_id')
            meta_extra = json.dumps({
                'auto_completed': True, 'attempt_count': attempt_count,
                'execution_mode': exec_mode,
                'deferred_to_human': deferred,
                'board_escalated': not bool(rule_result),
                'last_error': last_error_text[:500],
            })
            async with pool.acquire() as conn:
                if assignee_id:
                    await conn.execute("""
                        UPDATE tasks 
                        SET status = 'completed', result = $2, updated_at = NOW(),
                            assignee_expert_id = $4,
                            metadata = COALESCE(metadata, '{}'::jsonb) || $5::jsonb
                        WHERE id = $1
                    """, task_id, final_result, assignee_id, meta_extra)
                else:
                    await conn.execute("""
                        UPDATE tasks 
                        SET status = 'completed', result = $2, updated_at = NOW(),
                            assignee_expert_id = (SELECT id FROM experts WHERE name = '–í–∏–∫—Ç–æ—Ä–∏—è' LIMIT 1),
                            metadata = COALESCE(metadata, '{}'::jsonb) || $4::jsonb
                        WHERE id = $1
                    """, task_id, final_result, meta_extra)
            print(f'[{datetime.now()}] ‚úÖ Task {task_id} AUTO-COMPLETED after {attempt_count} attempts (mode={exec_mode}, board_escalated={not bool(rule_result)}).')
        else:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—á–µ—Ç—á–∏–∫ –ø–æ–ø—ã—Ç–æ–∫, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏—á–∏–Ω—É —Å–±–æ—è –∏ –∑–∞–¥–µ—Ä–∂–∫—É –ø–µ—Ä–µ–¥ –ø–æ–≤—Ç–æ—Ä–æ–º (—á—Ç–æ–±—ã –Ω–µ –±–∏—Ç—å LLM —Å—Ä–∞–∑—É)
            retry_delay_sec = int(os.getenv('SMART_WORKER_RETRY_DELAY_SEC', '90'))
            next_retry_after = (datetime.utcnow().timestamp() + retry_delay_sec) if retry_delay_sec > 0 else None
            meta_pending = {
                'last_attempt_failed': True,
                'attempt_count': attempt_count,
                'last_error': last_error_text[:500],
            }
            if next_retry_after is not None:
                from datetime import timezone
                # ISO timestamp –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞ –≤ SELECT
                meta_pending['next_retry_after'] = datetime.fromtimestamp(next_retry_after, tz=timezone.utc).isoformat()
            async with pool.acquire() as conn:
                await conn.execute("""
                    UPDATE tasks 
                    SET status = 'pending', 
                        updated_at = NOW(),
                        metadata = COALESCE(metadata, '{}'::jsonb) || $2::jsonb
                    WHERE id = $1
                """, task_id, json.dumps(meta_pending))
            print(f'[{datetime.now()}] ‚ö†Ô∏è Task {task_id} FAILED (attempt {attempt_count}/{MAX_ATTEMPTS}, reason: {last_error_text[:60]}...). Reverted to pending (retry after {retry_delay_sec}s).')
    
    # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º heartbeat –≤ –ª—é–±–æ–º —Å–ª—É—á–∞–µ
    heartbeat_stopped = True
    if heartbeat_task and not heartbeat_task.done():
        heartbeat_task.cancel()
        try:
            await asyncio.wait_for(heartbeat_task, timeout=1.0)
        except (asyncio.CancelledError, asyncio.TimeoutError):
            pass

async def main():
    print(f'[{datetime.now()}] üöÄ AUTONOMOUS SMART WORKER v4.0 (PARALLEL) starting...')
    pool = await get_pool()
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (Backend/SRE: –ø—É–ª –¥–æ—Å—Ç–∞—Ç–æ—á–µ–Ω –ø—Ä–∏ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–º N ‚Äî max_size –ø–æ –ø–æ—Ç–æ–ª–∫—É)
    MAX_CONCURRENT_TASKS = int(os.getenv('SMART_WORKER_MAX_CONCURRENT', '10'))
    BATCH_SIZE = int(os.getenv('SMART_WORKER_BATCH_SIZE', '50'))
    ADAPTIVE_CONCURRENCY = os.getenv('SMART_WORKER_ADAPTIVE_CONCURRENCY', 'true').lower() in ('true', '1', 'yes')
    
    print(f'[{datetime.now()}] ‚ö° Parallel processing: max {MAX_CONCURRENT_TASKS} concurrent, batch size: {BATCH_SIZE}, adaptive={ADAPTIVE_CONCURRENCY}')
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–∏—Å—Ç–µ–º—É —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ (Singularity 10.0)
    try:
        from corporation_self_learning import get_corporation_learner
        learner = get_corporation_learner()
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ —Ñ–æ–Ω–µ
        asyncio.create_task(learner.start_continuous_learning(interval_hours=6))
        print(f'[{datetime.now()}] üß† [SINGULARITY 10.0] –°–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –∑–∞–ø—É—â–µ–Ω–∞')
    except Exception as e:
        logger.debug(f"Could not start corporation learning: {e}")
    
    # –ò–Ω—Ç–µ—Ä–≤–∞–ª —Å–±—Ä–æ—Å–∞ –∑–∞–≤–∏—Å—à–∏—Ö in_progress: –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 15 –º–∏–Ω (—Ä–∞–Ω—å—à–µ 1 —á ‚Äî –∏–∑‚Äë–∑–∞ —ç—Ç–æ–≥–æ –ø—Ä–∏ 10 –∑–∞–≤–∏—Å—à–∏—Ö —Ç–æ–ª—å–∫–æ 5 pending –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–ª–∏—Å—å –∑–∞ —Ü–∏–∫–ª, ~5 –∑–∞–¥–∞—á/—á–∞—Å)
    STUCK_MINUTES = int(os.getenv('SMART_WORKER_STUCK_MINUTES', '15'))
    
    while True:
        try:
            # –í–µ—Ä–Ω—É—Ç—å –∑–∞–≤–∏—Å—à–∏–µ in_progress (> N –º–∏–Ω) –≤ pending, —á—Ç–æ–±—ã –≤–æ—Ä–∫–µ—Ä –∏—Ö –ø–æ–¥—Ö–≤–∞—Ç–∏–ª
            async with pool.acquire() as conn:
                stuck_result = await conn.execute("""
                    UPDATE tasks
                    SET status = 'pending', updated_at = NOW(),
                        metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object(
                            'stuck_reset', true, 'previous_status', 'in_progress'
                        )
                    WHERE status = 'in_progress'
                      AND updated_at < NOW() - make_interval(mins => $1::int)
                """, STUCK_MINUTES)
                if stuck_result and stuck_result.startswith("UPDATE"):
                    n = stuck_result.split()[-1]
                    if n != "0":
                        print(f'[{datetime.now()}] üîÑ –í–µ—Ä–Ω—É—Ç–æ –≤ –æ—á–µ—Ä–µ–¥—å –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–¥–∞—á (>{STUCK_MINUTES} –º–∏–Ω): {n}')
            
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            # BACKPRESSURE: –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ MLX/Ollama –ü–ï–†–ï–î –≤–∑—è—Ç–∏–µ–º –∑–∞–¥–∞—á (SRE, –ï–ª–µ–Ω–∞)
            # –ï—Å–ª–∏ –æ–±–∞ –±—ç–∫–µ–Ω–¥–∞ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω—ã ‚Äî –Ω–µ –±—Ä–∞—Ç—å –Ω–æ–≤—ã–µ –∑–∞–¥–∞—á–∏, –ø–æ–¥–æ–∂–¥–∞—Ç—å
            # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
            if ADAPTIVE_CONCURRENCY:
                try:
                    from adaptive_concurrency import check_backends_overload
                    is_overloaded, overload_reason = await check_backends_overload()
                    if is_overloaded:
                        print(f'[{datetime.now()}] ‚è∏Ô∏è BACKPRESSURE: {overload_reason}. –û–∂–∏–¥–∞–Ω–∏–µ 10 —Å–µ–∫...')
                        await asyncio.sleep(10)
                        continue  # –ù–µ –±—Ä–∞—Ç—å –∑–∞–¥–∞—á–∏, –≤–µ—Ä–Ω—É—Ç—å—Å—è –∫ –Ω–∞—á–∞–ª—É —Ü–∏–∫–ª–∞
                except ImportError:
                    pass  # –§—É–Ω–∫—Ü–∏—è –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞, –ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏
                except Exception as e:
                    logger.debug(f"Backpressure check failed: {e}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LEFT JOIN —á—Ç–æ–±—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∑–∞–¥–∞—á–∏ –¥–∞–∂–µ –µ—Å–ª–∏ —ç–∫—Å–ø–µ—Ä—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
            # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ–º –∑–∞–¥–∞—á–∏ —Å –≤—ã—Å–æ–∫–æ–π bug_probability (Code-Smell Predictor, Singularity 9.0)
            async with pool.acquire() as conn:
                tasks = await conn.fetch("""
                    SELECT t.id, t.title, t.description, t.metadata,
                           COALESCE(e.name, '–í–∏–∫—Ç–æ—Ä–∏—è') as assignee,
                           COALESCE(e.id, (SELECT id FROM experts WHERE name = '–í–∏–∫—Ç–æ—Ä–∏—è' LIMIT 1)) as assignee_expert_id,
                           COALESCE((t.metadata->>'bug_probability')::float, 0.0) as bug_probability
                    FROM tasks t 
                    LEFT JOIN experts e ON t.assignee_expert_id = e.id 
                    WHERE t.status = 'pending'
                      AND (t.metadata->>'next_retry_after' IS NULL OR (t.metadata->>'next_retry_after')::timestamptz < NOW())
                    ORDER BY 
                        COALESCE((t.metadata->>'bug_probability')::float, 0.0) DESC,  -- –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –∑–∞–¥–∞—á–∏ —Å –≤—ã—Å–æ–∫–æ–π bug_probability
                        t.created_at ASC 
                    LIMIT $1
                """, BATCH_SIZE)
            
            if tasks:
                # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º: N –ø–æ CPU/–ø–∞–º—è—Ç–∏ –∏ MLX/Ollama (ADAPTIVE_WORKER_CONCURRENCY_PLAN, SRE/Performance)
                effective_n = MAX_CONCURRENT_TASKS
                adaptive_metrics = {}
                if ADAPTIVE_CONCURRENCY:
                    try:
                        from adaptive_concurrency import get_effective_concurrent
                        effective_n, adaptive_metrics = await get_effective_concurrent(
                            n_max=MAX_CONCURRENT_TASKS, n_min=1
                        )
                        # –õ–æ–≥–∏—Ä—É–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ä–∞–∑ –≤ —Ü–∏–∫–ª (SRE: –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤)
                        print(f'[{datetime.now()}] üìä Adaptive N={effective_n} (max={MAX_CONCURRENT_TASKS}) | '
                              f'host RAM={adaptive_metrics.get("host_ram_percent", "?")}% CPU={adaptive_metrics.get("host_cpu_percent", "?")}% | '
                              f'MLX {adaptive_metrics.get("mlx_active", "?")}/{adaptive_metrics.get("mlx_max", "?")} '
                              f'Ollama active={adaptive_metrics.get("ollama_active", "?")}')
                    except Exception as e:
                        logger.debug("Adaptive concurrency failed, using max: %s", e)
                        effective_n = MAX_CONCURRENT_TASKS
                
                print(f'[{datetime.now()}] Found {len(tasks)} pending tasks. Processing in parallel (max {effective_n} concurrent)...')
                
                # –í–ê–ñ–ù–û: –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º asyncpg Records –≤ —Å–ª–æ–≤–∞—Ä–∏ (Records immutable!)
                tasks = [dict(t) for t in tasks]
                
                # –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï: –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä –Ω–∞–∑–Ω–∞—á–∞–µ—Ç preferred_source –ø—Ä–∏ assign_task_to_best_expert
                # –í–æ—Ä–∫–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç metadata.preferred_source –æ—Ç –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä–∞; –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî fallback –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                mlx_tasks = []
                ollama_tasks = []
                for task in tasks:
                    meta = task.get('metadata') or {}
                    if isinstance(meta, str):
                        try:
                            meta = json.loads(meta) if meta else {}
                        except Exception:
                            meta = {}
                    # –û—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä —É–∂–µ –Ω–∞–∑–Ω–∞—á–∏–ª preferred_source ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º
                    orch_source = meta.get('preferred_source')
                    if orch_source and str(orch_source).lower() in ('mlx', 'ollama'):
                        task['preferred_source'] = str(orch_source).lower()
                        task['_effective_category'] = task.get('_effective_category') or 'default'
                    else:
                        # Fallback: intelligent_model_router –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
                        try:
                            from intelligent_model_router import get_intelligent_router
                            ir = get_intelligent_router()
                            prompt = f"{task.get('title', '')} {task.get('description', '')}"
                            tc = ir.estimate_task_complexity(prompt, category=None)
                            if getattr(tc, 'requires_reasoning', False):
                                task['_effective_category'] = 'reasoning'
                            elif getattr(tc, 'requires_coding', False):
                                task['_effective_category'] = 'coding'
                            elif getattr(tc, 'task_type', '') == 'fast':
                                task['_effective_category'] = 'fast'
                            else:
                                task['_effective_category'] = 'default'
                            if (tc.complexity_score > 0.6 and (tc.requires_reasoning or tc.requires_coding)):
                                task['preferred_source'] = 'mlx'
                            elif tc.complexity_score < 0.4 or getattr(tc, 'task_type', '') == 'fast':
                                task['preferred_source'] = 'ollama'
                            else:
                                task['preferred_source'] = 'mlx' if len(mlx_tasks) <= len(ollama_tasks) else 'ollama'
                        except Exception:
                            task['_effective_category'] = 'default'
                            task['preferred_source'] = 'mlx' if len(mlx_tasks) <= len(ollama_tasks) else 'ollama'
                    if task['preferred_source'] == 'mlx':
                        mlx_tasks.append(task)
                    else:
                        ollama_tasks.append(task)
                
                print(f'[{datetime.now()}] üìä –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: MLX={len(mlx_tasks)}, Ollama={len(ollama_tasks)}')
                
                # –ë–∞—Ç—á–∏ –ø–æ –º–æ–¥–µ–ª–∏: —Å–∫–∞–Ω–µ—Ä –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π ‚Üí –Ω–∞–∑–Ω–∞—á–∏—Ç—å preferred_model ‚Üí –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –ø–æ (source, model) ‚Üí –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –±–ª–æ–∫–∞–º–∏ (–º–µ–Ω—å—à–µ load/unload –Ω–∞ MLX/Ollama)
                BATCH_BY_MODEL = os.getenv('SMART_WORKER_BATCH_BY_MODEL', 'true').lower() in ('true', '1', 'yes')
                use_pairing = os.getenv('SMART_WORKER_HEAVY_LIGHT_PAIRING', 'true').lower() in ('true', '1', 'yes')
                all_tasks_to_process = []
                if BATCH_BY_MODEL:
                    try:
                        from available_models_scanner import get_available_models, pick_mlx_for_category, pick_ollama_for_category
                        mlx_url = os.getenv('MLX_API_URL') or ('http://host.docker.internal:11435' if os.path.exists('/.dockerenv') else 'http://localhost:11435')
                        ollama_url = os.getenv('OLLAMA_API_URL') or os.getenv('OLLAMA_BASE_URL') or ('http://host.docker.internal:11434' if os.path.exists('/.dockerenv') else 'http://localhost:11434')
                        # –ö—ç—à —Å–∫–∞–Ω–µ—Ä–∞ –≤ –≥–ª–∞–≤–Ω–æ–º —Ü–∏–∫–ª–µ (TTL 120 —Å–µ–∫) ‚Äî –º–µ–Ω—å—à–µ –≤—ã–∑–æ–≤–æ–≤ –∫ Ollama/MLX, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –ø—Ä–∏ –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–∞—Ö
                        global _scanner_cache_time, _scanner_cache_mlx, _scanner_cache_ollama
                        import time as _time
                        _t = _time.time()
                        if _t - _scanner_cache_time < 120 and _scanner_cache_mlx is not None:
                            mlx_list = _scanner_cache_mlx
                            ollama_list = _scanner_cache_ollama
                        else:
                            mlx_list, ollama_list = await get_available_models(mlx_url, ollama_url)
                            _scanner_cache_time = _t
                            _scanner_cache_mlx = mlx_list
                            _scanner_cache_ollama = ollama_list
                        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –ø–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–º –≤ —Å–∫–∞–Ω–µ—Ä–µ: –µ—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –ø—É—Å—Ç ‚Äî –ø–µ—Ä–µ–Ω–∞–∑–Ω–∞—á–∞–µ–º –Ω–∞ –¥—Ä—É–≥–æ–π
                        for task in mlx_tasks + ollama_tasks:
                            cat = task.get('_effective_category', 'default')
                            src = task.get('preferred_source', 'ollama')
                            if src == 'mlx' and not mlx_list:
                                task['preferred_source'] = 'ollama'
                                src = 'ollama'
                            elif src == 'ollama' and not ollama_list:
                                task['preferred_source'] = 'mlx'
                                src = 'mlx'
                            if src == 'mlx' and mlx_list:
                                task['preferred_model'] = pick_mlx_for_category(cat, mlx_list)
                            elif src == 'ollama' and ollama_list:
                                task['preferred_model'] = pick_ollama_for_category(cat, ollama_list)
                            else:
                                task['preferred_model'] = None
                        # –¢—è–∂—ë–ª—ã–µ/–ª—ë–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ (ADAPTIVE_WORKER_CONCURRENCY_PLAN): –ª–∏–º–∏—Ç —Ç—è–∂—ë–ª—ã—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
                        try:
                            from adaptive_concurrency import is_model_heavy
                            for task in mlx_tasks + ollama_tasks:
                                task['_is_heavy'] = is_model_heavy(task.get('preferred_model'))
                        except ImportError:
                            for task in mlx_tasks + ollama_tasks:
                                task['_is_heavy'] = False
                        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ (preferred_source, preferred_model)
                        from collections import defaultdict
                        groups = defaultdict(list)
                        for task in mlx_tasks + ollama_tasks:
                            key = (task.get('preferred_source'), task.get('preferred_model') or '')
                            groups[key].append(task)
                        # –¢—è–∂—ë–ª—ã–π/–ª—ë–≥–∫–∏–π pairing (ADAPTIVE_WORKER_CONCURRENCY_PLAN): –∫–æ–≥–¥–∞ Ollama —Ç—è–∂—ë–ª–∞—è ‚Äî MLX –ª—ë–≥–∫–∞—è, –∏ –Ω–∞–æ–±–æ—Ä–æ—Ç
                        # –£–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ–º –±–ª–æ–∫–∏: (mlx/heavy, ollama/light), (mlx/light, ollama/heavy) ‚Äî —á—Ç–æ–±—ã –Ω–µ –≥—Ä—É–∑–∏—Ç—å –æ–±–∞ —Ç—è–∂—ë–ª—ã–º–∏
                        group_list = list(groups.items())
                        _is_heavy_fn = None
                        try:
                            from adaptive_concurrency import is_model_heavy as _is_heavy_fn
                        except ImportError:
                            _is_heavy_fn = lambda m: False
                        if use_pairing and len(group_list) > 1 and _is_heavy_fn:
                            def _heavy(k): return _is_heavy_fn(k[1])
                            mlx_heavy = [(k, v) for k, v in group_list if k[0] == 'mlx' and _heavy(k)]
                            mlx_light = [(k, v) for k, v in group_list if k[0] == 'mlx' and not _heavy(k)]
                            ollama_heavy = [(k, v) for k, v in group_list if k[0] == 'ollama' and _heavy(k)]
                            ollama_light = [(k, v) for k, v in group_list if k[0] == 'ollama' and not _heavy(k)]
                            # –ü–∞—Ä—ã: mlx_heavy+ollama_light, mlx_light+ollama_heavy ‚Äî —á–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ
                            paired = list(mlx_heavy) + list(ollama_light) + list(mlx_light) + list(ollama_heavy)
                            sorted_groups = [g for g in paired if g[1]]
                        else:
                            sorted_groups = sorted(group_list, key=lambda x: -len(x[1]))
                        # –õ–æ–≥ –±–ª–æ–∫–æ–≤
                        if sorted_groups:
                            blocks_desc = ", ".join(f"{src}/{model or 'auto'}:{len(gt)}" for (src, model), gt in sorted_groups)
                            print(f'[{datetime.now()}] üì¶ –ë–ª–æ–∫–∏ (source/–º–æ–¥–µ–ª—å: –∫–æ–ª-–≤–æ): {blocks_desc}')
                        # –ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ: MLX –∏ Ollama –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ; –ø—Ä–∏ pairing ‚Äî —Ç—è–∂—ë–ª—ã–π –Ω–∞ –æ–¥–Ω–æ–º, –ª—ë–≥–∫–∏–π –Ω–∞ –¥—Ä—É–≥–æ–º
                        INTERLEAVE = os.getenv('SMART_WORKER_INTERLEAVE_BLOCKS', 'true').lower() in ('true', '1', 'yes')
                        if INTERLEAVE and len(sorted_groups) > 1:
                            max_len = max(len(gt) for _, gt in sorted_groups)
                            for i in range(max_len):
                                for (_src, _model), group_tasks in sorted_groups:
                                    if i < len(group_tasks):
                                        all_tasks_to_process.append(group_tasks[i])
                            print(f'[{datetime.now()}] üì¶ –ß–µ—Ä–µ–¥–æ–≤–∞–Ω–∏–µ (MLX –∏ Ollama –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ, heavy/light pairing)')
                        else:
                            for (src, model), group_tasks in sorted_groups:
                                all_tasks_to_process.extend(group_tasks)
                    except Exception as e:
                        logger.debug(f"Batch by model failed: {e}, using flat order")
                        all_tasks_to_process = mlx_tasks + ollama_tasks
                else:
                    all_tasks_to_process = mlx_tasks + ollama_tasks
                
                # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ç—è–∂—ë–ª—ã—Ö –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (Performance: –Ω–µ OOM) ‚Äî –¥–æ 2 heavy MLX + 2 heavy Ollama –≤ –ø–µ—Ä–≤—ã—Ö —Å–ª–æ—Ç–∞—Ö
                if ADAPTIVE_CONCURRENCY and all_tasks_to_process:
                    try:
                        max_heavy_mlx = int(os.getenv('ADAPTIVE_MAX_HEAVY_MLX', '2'))
                        max_heavy_ollama = int(os.getenv('ADAPTIVE_MAX_HEAVY_OLLAMA', '2'))
                        heavy_mlx = [t for t in all_tasks_to_process if t.get('preferred_source') == 'mlx' and t.get('_is_heavy')]
                        heavy_ollama = [t for t in all_tasks_to_process if t.get('preferred_source') == 'ollama' and t.get('_is_heavy')]
                        first = list(heavy_mlx[:max_heavy_mlx]) + list(heavy_ollama[:max_heavy_ollama])
                        rest = [t for t in all_tasks_to_process if t not in first]
                        all_tasks_to_process = first + rest
                    except Exception as e:
                        logger.debug(f"Heavy/light reorder failed: {e}")
                
                # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –ø—É–ª: —Å–µ–º–∞—Ñ–æ—Ä –Ω–∞ effective_n
                sem = asyncio.Semaphore(effective_n)
                # –ë–∞—Ç—á –ø–æ batch_group (ARCHITECTURE_IMPROVEMENTS ¬ß2.5): –æ–¥–∏–Ω –≤—ã–∑–æ–≤ LLM –Ω–∞ –≥—Ä—É–ø–ø—É
                BATCH_GROUP_LLM = os.getenv("SMART_WORKER_BATCH_GROUP_LLM", "false").lower() in ("true", "1", "yes")
                work_items = []
                used_in_batch = set()
                if BATCH_GROUP_LLM:
                    from collections import defaultdict
                    batch_groups = defaultdict(list)
                    for t in all_tasks_to_process:
                        meta = t.get("metadata") or {}
                        if isinstance(meta, str):
                            try:
                                meta = json.loads(meta) if meta else {}
                            except Exception:
                                meta = {}
                        bg = meta.get("batch_group")
                        if bg:
                            key = (bg, t.get("preferred_source"), t.get("preferred_model"))
                            batch_groups[key].append(t)
                    max_batch = int(os.getenv("SMART_WORKER_BATCH_GROUP_MAX", "3"))
                    for (bg, src, model), group in batch_groups.items():
                        if len(group) >= 2:
                            for i in range(0, len(group), max_batch):
                                batch = group[i : i + max_batch]
                                work_items.append(("batch", batch))
                                used_in_batch.update(t["id"] for t in batch)
                    for t in all_tasks_to_process:
                        if t["id"] not in used_in_batch:
                            work_items.append(("single", t))
                else:
                    work_items = [("single", t) for t in all_tasks_to_process]

                async def process_work_item(item):
                    kind, payload = item
                    async with sem:
                        if kind == "batch":
                            ok = await process_batch_tasks(pool, payload)
                            if not ok:
                                for t in payload:
                                    await process_task(pool, t)
                        else:
                            await process_task(pool, payload)

                await asyncio.gather(*[process_work_item(w) for w in work_items], return_exceptions=True)
                
                print(f'[{datetime.now()}] ‚úÖ Completed: {len(tasks)} tasks processed')
            else:
                print(f'[{datetime.now()}] No pending tasks found. Waiting...')
            
            await asyncio.sleep(5)  # –£–º–µ–Ω—å—à–∏–ª–∏ –∑–∞–¥–µ—Ä–∂–∫—É, —Ç–∞–∫ –∫–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±—ã—Å—Ç—Ä–µ–µ
        except Exception as e:
            print(f'[{datetime.now()}] Main loop error: {e}')
            import traceback
            traceback.print_exc()
            await asyncio.sleep(30)

if __name__ == '__main__':
    asyncio.run(main())

