"""
File Context Enricher - –ú–∏—Ä–æ–≤—ã–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ –ø–µ—Ä–µ–¥–∞—á–∏ –∫–æ–¥–∞ —ç–∫—Å–ø–µ—Ä—Ç–∞–º

–†–µ–∞–ª–∏–∑—É–µ—Ç:
1. Context Window Management (chunking –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤)
2. Metadata-based file references
3. Selective context injection (—Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞—Å—Ç–∏)
4. Smart file reading —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º

–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö:
- LangChain Document Loaders
- AutoGPT File Context
- GitHub Copilot Context Management
"""

import os
import logging
from typing import Optional, Dict, List, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è context window management
MAX_CONTEXT_LENGTH = 8000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è LLM (–æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–ø–∞—Å)
MAX_FILE_SIZE = 50000  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —á—Ç–µ–Ω–∏—è (50KB)
CHUNK_SIZE = 3000  # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö —Ñ–∞–π–ª–æ–≤
OVERLAP_SIZE = 200  # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

class FileContextEnricher:
    """
    –û–±–æ–≥–∞—â–∞–µ—Ç –∑–∞–¥–∞—á–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ñ–∞–π–ª–æ–≤ –ø–æ –º–∏—Ä–æ–≤—ã–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º.
    """
    
    def __init__(self, base_path: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è enricher.
        
        Args:
            base_path: –ë–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∫ –ø—Ä–æ–µ–∫—Ç—É (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é - –∫–æ—Ä–µ–Ω—å knowledge_os)
        """
        if base_path is None:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –±–∞–∑–æ–≤—ã–π –ø—É—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            current_file = Path(__file__).resolve()
            # knowledge_os/app/file_context_enricher.py -> knowledge_os/
            self.base_path = current_file.parent.parent
        else:
            self.base_path = Path(base_path)
    
    def read_file_safe(self, file_path: str) -> Optional[str]:
        """
        –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫.
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É (–æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –∏–ª–∏ –∞–±—Å–æ–ª—é—Ç–Ω—ã–π)
            
        Returns:
            –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü—Ä–æ–±—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–π –ø—É—Ç—å –æ—Ç base_path
            full_path = self.base_path / file_path
            if not full_path.exists():
                # –ü—Ä–æ–±—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–π –ø—É—Ç—å
                full_path = Path(file_path)
                if not full_path.exists():
                    logger.warning(f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}")
                    return None
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
            file_size = full_path.stat().st_size
            if file_size > MAX_FILE_SIZE * 2:  # –ï—Å–ª–∏ —Ñ–∞–π–ª –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π (>100KB)
                logger.warning(f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({file_size} –±–∞–π—Ç): {file_path}, –∏—Å–ø–æ–ª—å–∑—É–µ–º chunking")
                return self._read_file_chunked(full_path)
            
            # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
            with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            logger.info(f"‚úÖ –§–∞–π–ª –ø—Ä–æ—á–∏—Ç–∞–Ω: {file_path} ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")
            return content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return None
    
    def _read_file_chunked(self, file_path: Path) -> str:
        """
        –ß–∏—Ç–∞–µ—Ç –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª –ø–æ —á–∞—Å—Ç—è–º (chunking).
        
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            
        Returns:
            –ü–µ—Ä–≤—ã–µ —á–∞–Ω–∫–∏ —Ñ–∞–π–ª–∞ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Ä–∞–∑–º–µ—Ä–µ
        """
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                # –ß–∏—Ç–∞–µ–º –ø–µ—Ä–≤—ã–µ —á–∞–Ω–∫–∏
                chunks = []
                total_read = 0
                
                while total_read < MAX_CONTEXT_LENGTH and len(chunks) < 3:  # –ú–∞–∫—Å–∏–º—É–º 3 —á–∞–Ω–∫–∞
                    chunk = f.read(CHUNK_SIZE)
                    if not chunk:
                        break
                    chunks.append(chunk)
                    total_read += len(chunk)
                
                content = '\n\n[...–ø—Ä–æ–ø—É—â–µ–Ω–æ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...]\n\n'.join(chunks)
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ä–∞–∑–º–µ—Ä–µ —Ñ–∞–π–ª–∞
                file_size = file_path.stat().st_size
                header = f"‚ö†Ô∏è –§–ê–ô–õ –ë–û–õ–¨–®–û–ô ({file_size} –±–∞–π—Ç). –ü–æ–∫–∞–∑–∞–Ω—ã –ø–µ—Ä–≤—ã–µ {total_read} —Å–∏–º–≤–æ–ª–æ–≤:\n\n"
                
                return header + content
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ chunking —Ñ–∞–π–ª–∞ {file_path}: {e}")
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}"
    
    def extract_relevant_sections(self, content: str, keywords: List[str]) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ —Ñ–∞–π–ª–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º.
        
        Args:
            content: –°–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞
            
        Returns:
            –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ —Ñ–∞–π–ª–∞
        """
        if not keywords:
            return content
        
        lines = content.split('\n')
        relevant_lines = []
        context_before = 5  # –°—Ç—Ä–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–æ
        context_after = 10  # –°—Ç—Ä–æ–∫ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ
        
        for i, line in enumerate(lines):
            line_lower = line.lower()
            if any(keyword.lower() in line_lower for keyword in keywords):
                # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–æ –∏ –ø–æ—Å–ª–µ
                start = max(0, i - context_before)
                end = min(len(lines), i + context_after + 1)
                relevant_lines.extend(lines[start:end])
                relevant_lines.append("---")  # –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å
        
        if relevant_lines:
            return '\n'.join(relevant_lines)
        else:
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞—á–∞–ª–æ —Ñ–∞–π–ª–∞
            return '\n'.join(lines[:100])
    
    def enrich_task_with_file_context(
        self,
        task_description: str,
        file_path: Optional[str] = None,
        metadata: Optional[Dict] = None,
        keywords: Optional[List[str]] = None
    ) -> str:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º —Ñ–∞–π–ª–∞.
        
        Args:
            task_description: –ò—Å—Ö–æ–¥–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É (–∏–∑ metadata –∏–ª–∏ –Ω–∞–ø—Ä—è–º—É—é)
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏ (–º–æ–∂–µ—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å file_path)
            keywords: –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–µ–∫—Ü–∏–π
            
        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        if not file_path and metadata:
            file_path = metadata.get('file_path') or metadata.get('file')
        
        if not file_path:
            return task_description
        
        # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
        file_content = self.read_file_safe(file_path)
        if not file_content:
            return task_description + f"\n\n‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª: {file_path}"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–µ–∫—Ü–∏–∏ –µ—Å–ª–∏ –µ—Å—Ç—å keywords
        if keywords:
            file_content = self.extract_relevant_sections(file_content, keywords)
        
        # –û–±–æ–≥–∞—â–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ
        enriched = f"""{task_description}

---
üìÅ –ö–û–ù–¢–ï–ö–°–¢ –§–ê–ô–õ–ê: {file_path}
---

```python
{file_content}
```

---
üí° –ò–ù–°–¢–†–£–ö–¶–ò–Ø: –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç –∫–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è. –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–¥–µ!
"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        if len(enriched) > MAX_CONTEXT_LENGTH:
            logger.warning(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({len(enriched)} —Å–∏–º–≤–æ–ª–æ–≤), –æ–±—Ä–µ–∑–∞–µ–º")
            # –û–±—Ä–µ–∑–∞–µ–º –¥–æ MAX_CONTEXT_LENGTH
            enriched = enriched[:MAX_CONTEXT_LENGTH] + "\n\n[...–∫–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏...]"
        
        return enriched
    
    def enrich_task_with_multiple_files(
        self,
        task_description: str,
        file_paths: List[str],
        metadata: Optional[Dict] = None
    ) -> str:
        """
        –û–±–æ–≥–∞—â–∞–µ—Ç –∑–∞–¥–∞—á—É –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤.
        
        Args:
            task_description: –ò—Å—Ö–æ–¥–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            metadata: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–¥–∞—á–∏
            
        Returns:
            –û–±–æ–≥–∞—â–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–¥–∞—á–∏
        """
        enriched = task_description + "\n\n---\nüìÅ –ö–û–ù–¢–ï–ö–°–¢ –§–ê–ô–õ–û–í:\n---\n\n"
        
        total_length = len(enriched)
        
        for file_path in file_paths[:3]:  # –ú–∞–∫—Å–∏–º—É–º 3 —Ñ–∞–π–ª–∞
            file_content = self.read_file_safe(file_path)
            if file_content:
                file_section = f"### {file_path}\n\n```python\n{file_content}\n```\n\n"
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø—Ä–µ–≤—ã—Å–∏–º –ª–∏ –ª–∏–º–∏—Ç
                if total_length + len(file_section) > MAX_CONTEXT_LENGTH:
                    enriched += f"\n\n[...–æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–ø—É—â–µ–Ω—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞...]"
                    break
                
                enriched += file_section
                total_length += len(file_section)
        
        return enriched

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –¥—Ä—É–≥–∏—Ö –º–æ–¥—É–ª—è—Ö
_enricher = None

def get_file_enricher() -> FileContextEnricher:
    """–ü–æ–ª—É—á–∏—Ç—å –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä FileContextEnricher."""
    global _enricher
    if _enricher is None:
        _enricher = FileContextEnricher()
    return _enricher
