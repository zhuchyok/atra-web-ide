"""
Enhanced Monitoring System for Knowledge OS
–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ VDS –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –∞–ª–µ—Ä—Ç–∞–º–∏
"""

import asyncio
import os
import psutil
import asyncpg
import httpx
from datetime import datetime
from typing import Dict, List
import json
import logging

logger = logging.getLogger(__name__)

# Tunnel manager –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ SSH tunnel
try:
    from tunnel_manager import get_tunnel_manager
except ImportError:
    get_tunnel_manager = None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')
TG_TOKEN = os.getenv('TG_TOKEN', '8422371257:AAEwgSCvSv637QqDsi-EAayVYj8dsENsLbU')
CHAT_ID = os.getenv('CHAT_ID', '556251171')
LOG_PATH = "/root/knowledge_os/logs/monitor.log"

# –ü–æ—Ä–æ–≥–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –∞–ª–µ—Ä—Ç–æ–≤
THRESHOLDS = {
    'cpu_percent': 85.0,
    'ram_percent': 85.0,
    'disk_percent': 90.0,
    'db_connections': 80,  # –∏–∑ max_size=20
    'response_time_ms': 1000.0,
}

async def send_telegram_alert(message: str, priority: str = "medium"):
    """–û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–∞ –≤ Telegram"""
    emoji = "üî¥" if priority == "high" else "üü°" if priority == "medium" else "üü¢"
    url = f"https://api.telegram.org/bot{TG_TOKEN}/sendMessage"
    async with httpx.AsyncClient() as client:
        try:
            await client.post(
                url,
                data={
                    'chat_id': CHAT_ID,
                    'text': f"{emoji} *KNOWLEDGE OS MONITOR*\n\n{message}",
                    'parse_mode': 'Markdown'
                },
                timeout=10.0
            )
        except Exception as e:
            print(f"Failed to send Telegram alert: {e}")

def log_message(message: str):
    """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_entry = f"[{timestamp}] {message}\n"
    with open(LOG_PATH, "a") as f:
        f.write(log_entry)
    print(log_entry.strip())

async def get_system_metrics() -> Dict:
    """–°–±–æ—Ä —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫"""
    cpu_percent = psutil.cpu_percent(interval=1)
    ram = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    return {
        'timestamp': datetime.now().isoformat(),
        'cpu': {
            'percent': cpu_percent,
            'count': psutil.cpu_count(),
        },
        'ram': {
            'total_gb': round(ram.total / (1024**3), 2),
            'used_gb': round(ram.used / (1024**3), 2),
            'available_gb': round(ram.available / (1024**3), 2),
            'percent': ram.percent,
        },
        'disk': {
            'total_gb': round(disk.total / (1024**3), 2),
            'used_gb': round(disk.used / (1024**3), 2),
            'free_gb': round(disk.free / (1024**3), 2),
            'percent': round((disk.used / disk.total) * 100, 2),
        },
    }

async def get_database_metrics() -> Dict:
    """–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    try:
        conn = await asyncpg.connect(DB_URL)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
        active_connections = await conn.fetchval(
            "SELECT count(*) FROM pg_stat_activity WHERE datname = 'knowledge_os'"
        )
        
        # –†–∞–∑–º–µ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        db_size = await conn.fetchval(
            "SELECT pg_database_size('knowledge_os')"
        )
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–∑–ª–æ–≤ –∑–Ω–∞–Ω–∏–π
        knowledge_nodes_count = await conn.fetchval(
            "SELECT count(*) FROM knowledge_nodes"
        )
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤
        experts_count = await conn.fetchval(
            "SELECT count(*) FROM experts"
        )
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        last_activity = await conn.fetchval(
            "SELECT max(created_at) FROM knowledge_nodes"
        )
        
        await conn.close()
        
        return {
            'active_connections': active_connections,
            'db_size_gb': round(db_size / (1024**3), 2),
            'knowledge_nodes': knowledge_nodes_count,
            'experts': experts_count,
            'last_activity': last_activity.isoformat() if last_activity else None,
        }
    except Exception as e:
        log_message(f"‚ùå Error getting DB metrics: {e}")
        return {}

async def get_api_health() -> Dict:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è API"""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            start_time = datetime.now()
            response = await client.get("http://localhost:8000/health", timeout=5.0)
            response_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return {
                'status': 'healthy' if response.status_code == 200 else 'unhealthy',
                'status_code': response.status_code,
                'response_time_ms': round(response_time, 2),
            }
    except Exception as e:
        return {
            'status': 'unreachable',
            'error': str(e),
        }

async def get_ab_test_metrics() -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ML-—Ä–æ—É—Ç–∏–Ω–≥–∞"""
    try:
        from ml_router_ab_test import get_ab_test
        ab_test = await get_ab_test()
        stats = await ab_test.get_ab_test_statistics(days=7)
        return stats
    except Exception as e:
        logger.error(f"Error getting AB test metrics: {e}")
        return {
            'ml': {'count': 0, 'avg_performance': 0, 'avg_tokens_saved': 0, 'success_rate': 0},
            'heuristic': {'count': 0, 'avg_performance': 0, 'avg_tokens_saved': 0, 'success_rate': 0}
        }

async def get_adaptive_learning_metrics() -> Dict:
    """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    try:
        from adaptive_learner import AdaptiveLearner
        from feedback_collector import get_feedback_collector
        
        learner = AdaptiveLearner()
        collector = await get_feedback_collector()
        
        feedback_stats = await collector.get_feedback_statistics(days=7)
        
        return {
            'feedback_total': feedback_stats.get('total', 0),
            'feedback_positive_rate': feedback_stats.get('positive_rate', 0),
            'reroute_rate': feedback_stats.get('reroute_rate', 0),
            'improvement_trend': 'analyzing'  # –ë—É–¥–µ—Ç –≤—ã—á–∏—Å–ª—è—Ç—å—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏–∏
        }
    except Exception as e:
        logger.error(f"Error getting adaptive learning metrics: {e}")
        return {
            'feedback_total': 0,
            'feedback_positive_rate': 0,
            'reroute_rate': 0,
            'improvement_trend': 'unknown'
        }

async def get_routing_metrics() -> Dict:
    """–°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ —Ä–æ—É—Ç–∏–Ω–≥–∞ –∏–∑ semantic_ai_cache"""
    try:
        conn = await asyncpg.connect(DB_URL)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –º–µ—Ç—Ä–∏–∫ —Ä–æ—É—Ç–∏–Ω–≥–∞
        columns_exist = await conn.fetchval("""
            SELECT COUNT(*) FROM information_schema.columns 
            WHERE table_name = 'semantic_ai_cache' 
            AND column_name IN ('routing_source', 'performance_score', 'tokens_saved')
        """) == 3
        
        if not columns_exist:
            await conn.close()
            return {}
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º —Ä–æ—É—Ç–∏–Ω–≥–∞ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        routing_stats = await conn.fetch("""
            SELECT 
                routing_source,
                COUNT(*) as count,
                AVG(performance_score) as avg_performance,
                SUM(tokens_saved) as total_tokens_saved,
                AVG(tokens_saved) as avg_tokens_saved
            FROM semantic_ai_cache
            WHERE routing_source IS NOT NULL
            AND last_used_at > NOW() - INTERVAL '24 hours'
            GROUP BY routing_source
        """)
        
        # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ —Å–µ–≥–æ–¥–Ω—è
        today_stats = await conn.fetchrow("""
            SELECT 
                COUNT(*) as total_requests,
                SUM(tokens_saved) as total_tokens_saved_today,
                AVG(performance_score) as avg_performance_today
            FROM semantic_ai_cache
            WHERE routing_source IS NOT NULL
            AND last_used_at > CURRENT_DATE
        """)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É–∑–ª–∞–º (Mac vs Server)
        node_stats = {}
        for stat in routing_stats:
            source = stat['routing_source']
            if source:
                node_stats[source] = {
                    'count': stat['count'],
                    'avg_performance': round(float(stat['avg_performance'] or 0), 3),
                    'total_tokens_saved': stat['total_tokens_saved'] or 0,
                    'avg_tokens_saved': round(float(stat['avg_tokens_saved'] or 0), 0),
                }
        
        await conn.close()
        
        return {
            'nodes': node_stats,
            'today': {
                'total_requests': today_stats['total_requests'] or 0,
                'total_tokens_saved': today_stats['total_tokens_saved'] or 0,
                'avg_performance': round(float(today_stats['avg_performance'] or 0), 3),
            },
            'timestamp': datetime.now().isoformat(),
        }
    except Exception as e:
        log_message(f"‚ùå Error getting routing metrics: {e}")
        return {}

async def check_thresholds(metrics: Dict) -> List[Dict]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∞–ª–µ—Ä—Ç–æ–≤"""
    alerts = []
    
    # CPU
    if metrics['system']['cpu']['percent'] > THRESHOLDS['cpu_percent']:
        alerts.append({
            'priority': 'high',
            'metric': 'CPU',
            'value': f"{metrics['system']['cpu']['percent']}%",
            'threshold': f"{THRESHOLDS['cpu_percent']}%",
            'message': f"‚ö†Ô∏è –í—ã—Å–æ–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ CPU: {metrics['system']['cpu']['percent']}%"
        })
    
    # RAM
    if metrics['system']['ram']['percent'] > THRESHOLDS['ram_percent']:
        alerts.append({
            'priority': 'high',
            'metric': 'RAM',
            'value': f"{metrics['system']['ram']['percent']}%",
            'threshold': f"{THRESHOLDS['ram_percent']}%",
            'message': f"‚ö†Ô∏è –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAM: {metrics['system']['ram']['percent']}% ({metrics['system']['ram']['used_gb']}GB / {metrics['system']['ram']['total_gb']}GB)"
        })
    
    # Disk
    if metrics['system']['disk']['percent'] > THRESHOLDS['disk_percent']:
        alerts.append({
            'priority': 'high',
            'metric': 'Disk',
            'value': f"{metrics['system']['disk']['percent']}%",
            'threshold': f"{THRESHOLDS['disk_percent']}%",
            'message': f"‚ö†Ô∏è –ú–∞–ª–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ: {metrics['system']['disk']['percent']}% ({metrics['system']['disk']['used_gb']}GB / {metrics['system']['disk']['total_gb']}GB)"
        })
    
    # Database connections
    if metrics.get('database', {}).get('active_connections', 0) > THRESHOLDS['db_connections']:
        alerts.append({
            'priority': 'medium',
            'metric': 'DB Connections',
            'value': metrics['database']['active_connections'],
            'threshold': THRESHOLDS['db_connections'],
            'message': f"‚ö†Ô∏è –ú–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π –∫ –ë–î: {metrics['database']['active_connections']}"
        })
    
    # API response time
    if metrics.get('api', {}).get('response_time_ms', 0) > THRESHOLDS['response_time_ms']:
        alerts.append({
            'priority': 'medium',
            'metric': 'API Response Time',
            'value': f"{metrics['api']['response_time_ms']}ms",
            'threshold': f"{THRESHOLDS['response_time_ms']}ms",
            'message': f"‚ö†Ô∏è –ú–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç API: {metrics['api']['response_time_ms']}ms"
        })
    
    # API unreachable
    if metrics.get('api', {}).get('status') == 'unreachable':
        alerts.append({
            'priority': 'high',
            'metric': 'API Status',
            'value': 'unreachable',
            'threshold': 'healthy',
            'message': f"‚ùå API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {metrics['api'].get('error', 'Unknown error')}"
        })
    
    return alerts

async def save_metrics_to_db(metrics: Dict):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –≤ –ë–î –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏"""
    try:
        conn = await asyncpg.connect(DB_URL)
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –Ω–µ—Ç
        await conn.execute("""
            CREATE TABLE IF NOT EXISTS system_metrics (
                id SERIAL PRIMARY KEY,
                timestamp TIMESTAMP DEFAULT NOW(),
                metrics JSONB NOT NULL
            )
        """)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        await conn.execute("""
            INSERT INTO system_metrics (metrics)
            VALUES ($1)
        """, json.dumps(metrics))
        
        # –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–æ—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π)
        await conn.execute("""
            DELETE FROM system_metrics
            WHERE timestamp < NOW() - INTERVAL '7 days'
        """)
        
        await conn.close()
        log_message("‚úÖ Metrics saved to database")
    except Exception as e:
        log_message(f"‚ùå Error saving metrics to DB: {e}")

async def run_adaptive_learning_cycle():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ü–∏–∫–ª –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    try:
        from adaptive_learner import run_adaptive_learning_cycle
        updated, deleted = await run_adaptive_learning_cycle()
        logger.info(f"‚úÖ [ADAPTIVE LEARNING] Cycle completed: {updated} updated, {deleted} deleted")
        return {"updated": updated, "deleted": deleted}
    except ImportError:
        logger.warning("‚ö†Ô∏è AdaptiveLearner not available")
        return {"updated": 0, "deleted": 0}
    except Exception as e:
        logger.error(f"‚ùå [ADAPTIVE LEARNING] Error: {e}")
        return {"updated": 0, "deleted": 0}

async def run_monitoring_cycle():
    """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
    log_message("üîç Starting monitoring cycle...")
    
    # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫
    system_metrics = await get_system_metrics()
    db_metrics = await get_database_metrics()
    api_health = await get_api_health()
    routing_metrics = await get_routing_metrics()
    
    metrics = {
        'system': system_metrics,
        'database': db_metrics,
        'api': api_health,
        'routing': routing_metrics,
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    await save_metrics_to_db(metrics)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Ä–æ–≥–æ–≤
    alerts = await check_thresholds(metrics)
    
    # Self-Healing: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —É–∑–ª–æ–≤
    try:
        from self_healing import SelfHealingManager
        manager = SelfHealingManager()
        nodes = [
            {"name": "MacBook (Normal)", "url": os.getenv('MAC_LLM_URL', 'http://localhost:11434')},
            {"name": "Server (Light)", "url": os.getenv('SERVER_LLM_URL', 'http://localhost:11434')}
        ]
        healed_nodes = await manager.check_and_heal(nodes)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∞–ª–µ—Ä—Ç—ã –¥–ª—è –æ—Ñ—Ñ–ª–∞–π–Ω —É–∑–ª–æ–≤
        for node in healed_nodes:
            if node.get('status') == 'offline' and not node.get('healed'):
                alerts.append({
                    'priority': 'high',
                    'metric': 'Node Status',
                    'value': 'offline',
                    'threshold': 'online',
                    'message': f"‚ùå –£–∑–µ–ª {node['name']} –æ—Ñ—Ñ–ª–∞–π–Ω"
                })
    except Exception as e:
        log_message(f"‚ö†Ô∏è Self-healing check failed: {e}")
    
    # –°–±–æ—Ä SLA –º–µ—Ç—Ä–∏–∫ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —á–µ—Ä–µ–∑ sla_monitor)
    # –ü–µ—Ä–µ–Ω–µ—Å–µ–Ω–æ –≤ —Å–µ–∫—Ü–∏—é "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ SLA/SLO" –Ω–∏–∂–µ
    
    # –û—Ç–ø—Ä–∞–≤–∫–∞ –∞–ª–µ—Ä—Ç–æ–≤
    if alerts:
        for alert in alerts:
            await send_telegram_alert(alert['message'], alert['priority'])
            log_message(f"üö® Alert sent: {alert['message']}")
    else:
        log_message("‚úÖ All metrics within thresholds")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
    log_message(f"üìä CPU: {system_metrics['cpu']['percent']}% | RAM: {system_metrics['ram']['percent']}% | Disk: {system_metrics['disk']['percent']}%")
    if db_metrics:
        log_message(f"üìä DB: {db_metrics.get('knowledge_nodes', 0)} nodes | {db_metrics.get('experts', 0)} experts | {db_metrics.get('db_size_gb', 0)}GB")
    if api_health:
        log_message(f"üìä API: {api_health.get('status', 'unknown')} | {api_health.get('response_time_ms', 0)}ms")
    if routing_metrics:
        today = routing_metrics.get('today', {})
        log_message(f"üìä Routing: {today.get('total_requests', 0)} requests | {today.get('total_tokens_saved', 0)} tokens saved | Performance: {today.get('avg_performance', 0):.2f}")
        nodes = routing_metrics.get('nodes', {})
        for node_name, node_data in nodes.items():
            log_message(f"   ‚îî‚îÄ {node_name}: {node_data.get('count', 0)} requests, {node_data.get('total_tokens_saved', 0)} tokens saved")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ SSH tunnel
    if get_tunnel_manager:
        try:
            tunnel_manager = get_tunnel_manager()
            if tunnel_manager:
                tunnel_status = tunnel_manager.check_tunnel()
                if tunnel_status:
                    log_message(f"‚úÖ SSH Tunnel: –∞–∫—Ç–∏–≤–µ–Ω (–ø–æ—Ä—Ç {tunnel_manager.tunnel_port})")
                else:
                    log_message(f"‚ö†Ô∏è SSH Tunnel: –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–µ—Ä–µ—Å–æ–∑–¥–∞—é...")
                    tunnel_manager.create_tunnel()
        except Exception as e:
            logger.debug(f"Tunnel monitoring failed: {e}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ SLA/SLO
    try:
        from sla_monitor import get_sla_monitor
        from telegram_alerter import get_telegram_alerter
        
        sla_monitor = get_sla_monitor()
        sla_compliance = await sla_monitor.check_sla_compliance()
        
        violations = []
        for metric_name, metric_data in sla_compliance.items():
            if not metric_data.get("compliant", True):
                value = metric_data.get('value', 0)
                target = metric_data.get('target', 0)
                unit = metric_data.get('unit', '')
                violations.append(f"{metric_name}: {value:.3f}{unit} (target: {target:.3f}{unit})")
        
        if violations:
            alert_msg = f"üö® SLA Violations detected:\n" + "\n".join(f"  ‚Ä¢ {v}" for v in violations)
            log_message(alert_msg)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —á–µ—Ä–µ–∑ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π Telegram Alerter
            alerter = get_telegram_alerter()
            await alerter.send_alert(alert_msg, priority="high", source="SLA Monitor")
        else:
            log_message("‚úÖ –í—Å–µ SLA –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            for metric_name, metric_data in sla_compliance.items():
                value = metric_data.get('value', 0)
                target = metric_data.get('target', 0)
                unit = metric_data.get('unit', '')
                log_message(f"   ‚Ä¢ {metric_name}: {value:.3f}{unit} (target: {target:.3f}{unit}) ‚úÖ")
    except Exception as e:
        logger.debug(f"SLA monitoring failed: {e}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Disaster Recovery
    try:
        from disaster_recovery import get_disaster_recovery
        disaster_recovery = get_disaster_recovery()
        mode_info = disaster_recovery.get_mode_info()
        
        if mode_info["mode"] != "normal":
            alert_msg = f"üîÑ –†–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã: {mode_info['mode']}"
            log_message(alert_msg)
            await send_telegram_alert(alert_msg, "medium")
    except Exception as e:
        logger.debug(f"Disaster recovery monitoring failed: {e}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª–µ–π
    try:
        from model_memory_manager import get_memory_manager
        import os
        server_url = os.getenv('SERVER_LLM_URL', 'http://localhost:11434')
        memory_manager = get_memory_manager(server_url)
        memory_stats = await memory_manager.get_memory_stats()
        
        available_mb = memory_stats.get("available_memory_mb", 0)
        actual_memory_usage = memory_stats.get("actual_memory_usage_mb", {})
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫—É—é –Ω–µ—Ö–≤–∞—Ç–∫—É –ø–∞–º—è—Ç–∏
        if available_mb < 200:
            alert_msg = f"‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ: {available_mb}MB"
            log_message(alert_msg)
            await send_telegram_alert(alert_msg, "high")
        
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª—è–º–∏
        if actual_memory_usage:
            log_message(f"üìä –†–µ–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –º–æ–¥–µ–ª—è–º–∏:")
            for model_name, memory_mb in actual_memory_usage.items():
                log_message(f"   ‚Ä¢ {model_name}: {memory_mb:.2f} MB")
                
                # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–∞–º—è—Ç–∏
                if memory_mb > 1000:  # –ë–æ–ª—å—à–µ 1GB
                    alert_msg = f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –∏—Å–ø–æ–ª—å–∑—É–µ—Ç {memory_mb:.2f}MB –ø–∞–º—è—Ç–∏"
                    await send_telegram_alert(alert_msg, "medium")
    except Exception as e:
        logger.debug(f"Memory monitoring failed: {e}")
    
    # Adaptive Learning: –∑–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (–µ–∂–µ–¥–Ω–µ–≤–Ω–æ)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –∑–∞–ø—É—Å–∫–∞—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ä–∞–∑ –≤ –¥–µ–Ω—å)
    try:
        adaptive_result = await run_adaptive_learning_cycle()
        if adaptive_result.get('updated', 0) > 0 or adaptive_result.get('deleted', 0) > 0:
            log_message(f"üîÑ [ADAPTIVE LEARNING] Updated {adaptive_result.get('updated', 0)} examples, deleted {adaptive_result.get('deleted', 0)}")
    except Exception as e:
        logger.debug(f"Adaptive learning cycle failed: {e}")
    
    # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Circuit Breaker —Å–æ–±—ã—Ç–∏–π
    try:
        conn = await asyncpg.connect(DB_URL)
        try:
            # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–∫—Ä—ã—Ç—ã–µ circuit breakers –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å
            open_breakers = await conn.fetch("""
                SELECT breaker_name, COUNT(*) as event_count, MAX(created_at) as last_event
                FROM circuit_breaker_events
                WHERE new_state = 'open' AND created_at > NOW() - INTERVAL '1 hour'
                GROUP BY breaker_name
            """)
            
            if open_breakers:
                for breaker in open_breakers:
                    alert_msg = (
                        f"‚ö†Ô∏è Circuit Breaker '{breaker['breaker_name']}' –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ OPEN. "
                        f"–°–æ–±—ã—Ç–∏–π –∑–∞ —á–∞—Å: {breaker['event_count']}. "
                        f"–ü–æ—Å–ª–µ–¥–Ω–µ–µ: {breaker['last_event']}"
                    )
                    log_message(alert_msg)
                    await send_telegram_alert(alert_msg, "high")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º circuit breakers
            breaker_stats = await conn.fetch("""
                SELECT 
                    breaker_name,
                    COUNT(*) FILTER (WHERE event_type = 'state_change' AND new_state = 'open') as open_count,
                    COUNT(*) FILTER (WHERE event_type = 'success') as success_count,
                    COUNT(*) FILTER (WHERE event_type = 'failure') as failure_count
                FROM circuit_breaker_events
                WHERE created_at > NOW() - INTERVAL '24 hours'
                GROUP BY breaker_name
            """)
            
            if breaker_stats:
                log_message("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Circuit Breaker –∑–∞ 24 —á–∞—Å–∞:")
                for stat in breaker_stats:
                    log_message(
                        f"   ‚Ä¢ {stat['breaker_name']}: "
                        f"OPEN={stat['open_count']}, "
                        f"SUCCESS={stat['success_count']}, "
                        f"FAILURE={stat['failure_count']}"
                    )
        finally:
            await conn.close()
    except Exception as e:
        logger.debug(f"Circuit breaker monitoring failed: {e}")
    
    # –°–±–æ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    try:
        from metrics_collector import get_metrics_collector
        metrics_collector = get_metrics_collector()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        if system_metrics:
            cpu_percent = system_metrics.get('cpu', {}).get('percent', 0)
            ram_percent = system_metrics.get('ram', {}).get('percent', 0)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ (–º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
            # await metrics_collector.collect_temperature(cpu_temp, 'cpu', 'system')
        
        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        await metrics_collector.flush()
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        tokens_stats = await metrics_collector.get_metrics_stats("tokens_per_second", hours=24)
        if tokens_stats.get('count', 0) > 0:
            log_message(
                f"üìä –¢–æ–∫–µ–Ω–æ–≤/—Å–µ–∫ (24—á): avg={tokens_stats.get('avg', 0):.2f}, "
                f"min={tokens_stats.get('min', 0):.2f}, max={tokens_stats.get('max', 0):.2f}"
            )
    except Exception as e:
        logger.debug(f"Metrics collection failed: {e}")
    
    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –∞–≤—Ç–æ–Ω–æ–º–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ Singularity 7.5
    try:
        # Auto Model Manager - –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        from auto_model_manager import get_auto_model_manager
        auto_model_mgr = get_auto_model_manager()
        if not auto_model_mgr._running:
            auto_model_mgr.start_monitoring()
            log_message("üîÑ Auto Model Manager –∑–∞–ø—É—â–µ–Ω")
    except Exception as e:
        logger.debug(f"Auto Model Manager integration failed: {e}")
    
    try:
        # Auto Backup Manager - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –±—ç–∫–∞–ø—ã
        from auto_backup_manager import get_auto_backup_manager
        backup_mgr = get_auto_backup_manager()
        if not backup_mgr._running:
            backup_mgr.start_monitoring()
            log_message("üíæ Auto Backup Manager –∑–∞–ø—É—â–µ–Ω")
    except Exception as e:
        logger.debug(f"Auto Backup Manager integration failed: {e}")
    
    try:
        # Anomaly Detector - –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π
        from anomaly_detector import get_anomaly_detector
        anomaly_detector = get_anomaly_detector()
        anomaly_stats = await anomaly_detector.get_anomaly_stats(hours=24)
        if anomaly_stats:
            log_message("üö® –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π –∑–∞ 24 —á–∞—Å–∞:")
            for key, count in anomaly_stats.items():
                log_message(f"   ‚Ä¢ {key}: {count}")
    except Exception as e:
        logger.debug(f"Anomaly Detector integration failed: {e}")
    
    # Predictive Cache Warming - –∞–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–æ–≤
    try:
        from optimizers import PredictiveCache
        from semantic_cache import SemanticAICache
        
        cache_manager = SemanticAICache()
        pred_cache = PredictiveCache(cache_manager, db_url=DB_URL)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞
        patterns = await pred_cache.analyze_query_history(hours=24)
        
        if patterns:
            log_message("üìä Predictive Cache: –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∑–∞ 24 —á–∞—Å–∞:")
            
            # –¢–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            if patterns.get("keywords"):
                top_keywords = sorted(patterns["keywords"].items(), key=lambda x: x[1], reverse=True)[:5]
                log_message(f"   üîë –¢–æ–ø –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: {', '.join([f'{kw}({count})' for kw, count in top_keywords])}")
            
            # –¢–æ–ø –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if patterns.get("sequences"):
                top_sequences = sorted(patterns["sequences"].items(), key=lambda x: x[1], reverse=True)[:3]
                log_message(f"   üîó –¢–æ–ø –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {', '.join([f'{seq}({count})' for seq, count in top_sequences])}")
            
            # –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
            if patterns.get("categories"):
                top_categories = sorted(patterns["categories"].items(), key=lambda x: x[1], reverse=True)[:3]
                log_message(f"   üìÇ –¢–æ–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: {', '.join([f'{cat}({count})' for cat, count in top_categories])}")
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            if patterns.get("temporal"):
                top_temporal = sorted(patterns["temporal"].items(), key=lambda x: x[1], reverse=True)[:3]
                log_message(f"   üïê –¢–æ–ø –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {', '.join([f'{time}({count})' for time, count in top_temporal])}")
        else:
            log_message("üìä Predictive Cache: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    except Exception as e:
        logger.debug(f"Predictive Cache integration failed: {e}")
    
    try:
        # Model Validator - –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π (—Ä–∞–∑ –≤ –¥–µ–Ω—å)
        from datetime import datetime
        current_hour = datetime.now().hour
        if current_hour == 2:  # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ 2:00 –Ω–æ—á–∏
            from model_validator import get_model_validator
            validator = get_model_validator()
            validation_results = await validator.validate_all_models()
            if validation_results:
                passed_count = sum(1 for r in validation_results if r.passed)
                log_message(f"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π: {passed_count}/{len(validation_results)} –ø—Ä–æ—à–ª–∏")
    except Exception as e:
        logger.debug(f"Model Validator integration failed: {e}")
    
    try:
        # Auto Prompt Optimizer - –∞–Ω–∞–ª–∏–∑ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —É–ª—É—á—à–µ–Ω–∏–π (—Ä–∞–∑ –≤ –¥–µ–Ω—å)
        if current_hour == 3:  # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ 3:00 –Ω–æ—á–∏
            from auto_prompt_optimizer import get_auto_prompt_optimizer
            optimizer = get_auto_prompt_optimizer()
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–ø—Ä–∏–º–µ—Ä)
            current_prompt = "–¢—ã - –í–∏–∫—Ç–æ—Ä–∏—è, Team Lead –∫–æ–º–∞–Ω–¥—ã —ç–∫—Å–ø–µ—Ä—Ç–æ–≤..."
            improvements = await optimizer.suggest_improvements(current_prompt, "–í–∏–∫—Ç–æ—Ä–∏—è")
            if improvements:
                log_message(f"üí° –ù–∞–π–¥–µ–Ω–æ {len(improvements)} –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –ø—Ä–æ–º–ø—Ç–æ–≤")
                for imp in improvements[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-3
                    log_message(f"   ‚Ä¢ {imp.improvement_reason} (impact: {imp.expected_impact})")
                    # –õ–æ–≥–∏—Ä—É–µ–º —É–ª—É—á—à–µ–Ω–∏–µ –≤ –ë–î
                    await optimizer.log_improvement(imp, "–í–∏–∫—Ç–æ—Ä–∏—è", applied=False)
    except Exception as e:
        logger.debug(f"Auto Prompt Optimizer integration failed: {e}")
    
    log_message("‚úÖ Monitoring cycle completed")

if __name__ == "__main__":
    asyncio.run(run_monitoring_cycle())

