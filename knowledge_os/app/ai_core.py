"""
[SINGULARITY CORE] AI Agent Coordination Module.
Handles caching, routing, knowledge retrieval (RAG), and consensus across agents.
Optimized for Hybrid Intelligence (Cloud Architect + Local Worker).
"""

import asyncio
import os
import logging
import getpass
import json
import time
from typing import Optional, List, Dict, Any

# Third-party imports with fallbacks
try:
    import asyncpg  # type: ignore
except ImportError:
    asyncpg = None  # type: ignore

try:
    import nest_asyncio  # type: ignore
except ImportError:
    nest_asyncio = None  # type: ignore

# Local project imports with fallbacks
try:
    from semantic_cache import SemanticAICache, get_embedding  # type: ignore
except ImportError:
    SemanticAICache = None  # type: ignore
    async def get_embedding(text: str) -> Optional[List[float]]: return None

try:
    from local_router import LocalAIRouter  # type: ignore
except ImportError:
    LocalAIRouter = None  # type: ignore

try:
    from distillation_engine import KnowledgeDistiller  # type: ignore
except ImportError:
    KnowledgeDistiller = None  # type: ignore

try:
    from context_compressor import ContextCompressor  # type: ignore
except ImportError:
    class ContextCompressor:
        @staticmethod
        def compress_all(prompt: str) -> str: return prompt

try:
    from safety_checker import SafetyChecker  # type: ignore
except ImportError:
    SafetyChecker = None  # type: ignore

try:
    from veronica_web_researcher import VeronicaWebResearcher  # type: ignore
except ImportError:
    VeronicaWebResearcher = None  # type: ignore

try:
    from optimizers import PromptOptimizer, EmbeddingCache, PredictiveCache, FrugalPrompt, BETokenManager, get_betoken_manager  # type: ignore
except ImportError:
    PromptOptimizer = None  # type: ignore
    EmbeddingCache = None  # type: ignore
    PredictiveCache = None  # type: ignore
    FrugalPrompt = None  # type: ignore
    BETokenManager = None  # type: ignore
    get_betoken_manager = None  # type: ignore

try:
    from parallel_request_processor import ParallelRequestProcessor, RequestSource, get_parallel_processor  # type: ignore
except ImportError:
    ParallelRequestProcessor = None  # type: ignore
    RequestSource = None  # type: ignore
    get_parallel_processor = None  # type: ignore

try:
    from quality_assurance import QualityAssurance, QualityGate  # type: ignore
except ImportError:
    QualityAssurance = None  # type: ignore
    QualityGate = None  # type: ignore

try:
    from ml_router_data_collector import get_collector  # type: ignore
except ImportError:
    get_collector = None  # type: ignore

try:
    from batch_processor import get_batch_processor  # type: ignore
except ImportError:
    get_batch_processor = None  # type: ignore

try:
    from optimizers import ParallelProcessor  # type: ignore
except ImportError:
    ParallelProcessor = None  # type: ignore

try:
    from query_orchestrator import QueryOrchestrator, QueryType  # type: ignore
    from prompt_templates import get_prompt_template, format_prompt  # type: ignore
except ImportError:
    QueryOrchestrator = None  # type: ignore
    QueryType = None  # type: ignore
    get_prompt_template = None  # type: ignore
    format_prompt = None  # type: ignore

try:
    from feedback_collector import get_feedback_collector  # type: ignore
except ImportError:
    get_feedback_collector = None  # type: ignore

try:
    from ml_router_v2 import get_ml_router_v2  # type: ignore
except ImportError:
    get_ml_router_v2 = None  # type: ignore

try:
    from session_context_manager import get_session_context_manager  # type: ignore
except ImportError:
    get_session_context_manager = None  # type: ignore

try:
    from context_analyzer import ContextAnalyzer  # type: ignore
except ImportError:
    ContextAnalyzer = None  # type: ignore

try:
    from vision_processor import get_vision_processor  # type: ignore
except ImportError:
    get_vision_processor = None  # type: ignore

try:
    from circuit_breaker import get_circuit_breaker, CircuitBreakerOpenError  # type: ignore
except ImportError:
    get_circuit_breaker = None  # type: ignore
    CircuitBreakerOpenError = Exception

try:
    from disaster_recovery import get_disaster_recovery, SystemMode  # type: ignore
except ImportError:
    get_disaster_recovery = None  # type: ignore
    SystemMode = None

try:
    from tacit_knowledge_miner import TacitKnowledgeMiner  # type: ignore
except ImportError:
    TacitKnowledgeMiner = None  # type: ignore

try:
    from emotion_detector import EmotionDetector  # type: ignore
except ImportError:
    EmotionDetector = None  # type: ignore

logger = logging.getLogger(__name__)

# Global user identification for conditional logic
USER_NAME = getpass.getuser()

# --- PERFORMANCE BOOST: DB CONNECTION POOLING ---
_DB_POOL = None

async def _get_db_pool():
    """Lazy initialization of the PostgreSQL connection pool."""
    global _DB_POOL
    if _DB_POOL is None and asyncpg:
        try:
            default_url = os.getenv('DATABASE_URL') or 'postgresql://admin:secret@localhost:5432/knowledge_os'
            db_url = os.getenv('DATABASE_URL_LOCAL', default_url)
            _DB_POOL = await asyncpg.create_pool(
                db_url, 
                min_size=1, 
                max_size=5,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ –ë–î
                max_inactive_connection_lifetime=300
            )
        except Exception as exc:
            logger.error("‚ùå Failed to create DB pool: %s", exc)
    return _DB_POOL

async def _run_cloud_agent_async(prompt: str):
    """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Ollama/MLX) ‚Üí cursor-agent. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–µ—Ä–≤—ã–º–∏."""
    # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Ollama/MLX) ‚Äî –ø–æ–ª–∏—Ç–∏–∫–∞ –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏
    if LocalAIRouter:
        try:
            router = LocalAIRouter()
            result = await router.run_local_llm(prompt, category="general")
            if isinstance(result, tuple):
                response, _ = result
            else:
                response = result
            if response and len(response) > 10:
                logger.info("‚úÖ [LOCAL FIRST] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (Ollama/MLX) –≤–º–µ—Å—Ç–æ –æ–±–ª–∞–∫–∞")
                return response
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [LOCAL FIRST] –õ–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}, –ø—Ä–æ–±—É–µ–º cursor-agent")
    
    # –ü–†–ò–û–†–ò–¢–ï–¢ 2: cursor-agent (–æ–±–ª–∞–∫–æ) ‚Äî —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
    try:
        env = os.environ.copy()
        agent_path = 'cursor-agent'
        process = await asyncio.create_subprocess_exec(
            agent_path, '--print', prompt,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            env=env
        )
        try:
            # –£–º–µ–Ω—å—à–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–æ 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ fallback
            stdout, stderr = await asyncio.wait_for(process.communicate(), timeout=30)
            if process.returncode == 0:
                return stdout.decode().strip()
            return f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –æ–±–ª–∞—á–Ω–æ–≥–æ –º–æ–∑–≥–∞: {stderr.decode()[:100]}"
        except asyncio.TimeoutError:
            process.kill()
            logger.warning("‚è±Ô∏è [CLOUD TIMEOUT] –û–±–ª–∞—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Ç–∞–π–º–∞—É—Ç–∏–ª—Å—è (30s), –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏")
            # –ü—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ –æ–±–ª–∞–∫–∞ –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
            if LocalAIRouter:
                try:
                    router = LocalAIRouter()
                    # –ë—ã—Å—Ç—Ä—ã–π fallback –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Å —Ç–∞–π–º–∞—É—Ç–æ–º 15 —Å–µ–∫—É–Ω–¥
                    result = await asyncio.wait_for(
                        router.run_local_llm(prompt, category="general"),
                        timeout=15
                    )
                    if isinstance(result, tuple):
                        response, _ = result
                    else:
                        response = result
                    if response and len(response) > 10:
                        logger.info("‚úÖ [TIMEOUT FALLBACK] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ —Ç–∞–π–º–∞—É—Ç–∞ –æ–±–ª–∞–∫–∞")
                        return response
                except asyncio.TimeoutError:
                    logger.warning("‚ö†Ô∏è [TIMEOUT FALLBACK] –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ —Ç–∞–π–º–∞—É—Ç—è—Ç—Å—è (15s)")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [TIMEOUT FALLBACK] –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
            return "‚åõ –û–±–ª–∞—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∑–∞–Ω—è–ª —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏. –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ —Ç–∞–∫–∂–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã."
    except FileNotFoundError:
        # üçé –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å MLX (Apple Neural Engine) –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ MacBook
        try:
            from knowledge_os.app.mlx_router import get_mlx_router, is_mlx_available
            if is_mlx_available():
                mlx_router = get_mlx_router()
                logger.info("üçé [MLX] –ü—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Apple MLX (Neural Engine) –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏")
                mlx_response = await mlx_router.generate_response(
                    prompt=prompt,
                    max_tokens=512,
                    temperature=0.7
                )
                if mlx_response and len(mlx_response) > 10:
                    logger.info("‚úÖ [MLX] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω Apple MLX (Neural Engine) - –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ MacBook —Å–Ω–∏–∂–µ–Ω–∞")
                    return mlx_response
                else:
                    logger.debug("‚ö†Ô∏è [MLX] MLX –Ω–µ –≤–µ—Ä–Ω—É–ª –æ—Ç–≤–µ—Ç, –ø—Ä–æ–±—É–µ–º Ollama")
        except ImportError:
            logger.debug("‚ö†Ô∏è MLX Router –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–±—É–µ–º Ollama")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [MLX] –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ MLX: {e}, –ø—Ä–æ–±—É–µ–º Ollama")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: cursor-agent not found - use direct Ollama call as fallback
        logger.warning("‚ö†Ô∏è cursor-agent not found, using direct Ollama API")
        try:
            import aiohttp
            async with aiohttp.ClientSession() as session:
                # Try server first (phi3 available), then MacBook if accessible
                # Note: localhost on server = server itself, not MacBook
                # MacBook would need to be accessible via network IP (not implemented yet)
                for ollama_url in ["http://localhost:11434"]:
                    try:
                        # MacBook: use better models (deepseek-r1, qwen2.5-coder)
                        # Server: use lightweight models (phi3, phi4) for low RAM
                        if "localhost" in ollama_url or "127.0.0.1" in ollama_url:
                            # MacBook - –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏
                            # MLX –º–æ–¥–µ–ª–∏ (Mac Studio): qwen2.5-coder:32b, deepseek-r1-distill-llama:70b
                            # Ollama –º–æ–¥–µ–ª–∏: glm-4.7-flash:q8_0, phi3.5:3.8b
                            models_to_try = ["deepseek-r1-distill-llama:70b", "qwen2.5-coder:32b", "glm-4.7-flash:q8_0", "phi3.5:3.8b"]
                        else:
                            # Server - –ª–µ–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ (1.9GB RAM)
                            models_to_try = ["phi3:latest", "phi3", "phi4:latest", "phi4", "tinyllama", "gemma:2b"]
                        
                        response = None
                        model_used = None
                        
                        for model_name in models_to_try:
                            try:
                                async with session.post(
                                    f"{ollama_url}/api/generate",
                                    json={
                                        "model": model_name,
                                        "prompt": prompt,
                                        "stream": False
                                    },
                                    timeout=aiohttp.ClientTimeout(total=120)
                                ) as resp:
                                    if resp.status == 200:
                                        data = await resp.json()
                                        response = data.get("response", "")
                                        if response and len(response) > 10:
                                            model_used = model_name
                                            break
                            except Exception as e:
                                logger.debug(f"Model {model_name} at {ollama_url} failed: {e}")
                                continue
                        
                        if response and model_used:
                            logger.info(f"‚úÖ [FALLBACK] Used Ollama at {ollama_url} with {model_used}")
                            return response
                        else:
                            async with session.post(
                                f"{ollama_url}/api/generate",
                                json={
                                    "model": model_used,
                                    "prompt": prompt,
                                    "stream": False
                                },
                                timeout=aiohttp.ClientTimeout(total=120)
                            ) as resp:
                                if resp.status == 200:
                                    data = await resp.json()
                                    response = data.get("response", "")
                                    if response:
                                        logger.info(f"‚úÖ [FALLBACK] Used Ollama at {ollama_url} with {model_used}")
                                        return response
                            if resp.status == 200:
                                data = await resp.json()
                                response = data.get("response", "")
                                if response:
                                    logger.info(f"‚úÖ [FALLBACK] Used Ollama at {ollama_url}")
                                    return response
                    except Exception as e:
                        logger.debug(f"Ollama at {ollama_url} failed: {e}")
                        continue
        except ImportError:
            logger.warning("aiohttp not available for Ollama fallback")
        except Exception as e:
            logger.warning(f"Ollama fallback failed: {e}")
        
        # Final fallback: return a helpful message
        return f"‚ö†Ô∏è –í—Å–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã. –ó–∞–ø—Ä–æ—Å: {prompt[:100]}..."
    except Exception as exc:
        return f"‚ùå –û—à–∏–±–∫–∞ —Å–≤—è–∑–∏ —Å –æ–±–ª–∞–∫–æ–º: {exc}"

async def _get_knowledge_context(query: str) -> str:
    """Retrieve relevant knowledge nodes (RAG) - –≤–∫–ª—é—á–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏."""
    try:
        embedding = await get_embedding(query)
        if not embedding: return ""
        pool = await _get_db_pool()
        if not pool: return ""
        async with pool.acquire() as conn:
            # –ò—â–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –∑–Ω–∞–Ω–∏—è, –≤–∫–ª—é—á–∞—è –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏
            rows = await conn.fetch("""
                SELECT content, metadata, (1 - (embedding <=> $1::vector)) as similarity
                FROM knowledge_nodes
                WHERE embedding IS NOT NULL
                AND confidence_score >= 0.3
                ORDER BY similarity DESC LIMIT 5
            """, embedding)
        if not rows: return ""
        context = "\nüìö [KNOWLEDGE CONTEXT]:\n"
        for row in rows:
            if row['similarity'] >= 0.6:  # –ü–æ–Ω–∏–∑–∏–ª–∏ –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
                metadata = row['metadata'] or {}
                source = metadata.get('source', 'unknown')
                knowledge_type = metadata.get('type', 'general')
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–ø–µ –∑–Ω–∞–Ω–∏—è
                if source == 'corporation_knowledge_system':
                    context += f"\n[–ö–û–†–ü–û–†–ê–¶–ò–Ø: {knowledge_type}] (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {row['similarity']:.2f}):\n"
                else:
                    context += f"\n[–ó–ù–ê–ù–ò–ï] (—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {row['similarity']:.2f}):\n"
                context += f"{row['content']}\n"
        return context
    except Exception as exc:
        logger.error("Knowledge retrieval error: %s", exc)
        return ""

async def run_smart_agent_async(
    prompt: str,
    expert_name: str = "–í–∏–∫—Ç–æ—Ä–∏—è",
    category: Optional[str] = None,
    require_cot: bool = False,
    is_critical: bool = False,
    images: Optional[list] = None,
    session_id: Optional[str] = None
):
    """
    Hybrid Intelligence Orchestrator.
    Victoria (Cloud) generates the plan, Local Worker (DeepSeek/Qwen) executes.
    """
    import time
    start_time = time.time()
    request_id = f"{expert_name}_{int(time.time())}"
    
    # 0. Anomaly Detection: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª–∏–∏
    try:
        from anomaly_detector import get_anomaly_detector
        anomaly_detector = get_anomaly_detector()
        should_block, alert = await anomaly_detector.analyze_request(
            prompt,
            identifier=request_id,
            metadata={"expert_name": expert_name, "category": category}
        )
        if should_block:
            logger.warning(f"üö® [ANOMALY DETECTOR] –ó–∞–ø—Ä–æ—Å –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {alert.description if alert else 'unknown'}")
            return "‚ö†Ô∏è –ó–∞–ø—Ä–æ—Å –æ—Ç–∫–ª–æ–Ω–µ–Ω —Å–∏—Å—Ç–µ–º–æ–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏."
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É
        if anomaly_detector.is_blocked(request_id):
            logger.warning(f"üö® [ANOMALY DETECTOR] –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω: {request_id}")
            return "‚ö†Ô∏è –î–æ—Å—Ç—É–ø –≤—Ä–µ–º–µ–Ω–Ω–æ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    except Exception as e:
        logger.debug(f"Anomaly detection failed: {e}")
    
    # 0.1. Disaster Recovery: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã
    disaster_recovery = None
    if get_disaster_recovery:
        disaster_recovery = get_disaster_recovery()
        await disaster_recovery.run_health_check()
        
        # –ï—Å–ª–∏ —Å–∏—Å—Ç–µ–º–∞ –≤ —Ä–µ–∂–∏–º–µ OFFLINE, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É
        if disaster_recovery.get_current_mode() == SystemMode.OFFLINE:
            logger.error("üö® [DISASTER RECOVERY] –°–∏—Å—Ç–µ–º–∞ –≤ —Ä–µ–∂–∏–º–µ OFFLINE")
            return "‚ö†Ô∏è –°–∏—Å—Ç–µ–º–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
    
    # 1. Initialization (–∫—ç—à –≤ —Ç–æ–π –∂–µ –ë–î, —á—Ç–æ –¥–∞—à–±–æ—Ä–¥/SLA ‚Äî DATABASE_URL)
    cache = SemanticAICache(db_url=os.getenv("DATABASE_URL")) if SemanticAICache else None
    router = LocalAIRouter() if LocalAIRouter else None
    distiller = KnowledgeDistiller() if KnowledgeDistiller else None
    qa = QualityAssurance(min_quality_threshold=0.7) if QualityAssurance else None
    quality_gate = QualityGate(qa) if QualityGate and qa else None
    parallel_processor = ParallelProcessor(max_concurrent=3) if ParallelProcessor else None
    
    # ML Router v2 –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–æ—É—Ç–∏–Ω–≥–∞ (Singularity 8.0)
    ml_router_v2 = get_ml_router_v2() if get_ml_router_v2 else None
    predicted_route = None
    route_confidence = 0.0
    
    # Circuit breakers –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    db_breaker = get_circuit_breaker("database", failure_threshold=5, recovery_timeout=60) if get_circuit_breaker else None
    local_breaker = get_circuit_breaker("local_models", failure_threshold=3, recovery_timeout=30) if get_circuit_breaker else None
    cloud_breaker = get_circuit_breaker("cloud", failure_threshold=3, recovery_timeout=30) if get_circuit_breaker else None
    
    user_part = prompt.split("–ó–∞–ø—Ä–æ—Å:")[-1].strip() if "–ó–∞–ø—Ä–æ—Å:" in prompt else prompt
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ Discovery ‚Üí MASTER_PLAN ‚Üí –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü–∏—è
    is_strategy_request = False
    if QueryOrchestrator and not session_id:
        try:
            temp_orch = QueryOrchestrator()
            query_type = temp_orch.classify_query(user_part)
            is_strategy_request = query_type == QueryType.STRATEGY
        except Exception:
            pass
    
    # –ï—Å–ª–∏ —ç—Ç–æ –∑–∞–ø—Ä–æ—Å –Ω–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∏ –Ω–µ—Ç session_id, —Å–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –∏ –∑–∞–ø—É—Å–∫–∞–µ–º Discovery
    if is_strategy_request and not session_id:
        try:
            from strategy_session_manager import StrategySessionManager
            from strategy_discovery import StrategyDiscovery
            
            session_manager = StrategySessionManager()
            new_session_id = session_manager.create_session(
                title=user_part[:100],  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ –Ω–∞–∑–≤–∞–Ω–∏–µ
                description=user_part
            )
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º Discovery —Ñ–∞–∑—É
            discovery = StrategyDiscovery(session_manager, temp_orch)
            question_ids = await discovery.start_discovery(new_session_id, user_part)
            
            if question_ids:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                conn = session_manager._get_connection()
                cursor = conn.cursor()
                questions_text_parts = []
                for i, qid in enumerate(question_ids):
                    cursor.execute("SELECT question_text FROM strategy_questions WHERE id = ?", (qid,))
                    row = cursor.fetchone()
                    if row:
                        questions_text_parts.append(f"‚ùì –í–æ–ø—Ä–æ—Å {i+1}: {row['question_text']}")
                conn.close()
                
                questions_text = "\n".join(questions_text_parts)
                return f"üìã Discovery —Ñ–∞–∑–∞ –Ω–∞—á–∞—Ç–∞ –¥–ª—è —Å–µ—Å—Å–∏–∏ {new_session_id}.\n\n{questions_text}\n\n–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è."
            
            # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ—Ç, —Å—Ä–∞–∑—É –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é
            if discovery.is_ready_for_planning(new_session_id):
                from master_plan_generator import MasterPlanGenerator
                from plan_decomposer import PlanDecomposer
                
                generator = MasterPlanGenerator(session_manager=session_manager, query_orch=temp_orch)
                plan_id = await generator.generate_master_plan(new_session_id)
                
                if plan_id:
                    decomposer = PlanDecomposer(session_manager=session_manager, query_orch=temp_orch)
                    await decomposer.decompose_master_plan(new_session_id)
                    
                    return f"‚úÖ MASTER_PLAN —Å–æ–∑–¥–∞–Ω –∏ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Å–µ—Å—Å–∏–∏ {new_session_id}. –ü–ª–∞–Ω ID: {plan_id}"
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [ITERATIVE PLANNING] –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –æ–±—ã—á–Ω—ã–π –ø—É—Ç—å
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—å)
    if images and get_vision_processor:
        vision_processor = get_vision_processor()
        image_analysis = await vision_processor.describe_image(image_base64=images[0] if isinstance(images[0], str) else None)
        if image_analysis:
            user_part = f"–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {image_analysis}\n\n–ó–∞–ø—Ä–æ—Å: {user_part}"
            logger.info("üñºÔ∏è [VISION] Image analyzed locally (0 tokens)")
    
    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤ (—Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –∫–∞—á–µ—Å—Ç–≤–∞)
    original_user_part = user_part  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    
    # –®–∞–≥ 1: BE-Token –∑–∞–º–µ–Ω–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞)
    if get_betoken_manager:
        try:
            betoken_manager = get_betoken_manager()
            user_part, token_used = betoken_manager.replace_with_token(user_part)
            if token_used:
                logger.info(f"üéØ [BE-TOKEN] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ç–æ–∫–µ–Ω: {token_used}")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [BE-TOKEN] –û—à–∏–±–∫–∞: {e}")
    
    # –®–∞–≥ 2: FrugalPrompt —Å–∂–∞—Ç–∏–µ (—É–ª—É—á—à–µ–Ω–Ω–∞—è —Ç–µ—Ö–Ω–∏–∫–∞)
    if FrugalPrompt:
        try:
            frugal_compressed = FrugalPrompt.compress(user_part, max_length=2000, aggressive=True)
            if len(frugal_compressed) < len(user_part):
                logger.info(f"üí∞ [FRUGAL PROMPT] –°–∂–∞—Ç–æ —Å {len(user_part)} –¥–æ {len(frugal_compressed)} —Å–∏–º–≤–æ–ª–æ–≤")
                user_part = frugal_compressed
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [FRUGAL PROMPT] –û—à–∏–±–∫–∞: {e}")
    
    # –®–∞–≥ 3: Fallback –Ω–∞ PromptOptimizer (–µ—Å–ª–∏ FrugalPrompt –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
    if PromptOptimizer and user_part == original_user_part:
        optimizer = PromptOptimizer()
        optimized_part = optimizer.remove_redundancy(user_part)
        optimized_part = optimizer.compress_prompt(optimized_part, max_length=2000)
        user_part = optimized_part
    
    # Quality Gate: –ø—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–Ω–∏–∑–∏–ª–æ –ª–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–æ
    if quality_gate and len(user_part) < len(original_user_part) * 0.5:
        # –ï—Å–ª–∏ —Å–∂–∞–ª–∏ –±–æ–ª–µ–µ —á–µ–º –≤ 2 —Ä–∞–∑–∞, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        if len(user_part) > 100:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–º—ã—Å–ª–∞
            logger.info(f"‚úÖ [QUALITY GATE] –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∞: {len(original_user_part)} -> {len(user_part)} —Å–∏–º–≤–æ–ª–æ–≤")
        else:
            logger.warning("‚ö†Ô∏è [QUALITY GATE] Prompt optimization too aggressive, using original")
            user_part = original_user_part

    # 1.5. Tacit Knowledge Extractor: –ø–æ–ª—É—á–∞–µ–º —Å—Ç–∏–ª–µ–≤–æ–π –ø—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (Singularity 9.0)
    style_profile = None
    style_modifier = ""
    user_identifier = session_id or "default_user"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º session_id –∫–∞–∫ user_identifier –∏–ª–∏ –¥–µ—Ñ–æ–ª—Ç
    style_similarity_score = 0.0
    
    # 1.6. Emotional Response Modulation: –¥–µ—Ç–µ–∫—Ç–∏—Ä—É–µ–º —ç–º–æ—Ü–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (Singularity 9.0)
    emotion_result = None
    emotion_modifier = ""
    
    if EmotionDetector:
        try:
            detector = EmotionDetector()
            emotion_result = await detector.detect_emotion_with_history(user_part, user_identifier)
            
            if emotion_result and emotion_result.confidence >= 0.5:  # MIN_EMOTION_CONFIDENCE = 0.5
                emotion_modifier = detector.create_style_modifier(emotion_result)
                logger.info(f"üòä [EMOTION DETECTOR] Detected emotion: {emotion_result.detected_emotion} (confidence: {emotion_result.confidence:.2f})")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [EMOTION DETECTOR] Error detecting emotion: {e}")
            emotion_result = None
    
    if TacitKnowledgeMiner and is_coding_task:
        try:
            miner = TacitKnowledgeMiner()
            style_profile = await miner.get_style_profile(user_identifier)
            
            if style_profile and style_profile.preferences:
                # –§–æ—Ä–º–∏—Ä—É–µ–º –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø—Ä–æ–º–ø—Ç–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∏–ª–µ–≤—ã—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π
                prefs = style_profile.preferences
                style_modifier = f"""
–°–¢–ò–õ–ï–í–´–ï –ü–†–ï–î–ü–û–ß–¢–ï–ù–ò–Ø –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø:
- –ö–æ–Ω–≤–µ–Ω—Ü–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è: {prefs.get('naming_convention', 'snake_case')}
- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫: {prefs.get('error_handling', 'defensive_with_exceptions')}
- –°—Ç–∏–ª—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {prefs.get('testing_style', 'tdd_with_pytest')}
- –°—Ç–∏–ª—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏: {prefs.get('documentation_style', 'detailed_docstrings')}
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–æ–¥–∞: {prefs.get('code_structure', 'functional')}
- –ò–º–µ–Ω–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö: {prefs.get('variable_naming', 'descriptive_names')}
- –°—Ç–∏–ª—å —Ñ—É–Ω–∫—Ü–∏–π: {prefs.get('function_style', 'simple')}

–í–ê–ñ–ù–û: –ì–µ–Ω–µ—Ä–∏—Ä—É–π –∫–æ–¥ —Å—Ç—Ä–æ–≥–æ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —ç—Ç–∏–º–∏ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º–∏.
"""
                logger.info(f"üé® [TACIT KNOWLEDGE] Style profile loaded for user {user_identifier}")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [TACIT KNOWLEDGE] Error loading style profile: {e}")
            style_profile = None

    # 1.6. –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏ (–¥–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫—ç—à–∞, —Ç–∞–∫ –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Tacit Knowledge)
    is_coding_task = any(kw in user_part.lower() for kw in ["–∫–æ–¥", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–π", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥", "—Ç–µ—Å—Ç", "–∞—É–¥–∏—Ç", "–ø—Ä–æ–≤–µ—Ä—å"])

    # 2. Cache Check (—É–ª—É—á—à–µ–Ω–Ω—ã–π) - —á–µ—Ä–µ–∑ circuit breaker
    if cache and not images:
        try:
            if db_breaker:
                cached = await db_breaker.call(cache.get_cached_response, user_part, expert_name)
            else:
                cached = await cache.get_cached_response(user_part, expert_name)
            
            if cached:
                logger.info("üöÄ [CACHE HIT] %s", expert_name)
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ: –ø—Ä–µ–¥-–≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
                if PredictiveCache:
                    pred_cache = PredictiveCache(cache)
                    await pred_cache.predict_and_cache(user_part, expert_name)
                
                return cached
        except CircuitBreakerOpenError as e:
            logger.warning(f"‚ö†Ô∏è [CIRCUIT BREAKER] Cache –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∫—ç—à–∞
        except Exception as e:
            logger.debug(f"Cache check failed: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –∫—ç—à–∞

    # 3. Hybrid Strategy: Manager-Worker Pattern
    # If the task is coding or audit, we use Victoria to plan and Local to execute
    
    # Track token savings
    tokens_saved = 0
    
    if is_coding_task and not is_critical:
        logger.info("üë©‚Äçüíº [ORCHESTRATOR MODE] Victoria is planning for Local Worker...")
        
        # Phase 1: Victoria generates a TECHNICAL SPECIFICATION (short cloud call)
        spec_prompt = f"""
        –í—ã - –í–∏–∫—Ç–æ—Ä–∏—è, Team Lead. –°–æ—Å—Ç–∞–≤—å—Ç–µ –∫—Ä–∞—Ç–∫–æ–µ –¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –ó–ê–î–ê–ù–ò–ï (–¢–ó) –¥–ª—è –º–ª–∞–¥—à–µ–≥–æ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ 
        –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –£–∫–∞–∂–∏—Ç–µ —Ç–æ–ª—å–∫–æ –ß–¢–û —Å–¥–µ–ª–∞—Ç—å, –±–µ–∑ –Ω–∞–ø–∏—Å–∞–Ω–∏—è —Å–∞–º–æ–≥–æ –∫–æ–¥–∞.
        
        {style_modifier}
        {emotion_modifier}
        
        –ó–ê–ü–†–û–°: {user_part}
        """
        spec = await _run_cloud_agent_async(spec_prompt)
        
        if spec and not spec.startswith(('‚ùå', '‚ö†Ô∏è')):
            # Phase 2: Local Worker executes the spec
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ disaster recovery
            if disaster_recovery and not disaster_recovery.can_use_local_models():
                logger.warning("‚ö†Ô∏è [DISASTER RECOVERY] –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–ª–∞–∫–æ")
                local_result = None
            else:
                # Inject few-shot examples from distillation engine
                examples = ""
                if distiller:
                    try:
                        if db_breaker:
                            examples = await db_breaker.call(distiller.get_relevant_examples, user_part, category or "coding")
                        else:
                            examples = await distiller.get_relevant_examples(user_part, category or "coding")
                    except CircuitBreakerOpenError:
                        logger.warning("‚ö†Ô∏è [CIRCUIT BREAKER] Distillation engine –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ –ø—Ä–∏–º–µ—Ä–æ–≤")
                        examples = ""
                
                worker_prompt = f"{examples}\n\n{style_modifier}\n{emotion_modifier}\n\n–¢–ó –û–¢ –¢–ò–ú–õ–ò–î–ê:\n{spec}\n\n–í–´–ü–û–õ–ù–ò–¢–ï –ó–ê–î–ê–ù–ò–ï:"
                logger.info("üë∑ [WORKER START] Executing TS locally...")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ ML vs —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
                if router and hasattr(router, 'ml_model') and router.ml_model:
                    logger.info("ü§ñ [ML ROUTER] Using ML-based routing")
                else:
                    logger.info("üìä [HEURISTIC ROUTER] Using heuristic routing")
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º circuit breaker –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
                try:
                    if local_breaker and router:
                        local_result = await local_breaker.call(router.run_local_llm, worker_prompt, category="coding")
                    elif router:
                        local_result = await router.run_local_llm(worker_prompt, category="coding")
                    else:
                        local_result = None
                except CircuitBreakerOpenError as e:
                    logger.warning(f"‚ö†Ô∏è [CIRCUIT BREAKER] –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
                    local_result = None
            local_resp, routing_source = local_result if isinstance(local_result, tuple) else (local_result, None)
            
            # Quality Assurance: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–∞
            if local_resp and qa:
                is_acceptable, metrics, recommendation = await qa.validate_response(
                    local_resp, user_part, response_type="code", source="local"
                )
                
                if not is_acceptable:
                    logger.warning(f"‚ö†Ô∏è [QUALITY CHECK] Local response quality {metrics.overall_score:.2f} below threshold")
                    
                    # –°–æ–±–∏—Ä–∞–µ–º feedback –æ –Ω–∏–∑–∫–æ–º –∫–∞—á–µ—Å—Ç–≤–µ
                    if get_feedback_collector:
                        collector = await get_feedback_collector()
                        await collector.collect_implicit_feedback(
                            query=user_part,
                            response=local_resp,
                            routing_source=routing_source or "local",
                            rerouted_to_cloud=True,
                            reroute_reason="low_quality",
                            quality_score=metrics.overall_score
                        )
                    
                    if recommendation == "reroute_to_cloud":
                        logger.warning("üîÑ [QUALITY GATE] Rerouting to cloud due to low quality")
                        local_resp = None  # Force cloud fallback
                    elif recommendation == "retry_local":
                        logger.info("üîÑ [QUALITY GATE] Retrying with local model...")
                        # –ú–æ–∂–Ω–æ –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –µ—â–µ —Ä–∞–∑ —Å –¥—Ä—É–≥–∏–º –ø—Ä–æ–º–ø—Ç–æ–º
                        # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –≤ –æ–±–ª–∞–∫–æ
                        local_resp = None
            
            # Safety check for local response (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)
            if local_resp and SafetyChecker:
                checker = SafetyChecker()
                if checker.should_reroute_to_cloud(local_resp, response_type="code"):
                    logger.warning("üõ°Ô∏è [SAFETY CHECK] Local response failed safety check, rerouting to cloud")
                    
                    # –°–æ–±–∏—Ä–∞–µ–º feedback –æ failed safety check
                    if get_feedback_collector:
                        collector = await get_feedback_collector()
                        await collector.collect_implicit_feedback(
                            query=user_part,
                            response=local_resp,
                            routing_source=routing_source or "local",
                            rerouted_to_cloud=True,
                            reroute_reason="safety_check_failed"
                        )
                    
                    local_resp = None  # Force cloud fallback
            
            # Fallback to cloud if local model failed or safety check failed
            if not local_resp:
                logger.warning("‚ö†Ô∏è [LOCAL FAILED] Local model returned None, falling back to cloud...")
                # Use cloud for execution if local failed
                local_resp = await _run_cloud_agent_async(worker_prompt)
                if local_resp and not local_resp.startswith(('‚ùå', '‚ö†Ô∏è')):
                    logger.info("‚úÖ [CLOUD FALLBACK] Cloud executed the task successfully")
                    if cache: await cache.save_to_cache(user_part, local_resp, expert_name)
                    return local_resp
            
            if local_resp:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è ML-–æ–±—É—á–µ–Ω–∏—è
                quality_metrics = None
                if qa:
                    _, quality_metrics, _ = await qa.validate_response(
                        local_resp, user_part, response_type="code", source="local"
                    )
                
                # Phase 3: Victoria validates the result (Short audit)
                audit_prompt = f"""
                –í—ã - –í–∏–∫—Ç–æ—Ä–∏—è, Team Lead. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ–¥, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã–π —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º. 
                –ï—Å–ª–∏ –≤ –∫–æ–¥–µ –µ—Å—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –Ω–∞–ø–∏—à–∏—Ç–µ –ü–õ–ê–ù –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø. 
                –ï—Å–ª–∏ –∫–æ–¥ –æ—Ç–ª–∏—á–Ω—ã–π, –Ω–∞–ø–∏—à–∏—Ç–µ 'APPROVED'.
                
                –ö–û–î –†–ê–ó–†–ê–ë–û–¢–ß–ò–ö–ê:
                {local_resp}
                """
                audit_result = await _run_cloud_agent_async(audit_prompt)
                
            if audit_result and "APPROVED" in audit_result:
                # Estimate token savings (local execution vs full cloud)
                estimated_cloud_tokens = len(user_part) // 4 + len(local_resp) // 4
                estimated_local_tokens = len(spec) // 4 + len(audit_result) // 4  # Only planning + audit
                tokens_saved = estimated_cloud_tokens - estimated_local_tokens
                logger.info(f"‚úÖ [AUDIT PASSED] Code approved by Victoria. üí∞ Tokens saved: ~{tokens_saved}")
                
                # Tacit Knowledge: –≤—ã—á–∏—Å–ª—è–µ–º style_similarity_score (Singularity 9.0)
                if TacitKnowledgeMiner and style_profile and local_resp:
                    try:
                        miner = TacitKnowledgeMiner()
                        style_similarity_score = await miner.calculate_style_similarity(local_resp, user_identifier)
                        logger.info(f"üé® [TACIT KNOWLEDGE] Style similarity: {style_similarity_score:.2f}")
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è [TACIT KNOWLEDGE] Error calculating similarity: {e}")
                        style_similarity_score = 0.0
                
                # Use routing_source from router, fallback to "local" if not available
                final_routing_source = routing_source or "local"
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à —á–µ—Ä–µ–∑ circuit breaker (–µ—Å–ª–∏ –ë–î –¥–æ—Å—Ç—É–ø–Ω–∞)
                if cache and disaster_recovery and disaster_recovery.can_write_to_db():
                    try:
                        if db_breaker:
                            await db_breaker.call(
                                cache.save_to_cache,
                                user_part, local_resp, expert_name,
                                routing_source=final_routing_source,
                                performance_score=1.0,  # Approved = high score
                                tokens_saved=tokens_saved
                            )
                        else:
                            await cache.save_to_cache(
                                user_part, local_resp, expert_name,
                                routing_source=final_routing_source,
                                performance_score=1.0,  # Approved = high score
                                tokens_saved=tokens_saved
                            )
                    except Exception as e:
                        logger.debug(f"Cache save failed: {e}")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è ML-–æ–±—É—á–µ–Ω–∏—è
                if get_collector:
                    try:
                        collector = await get_collector()
                        await collector.collect_routing_decision(
                            task_type="coding",
                            prompt_length=len(user_part),
                            category="coding",
                            selected_route=final_routing_source,
                            performance_score=1.0,  # Approved
                            tokens_saved=tokens_saved,
                            quality_score=quality_metrics.overall_score if quality_metrics else None,
                            success=True,
                            features={
                                "expert_name": expert_name,
                                "final_approved": True
                            }
                        )
                    except CircuitBreakerOpenError:
                        logger.warning("‚ö†Ô∏è [CIRCUIT BREAKER] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –∫—ç—à, –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                elif cache and disaster_recovery:
                    logger.debug("‚ö†Ô∏è [DISASTER RECOVERY] –ë–î –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –∑–∞–ø–∏—Å–∏, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à")
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è ML-–æ–±—É—á–µ–Ω–∏—è
                if get_collector:
                    collector = await get_collector()
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º routing_source –µ—Å–ª–∏ –Ω–µ –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω
                    actual_routing_source = routing_source or "local"
                    await collector.collect_routing_decision(
                        task_type="coding",
                        prompt_length=len(user_part),
                        category="coding",
                        selected_route=actual_routing_source,
                        performance_score=1.0,  # Approved
                        tokens_saved=tokens_saved,
                        quality_score=quality_metrics.overall_score if quality_metrics else None,
                        success=True,
                        features={
                            "audit_result": "approved",
                            "expert_name": expert_name
                            }
                        )
                
                # –õ–æ–≥–∏—Ä—É–µ–º style_similarity_score –∏ emotion –≤ metadata (Singularity 9.0)
                metadata_dict = {}
                if TacitKnowledgeMiner and style_similarity_score > 0:
                    metadata_dict["style_similarity"] = style_similarity_score
                    metadata_dict["user_identifier"] = user_identifier
                
                if EmotionDetector and emotion_result:
                    metadata_dict["detected_emotion"] = emotion_result.detected_emotion
                    metadata_dict["emotion_confidence"] = emotion_result.confidence
                    metadata_dict["tone_used"] = emotion_result.tone
                    metadata_dict["detail_level"] = emotion_result.detail_level
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º —ç–º–æ—Ü–∏—é –≤ emotion_logs
                    try:
                        from token_logger import log_ai_interaction
                        interaction_log_id = await log_ai_interaction(
                            prompt=user_part,
                            response=local_resp[:2000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                            expert_name=expert_name,
                            model_type="local",
                            source="ai_core",
                            metadata=metadata_dict
                        )
                        
                        if interaction_log_id:
                            detector = EmotionDetector()
                            feedback_score = None  # –ë—É–¥–µ—Ç –æ–±–Ω–æ–≤–ª–µ–Ω –ø–æ–∑–∂–µ, –∫–æ–≥–¥–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–∞—Å—Ç feedback
                            await detector.log_emotion(interaction_log_id, emotion_result, feedback_score)
                    except Exception as e:
                        logger.debug(f"‚ö†Ô∏è [EMOTION DETECTOR] Error logging emotion: {e}")
                
                return local_resp
            else:
                # FEEDBACK LOOP: Send back to local with audit notes
                logger.warning("üîÑ [REVISION NEEDED] Victoria found issues. Retrying locally with feedback...")
                if distiller:
                    # Save the error for learning
                    expert_id = await _get_expert_id(expert_name)
                    if expert_id:  # Only save if expert_id is valid
                        await distiller.save_correction(
                            expert_id, category or "coding", user_part, local_resp, "...", audit_result
                        )
                
                final_prompt = f"–ü–õ–ê–ù –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –û–¢ –¢–ò–ú–õ–ò–î–ê:\n{audit_result}\n\n–ò–°–ü–†–ê–í–¨–¢–ï –ö–û–î:"
                final_result = await router.run_local_llm(final_prompt, category="coding")
                final_resp, _ = final_result if isinstance(final_result, tuple) else (final_result, None)
                if not final_resp:
                    logger.warning("‚ö†Ô∏è [REVISION FAILED] Local model failed on revision, returning original")
                    return local_resp
                return final_resp  # Return revised version

    # 4. Web-Enabled Local Route (–í–µ—Ä–æ–Ω–∏–∫–∞ —Å –≤–µ–±-–ø–æ–∏—Å–∫–æ–º)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ –≤–µ–±-–ø–æ–∏—Å–∫ (–∑–∞–ø—Ä–æ—Å—ã –æ —Ç–µ–∫—É—â–∏—Ö —Å–æ–±—ã—Ç–∏—è—Ö, –Ω–æ–≤–æ—Å—Ç—è—Ö, —Ç—Ä–µ–Ω–¥–∞—Ö)
    needs_web_search = any(kw in user_part.lower() for kw in [
        "–Ω–æ–≤–æ—Å—Ç–∏", "—Ç—Ä–µ–Ω–¥—ã", "—Å–µ–π—á–∞—Å", "—Ç–µ–∫—É—â–∏–µ", "–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ", 
        "–ø–æ—Å–ª–µ–¥–Ω–∏–µ", "2025", "2024", "—Å–µ–≥–æ–¥–Ω—è", "–Ω–µ–¥–∞–≤–Ω–æ", "latest", "recent"
    ])
    
    use_local_route = bool(router and (images or router.should_use_local(prompt, category)) or needs_web_search)
    if use_local_route:
        logger.info("üè† [ROUTE] –í—ã–±—Ä–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç (Ollama/MLX): images=%s, should_use_local=%s, needs_web=%s",
                    bool(images), bool(router and router.should_use_local(prompt, category)), needs_web_search)
    else:
        logger.info("‚òÅÔ∏è [ROUTE] –í—ã–±—Ä–∞–Ω –æ–±–ª–∞—á–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç (—Å–Ω–∞—á–∞–ª–∞ –ø–æ–ø—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –≤–Ω—É—Ç—Ä–∏ _run_cloud_agent_async): category=%s", category)
    
    if router and (images or router.should_use_local(prompt, category)) or needs_web_search:
        # –ï—Å–ª–∏ –Ω—É–∂–µ–Ω –≤–µ–±-–ø–æ–∏—Å–∫, –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–µ—Ä–æ–Ω–∏–∫—É
        if needs_web_search and VeronicaWebResearcher:
            logger.info("üåê [VERONICA WEB] –ó–∞–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≤–µ–±-–ø–æ–∏—Å–∫–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –í–µ—Ä–æ–Ω–∏–∫—É")
            veronica = VeronicaWebResearcher()
            result = await veronica.research_and_analyze(
                user_part,
                category=category or "research",
                use_web=True
            )
            
            if result and result.get('analysis'):
                    logger.info(f"‚úÖ [VERONICA WEB] –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω (0 —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ!)")
                    if cache:
                        await cache.save_to_cache(
                            user_part, result['analysis'], expert_name,
                            routing_source="veronica_web",
                            tokens_saved=len(result['analysis']) // 4,  # –≠–∫–æ–Ω–æ–º–∏—è –æ—Ç –æ–±–ª–∞–∫–∞
                            performance_score=0.9
                        )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ä–æ—É—Ç–∏–Ω–≥–µ –¥–ª—è ML-–æ–±—É—á–µ–Ω–∏—è
                    if get_collector:
                        try:
                            collector = await get_collector()
                            await collector.collect_routing_decision(
                                task_type="research",
                                prompt_length=len(user_part),
                                category="research",
                                selected_route="veronica_web",
                                performance_score=0.9,
                                tokens_saved=len(result['analysis']) // 4,
                                success=True,
                                features={
                                    "expert_name": expert_name,
                                    "web_search": True
                                }
                            )
                        except Exception as e:
                            logger.debug(f"Failed to collect veronica routing data: {e}")
                    
                    return result['analysis']
        
        # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ–±–ª–∞–∫–æ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        if ParallelRequestProcessor and get_parallel_processor and router:
            logger.info("‚ö° [PARALLEL] –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –æ–±–ª–∞–∫–æ")
            parallel_processor = get_parallel_processor(max_concurrent=3)
            
            # –°–æ–∑–¥–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            sources = []
            
            # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1 - –±—ã—Å—Ç—Ä–µ–µ)
            async def try_local():
                if disaster_recovery and not disaster_recovery.can_use_local_models():
                    return None
                try:
                    if local_breaker:
                        result = await local_breaker.call(router.run_local_llm, prompt, category=category, images=images)
                    else:
                        result = await router.run_local_llm(prompt, category=category, images=images)
                    if isinstance(result, tuple):
                        return result[0]
                    return result
                except Exception as e:
                    logger.debug(f"Local model failed in parallel: {e}")
                    return None
            
            sources.append(RequestSource(
                name="local",
                handler=try_local,
                priority=1,
                timeout=30.0
            ))
            
            # –û–±–ª–∞–∫–æ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2 - –º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–µ–µ)
            async def try_cloud():
                try:
                    if cloud_breaker:
                        return await cloud_breaker.call(_run_cloud_agent_async, prompt)
                    else:
                        return await _run_cloud_agent_async(prompt)
                except Exception as e:
                    logger.debug(f"Cloud failed in parallel: {e}")
                    return None
            
            sources.append(RequestSource(
                name="cloud",
                handler=try_cloud,
                priority=2,
                timeout=60.0
            ))
            
            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            response_source_name, response = await parallel_processor.process_parallel_sources(sources)
            
            if response:
                routing_source = f"{response_source_name}_parallel" if response_source_name else "parallel"
                local_resp = response
                logger.info(f"‚úÖ [PARALLEL] –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç {routing_source}")
            else:
                # –ï—Å–ª–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ
                logger.warning("‚ö†Ô∏è [PARALLEL] –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–µ –¥–∞–ª–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞, –ø—Ä–æ–±—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ")
                if router:
                    logger.info("üè† [LOCAL ROUTE] %s", expert_name)
                    local_result = await router.run_local_llm(prompt, category=category, images=images)
                    local_resp, routing_source = local_result if isinstance(local_result, tuple) else (local_result, None)
                else:
                    logger.warning("‚ö†Ô∏è [FALLBACK] Local router unavailable, using cloud")
                    local_resp = await _run_cloud_agent_async(prompt)
                    routing_source = "cloud_fallback"
        else:
            # –û–±—ã—á–Ω—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç (–±–µ–∑ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏)
            if router:
                logger.info("üè† [LOCAL ROUTE] %s", expert_name)
                local_result = await router.run_local_llm(prompt, category=category, images=images)
                local_resp, routing_source = local_result if isinstance(local_result, tuple) else (local_result, None)
            else:
                # Fallback –Ω–∞ –æ–±–ª–∞–∫–æ, –µ—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                logger.warning("‚ö†Ô∏è [FALLBACK] Local router unavailable, using cloud")
                local_resp = await _run_cloud_agent_async(prompt)
                routing_source = "cloud_fallback"
        
        # Safety check for direct local responses
        if local_resp and SafetyChecker:
            checker = SafetyChecker()
            if checker.should_reroute_to_cloud(local_resp, response_type="code" if category == "coding" else "text"):
                logger.warning("üõ°Ô∏è [SAFETY CHECK] Local response failed, using cloud")
                local_resp = None
        
        if local_resp:
            # Estimate savings for direct local usage
            estimated_cloud_tokens = len(prompt) // 4 + len(local_resp) // 4
            logger.info(f"üí∞ [TOKEN SAVINGS] Used local model, saved ~{estimated_cloud_tokens} tokens")
            # Save to cache with routing info and quality metrics
            if cache:
                final_routing_source = routing_source or "local"
                
                # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                performance_score = 0.9  # Default
                if qa:
                    _, metrics, _ = await qa.validate_response(
                        local_resp, user_part, response_type="code", source="local"
                    )
                    performance_score = metrics.overall_score
                
                await cache.save_to_cache(
                    user_part, local_resp, expert_name,
                    routing_source=final_routing_source,
                    tokens_saved=estimated_cloud_tokens,
                    performance_score=performance_score
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è ML-–æ–±—É—á–µ–Ω–∏—è
                if get_collector:
                    collector = await get_collector()
                    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º final_routing_source –µ—Å–ª–∏ –Ω–µ –±—ã–ª –ø–µ—Ä–µ–¥–∞–Ω
                    actual_routing_source = final_routing_source or routing_source or "local"
                    await collector.collect_routing_decision(
                        task_type="general",
                        prompt_length=len(user_part),
                        category=category,
                        selected_route=actual_routing_source,
                        performance_score=performance_score,
                        tokens_saved=estimated_cloud_tokens,
                        quality_score=metrics.overall_score if metrics else None,
                        success=True,
                        features={
                            "expert_name": expert_name,
                            "direct_local": True
                        }
                    )
            
            # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            try:
                from metrics_collector import get_metrics_collector
                duration = time.time() - start_time
                metrics_collector = get_metrics_collector()
                # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω)
                estimated_tokens = len(local_resp) // 4
                await metrics_collector.collect_tokens_per_second(
                    estimated_tokens, duration, "local"
                )
            except Exception as e:
                logger.debug(f"Metrics collection failed: {e}")
            
            return local_resp

    # 5. Query Orchestrator: –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–∞ –∏ —Å–±–æ—Ä–∫–∞ role-aware –ø—Ä–æ–º–ø—Ç–∞
    query_orchestrator = None
    normalized_query = None
    optimized_role = expert_name
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º SessionManager –µ—Å–ª–∏ –µ—Å—Ç—å session_id
    session_manager = None
    if session_id:
        try:
            from strategy_session_manager import StrategySessionManager
            session_manager = StrategySessionManager()
        except Exception:
            pass
    
    if QueryOrchestrator and get_prompt_template:
        try:
            query_orchestrator = QueryOrchestrator(session_manager=session_manager)
            normalized_query = query_orchestrator.normalize_query(user_part)
            optimized_role = query_orchestrator.select_role(normalized_query.query_type)
            logger.info(f"üéØ [QUERY ORCHESTRATOR] –ó–∞–ø—Ä–æ—Å –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω: —Ç–∏–ø={normalized_query.query_type.value}, —Ä–æ–ª—å={optimized_role}")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [QUERY ORCHESTRATOR] –û—à–∏–±–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–ø—Ä–æ—Å–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å")
            query_orchestrator = None
    
    # 6. Full Cloud Call (for Strategic / Architecture tasks)
    knowledge_context = await _get_knowledge_context(user_part)
    
    # –ï—Å–ª–∏ Query Orchestrator –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º role-aware –ø—Ä–æ–º–ø—Ç
    if query_orchestrator and normalized_query and get_prompt_template:
        try:
            # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
            prompt_context = await query_orchestrator.select_context(
                session_id=session_id,  # –ü–µ—Ä–µ–¥–∞–µ–º session_id –¥–ª—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                role=optimized_role,
                normalized_query=normalized_query
            )
            
            # –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç (—Å–∂–∞—Ç–∏–µ –¥–æ 70% –æ–∫–Ω–∞)
            prompt_context = query_orchestrator.optimize_context(prompt_context, max_length=2000, max_window_percent=0.7)
            
            # –ü–æ–ª—É—á–∞–µ–º —à–∞–±–ª–æ–Ω —Ä–æ–ª–∏
            role_template = get_prompt_template(optimized_role)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
            context_str = query_orchestrator.format_context(prompt_context)
            structured_task = query_orchestrator.format_structured_task(normalized_query)
            
            # –î–æ–±–∞–≤–ª—è–µ–º knowledge_context –µ—Å–ª–∏ –µ—Å—Ç—å
            if knowledge_context:
                context_str = f"{context_str}\n\n–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:\n{knowledge_context}"
            
            # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç —á–µ—Ä–µ–∑ —à–∞–±–ª–æ–Ω —Ä–æ–ª–∏
            full_prompt = format_prompt(
                role_template,
                task=structured_task,
                context=context_str,
                constraints=", ".join(normalized_query.constraints) if normalized_query.constraints else "–ù–µ—Ç",
                preferences=", ".join(normalized_query.preferences) if normalized_query.preferences else "–ù–µ—Ç"
            )
            
            logger.info(f"‚úÖ [QUERY ORCHESTRATOR] –ü—Ä–æ–º–ø—Ç —Å–æ–±—Ä–∞–Ω —á–µ—Ä–µ–∑ —à–∞–±–ª–æ–Ω —Ä–æ–ª–∏: –¥–ª–∏–Ω–∞={len(full_prompt)}, —Ä–æ–ª—å={optimized_role}")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [QUERY ORCHESTRATOR] –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∫–∏ –ø—Ä–æ–º–ø—Ç–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π –ø—É—Ç—å")
            full_prompt = (knowledge_context + "\n" + prompt) if knowledge_context else prompt
    else:
        # –°—Ç–∞—Ä—ã–π –ø—É—Ç—å: –ø—Ä–æ—Å—Ç–æ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–æ–º–ø—Ç —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º
        full_prompt = (knowledge_context + "\n" + prompt) if knowledge_context else prompt
    
    # –£–º–Ω–æ–µ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–µ—Ä–µ–¥ –æ—Ç–ø—Ä–∞–≤–∫–æ–π –≤ –æ–±–ª–∞–∫–æ (–∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ)
    # Predictive Compression: –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–µ–¥—Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (Singularity 9.0)
    compressed_prompt = full_prompt
    latency_before_compression = time.time()
    latency_reduction = 0.0
    
    if ContextAnalyzer and len(full_prompt) > 2000:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø—Ä–µ–¥—Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (Predictive Compression)
        precompressed = None
        try:
            analyzer = ContextAnalyzer(relevance_threshold=0.65)
            precompressed = await analyzer.get_precompressed_context(user_part, user_identifier)
            
            if precompressed:
                compressed_prompt = precompressed
                latency_after_compression = time.time()
                latency_reduction = ((latency_before_compression - latency_after_compression) / latency_before_compression) if latency_before_compression > 0 else 0.0
                tokens_saved = (len(full_prompt) - len(compressed_prompt)) // 4
                logger.info(f"üöÄ [PREDICTIVE COMPRESSION] Using precompressed context: {len(compressed_prompt)} chars (~{tokens_saved} tokens saved, latency ‚Üì {latency_reduction:.1%})")
            else:
                # –û–±—ã—á–Ω–æ–µ —Å–∂–∞—Ç–∏–µ, –µ—Å–ª–∏ –ø—Ä–µ–¥—Å–∂–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω
                analyzer = ContextAnalyzer(relevance_threshold=0.65)
                compressed_prompt = await analyzer.compress_context(full_prompt, user_part, max_length=2000)
                tokens_saved = (len(full_prompt) - len(compressed_prompt)) // 4
                logger.info(f"üìâ [CONTEXT COMPRESSION] Compressed from {len(full_prompt)} to {len(compressed_prompt)} chars (~{tokens_saved} tokens saved)")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [PREDICTIVE COMPRESSION] Error checking precompressed context: {e}")
            # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É —Å–∂–∞—Ç–∏—é
            analyzer = ContextAnalyzer(relevance_threshold=0.65)
            compressed_prompt = await analyzer.compress_context(full_prompt, user_part, max_length=2000)
            tokens_saved = (len(full_prompt) - len(compressed_prompt)) // 4
            logger.info(f"üìâ [CONTEXT COMPRESSION] Compressed from {len(full_prompt)} to {len(compressed_prompt)} chars (~{tokens_saved} tokens saved)")
    elif ContextCompressor:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∂–∞—Ç–∏–µ
        compressed_prompt = await ContextCompressor.compress_smart(full_prompt, user_part, max_length=2000, aggressive=True)
        if len(compressed_prompt) < len(full_prompt):
            tokens_saved = (len(full_prompt) - len(compressed_prompt)) // 4
            logger.info(f"üìâ [CONTEXT COMPRESSION] Compressed from {len(full_prompt)} to {len(compressed_prompt)} chars (~{tokens_saved} tokens saved)")
        else:
            compressed_prompt = ContextCompressor.compress_all(full_prompt)
    
    cloud_start_time = time.time()
    response = await _run_cloud_agent_async(compressed_prompt)
    cloud_latency_ms = (time.time() - cloud_start_time) * 1000
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ —Ä–æ—É—Ç–∏–Ω–≥–µ –≤ –æ–±–ª–∞–∫–æ –¥–ª—è ML-–æ–±—É—á–µ–Ω–∏—è
    if get_collector and response:
        try:
            collector = await get_collector()
            await collector.collect_routing_decision(
                task_type="general",
                prompt_length=len(user_part),
                category=category,
                selected_route="cloud",
                performance_score=0.9,  # Cloud –æ–±—ã—á–Ω–æ —Ö–æ—Ä–æ—à –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
                tokens_saved=0,  # –û–±–ª–∞–∫–æ –Ω–µ —ç–∫–æ–Ω–æ–º–∏—Ç —Ç–æ–∫–µ–Ω—ã
                latency_ms=cloud_latency_ms,
                quality_score=None,  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å QA –ø—Ä–æ–≤–µ—Ä–∫—É
                success=True,
                features={
                    "expert_name": expert_name,
                    "full_cloud_call": True,
                    "has_knowledge_context": bool(knowledge_context),
                    "prompt_compressed": len(compressed_prompt) < len(full_prompt)
                }
            )
            logger.debug("‚úÖ [ML DATA] Saved cloud routing decision")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [ML DATA] Failed to collect cloud routing data: {e}")

    # Offline fallback
    if response and (response.startswith('‚ùå') or response.startswith('‚ö†Ô∏è')) and router:
        logger.warning("üõ°Ô∏è [BUNKER MODE] Cloud failed, switching to Local.")
        return await router.run_local_llm(prompt)
    
    # –î–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤–Ω–µ—à–Ω–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (Singularity 8.0)
    if response and not response.startswith(('‚ö†Ô∏è', '‚ùå')):
        try:
            from external_api_integration import get_external_api_integration
            external_api = get_external_api_integration()
            enhanced_response = await external_api.enhance_response_with_external_data(user_part, response)
            if enhanced_response and len(enhanced_response) > len(response):
                response = enhanced_response
                logger.info("üåê [EXTERNAL API] –û—Ç–≤–µ—Ç –¥–æ–ø–æ–ª–Ω–µ–Ω –≤–Ω–µ—à–Ω–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [EXTERNAL API] –û—à–∏–±–∫–∞ –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞: {e}")

    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π response –µ—Å–ª–∏ –µ—â–µ –Ω–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω
    if 'response' not in locals():
        response = local_resp if 'local_resp' in locals() else None
    
    if cache and response and not response.startswith(('‚ö†Ô∏è', '‚ùå')):
        await cache.save_to_cache(user_part, response, expert_name)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å–µ—Å—Å–∏–∏ (Singularity 8.0)
    if get_session_context_manager and response and not response.startswith(('‚ö†Ô∏è', '‚ùå')):
        try:
            # –ü–æ–ª—É—á–∞–µ–º user_id –∏–∑ request_id (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω) –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π
            user_id = request_id.split('_')[0] if '_' in request_id else "default"
            context_manager = get_session_context_manager()
            await context_manager.save_to_context(
                user_id=user_id,
                expert_name=expert_name,
                query=user_part,
                response=response
            )
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [SESSION CONTEXT] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}")
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ (—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ)
    if response and isinstance(response, str) and len(response) > 0:
        try:
            from token_logger import log_ai_interaction_fire_and_forget
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ routing_source (–µ—Å–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω)
            model_type = "gpt-4o-mini"  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
            routing_src = None
            try:
                if 'routing_source' in locals() or 'routing_source' in globals():
                    routing_src = locals().get('routing_source') or globals().get('routing_source')
                elif 'local_resp' in locals() and locals().get('local_resp'):
                    routing_src = "local"  # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω local_resp, –∑–Ω–∞—á–∏—Ç –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
            except Exception as e:
                logger.debug("–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ routing_source: %s", e)
            if routing_src:
                if "local" in str(routing_src).lower():
                    model_type = "local"
                elif "cloud" in str(routing_src).lower() or routing_src == "cloud_fallback":
                    model_type = "gpt-4o-mini"
                elif routing_src == "cursor-agent":
                    model_type = "cursor-agent"
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –∑–Ω–∞–Ω–∏—è –∏–∑ –∫—ç—à–∞ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
            knowledge_ids = None
            knowledge_applied = False
            if cache:
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∑–Ω–∞–Ω–∏—è—Ö –∏–∑ –∫—ç—à–∞
                try:
                    cache_info = await cache.get_cache_info(user_part)
                    if cache_info and cache_info.get("knowledge_nodes"):
                        knowledge_ids = cache_info.get("knowledge_node_ids", [])
                        knowledge_applied = bool(knowledge_ids)
                except Exception as e:
                    logger.debug("get_cache_info: %s", e)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (fire and forget - –Ω–µ –±–ª–æ–∫–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç)
            # –§–æ—Ä–º–∏—Ä—É–µ–º metadata –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è (Singularity 9.0 - Predictive Compression)
            metadata_for_logging = {}
            if latency_reduction > 0:
                metadata_for_logging["latency_reduction"] = latency_reduction
                metadata_for_logging["predictive_compression_used"] = True
            
            log_ai_interaction_fire_and_forget(
                prompt=user_part,
                response=response,
                expert_id=None,  # –ë—É–¥–µ—Ç –Ω–∞–π–¥–µ–Ω –ø–æ –∏–º–µ–Ω–∏
                expert_name=expert_name,
                model_type=model_type,
                source="ai_core",
                knowledge_ids=knowledge_ids,
                knowledge_applied=knowledge_applied,
                category=category,
                metadata=metadata_for_logging if metadata_for_logging else None
            )
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è [TOKEN LOGGING] –û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤: {e}")
    
    # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±–ª–∞—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
    try:
        from metrics_collector import get_metrics_collector
        duration = time.time() - start_time
        metrics_collector = get_metrics_collector()
        # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ 4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω)
        estimated_tokens = len(response) // 4 if response else 0
        if estimated_tokens > 0 and duration > 0:
            await metrics_collector.collect_tokens_per_second(
                estimated_tokens, duration, "cloud"
            )
    except Exception as e:
        logger.debug(f"Metrics collection failed: {e}")

    return response

async def _get_expert_id(name: str) -> str:
    """Helper to get expert UUID from DB."""
    pool = await _get_db_pool()
    if not pool: return None
    async with pool.acquire() as conn:
        return await conn.fetchval("SELECT id FROM experts WHERE name = $1", name)

# Sync wrapper implementation would go here (omitted for brevity)
