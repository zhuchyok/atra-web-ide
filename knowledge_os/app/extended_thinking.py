"""
Extended Thinking Mode - –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ Anthropic Claude Extended Thinking
"""

import os
import asyncio
import logging
from typing import Dict, Optional, List
from dataclasses import dataclass
from datetime import datetime, timezone

logger = logging.getLogger(__name__)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û MLX API Server (–ø–æ—Ä—Ç 11435)
# –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ MLX API Server
MLX_URL = os.getenv('MLX_API_URL', 'http://localhost:11435')
# –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º MLX
DEFAULT_LLM_URL = MLX_URL

# –ö—ç—à –¥–ª—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π (—á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∞—Ç—å —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ /api/tags)
_models_cache = {"data": None, "timestamp": 0}
_MODELS_CACHE_TTL = 120  # 2 –º–∏–Ω—É—Ç—ã –∫—ç—à –¥–ª—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π


@dataclass
class ThinkingStep:
    """–û–¥–∏–Ω —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
    step_number: int
    thought: str
    conclusion: Optional[str] = None
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now(timezone.utc)


@dataclass
class ExtendedThinkingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
    final_answer: str
    thinking_steps: List[ThinkingStep]
    total_tokens_used: int
    thinking_time_seconds: float
    confidence: float


class ExtendedThinkingEngine:
    """
    Extended Thinking Mode - –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º
    
    –ú–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç –ø–æ—à–∞–≥–æ–≤–æ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ),
    –∑–∞—Ç–µ–º —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.
    """
    
    def __init__(
        self,
        model_name: str = "deepseek-r1-distill-llama:70b",
        thinking_budget: int = 10000,  # –¢–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        max_steps: int = 10,
        use_intelligent_routing: bool = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥
    ):
        self.model_name = model_name  # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (fallback)
        self.use_intelligent_routing = use_intelligent_routing
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ MLX
        self.llm_url = DEFAULT_LLM_URL
        self.thinking_budget = thinking_budget
        self.max_steps = max_steps
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        if self.use_intelligent_routing:
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–ø–æ—Ä—Ç–∞
                try:
                    from app.intelligent_model_router import get_intelligent_router
                except ImportError:
                    try:
                        from intelligent_model_router import get_intelligent_router
                    except ImportError:
                        import sys
                        import os
                        router_path = os.path.join(os.path.dirname(__file__), 'intelligent_model_router.py')
                        if os.path.exists(router_path):
                            sys.path.insert(0, os.path.dirname(__file__))
                            from intelligent_model_router import get_intelligent_router
                        else:
                            raise ImportError("intelligent_model_router.py not found")
                
                self.model_router = get_intelligent_router()
                logger.info(f"‚úÖ ExtendedThinkingEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —Ä–æ—É—Ç–∏–Ω–≥–æ–º: URL={self.llm_url}, –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å={self.model_name}")
            except (ImportError, Exception) as e:
                logger.warning(f"‚ö†Ô∏è Intelligent router –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
                self.use_intelligent_routing = False
                self.model_router = None
        else:
            self.model_router = None
            logger.info(f"‚úÖ ExtendedThinkingEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: URL={self.llm_url}, –º–æ–¥–µ–ª—å={self.model_name}")
    
    async def think(
        self,
        prompt: str,
        context: Optional[str] = None,
        use_iterative: bool = True,
        category: Optional[str] = None
    ) -> ExtendedThinkingResult:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏
        
        Args:
            prompt: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            use_iterative: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏ —à–∞–≥–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        """
        start_time = datetime.now(timezone.utc)
        
        if use_iterative:
            return await self._iterative_thinking(prompt, context, category)
        else:
            return await self._single_pass_thinking(prompt, context, category)
    
    async def _get_available_models(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ MLX API Server —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        global _models_cache
        import httpx
        import os
        import time
        
        current_time = time.time()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ –µ—â–µ –≤–∞–ª–∏–¥–µ–Ω
        if _models_cache["data"] and (current_time - _models_cache["timestamp"]) < _MODELS_CACHE_TTL:
            logger.debug(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ({len(_models_cache['data'])} –º–æ–¥–µ–ª–µ–π)")
            return _models_cache["data"]
        
        try:
            mlx_url = os.getenv('MLX_API_URL', 'http://localhost:11435')
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{mlx_url}/api/tags")
                if response.status_code == 200:
                    models_data = response.json()
                    models = models_data.get("models", [])
                    available = [m.get("name") for m in models if m.get("exists", True)]
                    logger.debug(f"üìã –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π –≤ MLX: {len(available)}")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
                    _models_cache = {"data": available, "timestamp": current_time}
                    return available
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫—ç—à, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Å—Ç–µ–∫
            if _models_cache["data"]:
                logger.debug("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏")
                return _models_cache["data"]
        
        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ PLAN.md
        # –í–ê–ñ–ù–û: tinyllama –∏—Å–∫–ª—é—á–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
        fallback_models = [
            "command-r-plus:104b",
            "deepseek-r1-distill-llama:70b",
            "llama3.3:70b",
            "qwen2.5-coder:32b",
            "phi3.5:3.8b",
            "phi3:mini-4k",
            "qwen2.5:3b"
        ]
    
    async def _iterative_thinking(
        self,
        prompt: str,
        context: Optional[str] = None,
        category: Optional[str] = None
    ) -> ExtendedThinkingResult:
        """–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤"""
        thinking_steps = []
        current_understanding = ""
        
        # –ù–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        thinking_prompt = self._build_thinking_prompt(prompt, context, step=1)
        
        for step_num in range(1, self.max_steps + 1):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
            step_thought = await self._generate_thinking_step(
                thinking_prompt,
                step_num,
                current_understanding,
                category
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º—ã—Å–ª—å –∏ –≤—ã–≤–æ–¥
            thought, conclusion = self._parse_thinking_step(step_thought)
            
            step = ThinkingStep(
                step_number=step_num,
                thought=thought,
                conclusion=conclusion
            )
            thinking_steps.append(step)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ
            current_understanding += f"\n–®–∞–≥ {step_num}: {thought}\n"
            if conclusion:
                current_understanding += f"–í—ã–≤–æ–¥: {conclusion}\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å
            if conclusion and self._is_final_conclusion(conclusion):
                break
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
            thinking_prompt = self._build_thinking_prompt(
                prompt, context, step=step_num + 1, previous_steps=current_understanding
            )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
        final_answer = await self._synthesize_final_answer(prompt, thinking_steps, category)
        
        elapsed = (datetime.now(timezone.utc) - datetime.now(timezone.utc)).total_seconds()
        
        return ExtendedThinkingResult(
            final_answer=final_answer,
            thinking_steps=thinking_steps,
            total_tokens_used=sum(len(s.thought) for s in thinking_steps) + len(final_answer),
            thinking_time_seconds=elapsed,
            confidence=self._calculate_confidence(thinking_steps)
        )
    
    async def _single_pass_thinking(
        self,
        prompt: str,
        context: Optional[str] = None,
        category: Optional[str] = None
    ) -> ExtendedThinkingResult:
        """–û–¥–Ω–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - –≤—Å–µ —Å—Ä–∞–∑—É"""
        thinking_prompt = self._build_thinking_prompt(prompt, context, step=1)
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
        full_thinking = await self._generate_response(thinking_prompt, max_tokens=self.thinking_budget, category=category)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        final_answer = await self._extract_final_answer(full_thinking, prompt)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —à–∞–≥–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω—É–º–µ—Ä–∞—Ü–∏—è)
        thinking_steps = self._parse_thinking_into_steps(full_thinking)
        
        elapsed = 0.0  # TODO: —Ä–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è
        
        return ExtendedThinkingResult(
            final_answer=final_answer,
            thinking_steps=thinking_steps,
            total_tokens_used=len(full_thinking) + len(final_answer),
            thinking_time_seconds=elapsed,
            confidence=0.8  # TODO: —Ä–µ–∞–ª—å–Ω–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        )
    
    def _build_thinking_prompt(
        self,
        prompt: str,
        context: Optional[str],
        step: int,
        previous_steps: Optional[str] = None
    ) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        thinking_prompt = f"""–¢—ã —Ä–µ—à–∞–µ—à—å —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É. –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥—É–º–∞–π –ø–æ—à–∞–≥–æ–≤–æ, –∑–∞—Ç–µ–º –¥–∞–π –æ—Ç–≤–µ—Ç.

–ó–ê–î–ê–ß–ê: {prompt}

"""
        
        if context:
            thinking_prompt += f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n"
        
        if previous_steps:
            thinking_prompt += f"–ü–†–ï–î–´–î–£–©–ò–ï –®–ê–ì–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–Ø:\n{previous_steps}\n\n"
        
        if step == 1:
            thinking_prompt += """–ù–ê–ß–ù–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:

–®–∞–≥ 1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–¥–∞—á—É:
"""
        else:
            thinking_prompt += f"""–ü–†–û–î–û–õ–ñ–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:

–®–∞–≥ {step}. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤, —á—Ç–æ –¥–∞–ª—å—à–µ?
"""
        
        thinking_prompt += """
–§–æ—Ä–º–∞—Ç:
1. –ú—ã—Å–ª—å/–∞–Ω–∞–ª–∏–∑
2. –í—ã–≤–æ–¥ (–µ—Å–ª–∏ –≥–æ—Ç–æ–≤ –¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –Ω–∞–ø–∏—à–∏ "–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢: ...")

–¢–í–û–ï –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:"""
        
        return thinking_prompt
    
    async def _generate_thinking_step(
        self,
        prompt: str,
        step_num: int,
        current_understanding: str,
        category: Optional[str] = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        return await self._generate_response(prompt, max_tokens=2048, category=category)
    
    def _parse_thinking_step(self, step_text: str) -> tuple[str, Optional[str]]:
        """–ü–∞—Ä—Å–∏—Ç—å —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        lines = step_text.strip().split('\n')
        thought = ""
        conclusion = None
        
        for line in lines:
            if "–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:" in line or "–í–´–í–û–î:" in line:
                conclusion = line.split(":", 1)[1].strip() if ":" in line else line
            else:
                thought += line + "\n"
        
        return thought.strip(), conclusion
    
    def _is_final_conclusion(self, conclusion: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º"""
        return "–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢" in conclusion.upper() or len(conclusion) > 50
    
    async def _synthesize_final_answer(
        self,
        original_prompt: str,
        thinking_steps: List[ThinkingStep],
        category: Optional[str] = None
    ) -> str:
        """–°–∏–Ω—Ç–µ–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π"""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤—ã–≤–æ–¥—ã
        all_conclusions = [s.conclusion for s in thinking_steps if s.conclusion]
        
        if all_conclusions:
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤—ã–≤–æ–¥ –∏–ª–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º
            if len(all_conclusions) == 1:
                return all_conclusions[0]
            else:
                # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—ã–≤–æ–¥—ã
                synthesis_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–∏—Ö —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è, —Å—Ñ–æ—Ä–º–∏—Ä—É–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:

–ó–ê–î–ê–ß–ê: {original_prompt}

–®–ê–ì–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–Ø:
"""
                for i, step in enumerate(thinking_steps, 1):
                    synthesis_prompt += f"\n{i}. {step.thought}\n"
                    if step.conclusion:
                        synthesis_prompt += f"   –í—ã–≤–æ–¥: {step.conclusion}\n"
                
                synthesis_prompt += "\n–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:"
                
                return await self._generate_response(synthesis_prompt, max_tokens=2048, category=category)
        
        # Fallback: –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –º—ã—Å–ª—å
        if thinking_steps:
            return thinking_steps[-1].thought
        
        return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç"
    
    async def _extract_final_answer(self, thinking: str, original_prompt: str) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        # –ò—â–µ–º –º–∞—Ä–∫–µ—Ä—ã —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
        markers = ["–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:", "–û–¢–í–ï–¢:", "–ò–¢–û–ì:", "–í–´–í–û–î:"]
        
        for marker in markers:
            if marker in thinking:
                parts = thinking.split(marker, 1)
                if len(parts) > 1:
                    return parts[1].strip()
        
        # –ï—Å–ª–∏ –º–∞—Ä–∫–µ—Ä–æ–≤ –Ω–µ—Ç, –±–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–±–∑–∞—Ü
        paragraphs = thinking.split('\n\n')
        if paragraphs:
            return paragraphs[-1].strip()
        
        return thinking.strip()
    
    def _parse_thinking_into_steps(self, thinking: str) -> List[ThinkingStep]:
        """–†–∞–∑–æ–±—Ä–∞—Ç—å —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –Ω–∞ —à–∞–≥–∏"""
        steps = []
        
        # –ò—â–µ–º –Ω—É–º–µ—Ä–æ–≤–∞–Ω–Ω—ã–µ —à–∞–≥–∏
        import re
        step_pattern = r'(?:–®–∞–≥|Step|–®–∞–≥)\s*(\d+)[:.]\s*(.+?)(?=(?:–®–∞–≥|Step|–®–∞–≥)\s*\d+|$)'
        matches = re.finditer(step_pattern, thinking, re.IGNORECASE | re.DOTALL)
        
        for match in matches:
            step_num = int(match.group(1))
            thought = match.group(2).strip()
            steps.append(ThinkingStep(step_number=step_num, thought=thought))
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, —Å–æ–∑–¥–∞–µ–º –æ–¥–∏–Ω —à–∞–≥
        if not steps:
            steps.append(ThinkingStep(step_number=1, thought=thinking))
        
        return steps
    
    def _calculate_confidence(self, thinking_steps: List[ThinkingStep]) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —à–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        if not thinking_steps:
            return 0.0
        
        # –ë–∞–∑–æ–≤–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        confidence = 0.5
        
        # –ë–æ–Ω—É—Å –∑–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ (–±–æ–ª—å—à–µ —à–∞–≥–æ–≤ = –±–æ–ª–µ–µ —Ç—â–∞—Ç–µ–ª—å–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ)
        if len(thinking_steps) >= 3:
            confidence += 0.2
        
        # –ë–æ–Ω—É—Å –∑–∞ –Ω–∞–ª–∏—á–∏–µ –≤—ã–≤–æ–¥–æ–≤
        conclusions_count = sum(1 for s in thinking_steps if s.conclusion)
        if conclusions_count > 0:
            confidence += 0.2 * min(conclusions_count / len(thinking_steps), 1.0)
        
        # –ë–æ–Ω—É—Å –∑–∞ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
        if thinking_steps[-1].conclusion:
            confidence += 0.1
        
        return min(confidence, 1.0)
    
    async def _get_available_models(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ MLX API Server —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        global _models_cache
        import httpx
        import os
        import time
        from typing import List
        
        current_time = time.time()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ –µ—â–µ –≤–∞–ª–∏–¥–µ–Ω
        if _models_cache["data"] and (current_time - _models_cache["timestamp"]) < _MODELS_CACHE_TTL:
            logger.debug(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ({len(_models_cache['data'])} –º–æ–¥–µ–ª–µ–π)")
            return _models_cache["data"]
        
        try:
            mlx_url = os.getenv('MLX_API_URL', 'http://localhost:11435')
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{mlx_url}/api/tags")
                if response.status_code == 200:
                    models_data = response.json()
                    models = models_data.get("models", [])
                    available = [m.get("name") for m in models if m.get("exists", True)]
                    logger.debug(f"üìã –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π –≤ MLX: {len(available)}")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
                    _models_cache = {"data": available, "timestamp": current_time}
                    return available
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫—ç—à, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Å—Ç–µ–∫
            if _models_cache["data"]:
                logger.debug("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏")
                return _models_cache["data"]
        
        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ PLAN.md
        return [
            "command-r-plus:104b",
            "deepseek-r1-distill-llama:70b",
            "llama3.3:70b",
            "qwen2.5-coder:32b",
            "phi3.5:3.8b",
            "phi3:mini-4k",
            "qwen2.5:3b"
            # "tinyllama:1.1b-chat"  # –ò—Å–∫–ª—é—á–µ–Ω–∞ - —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
        ]
    
    async def _generate_response(
        self,
        prompt: str,
        max_tokens: int = 2048,
        category: Optional[str] = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å (—Ç–æ–ª—å–∫–æ MLX)
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥ –¥–ª—è –≤—ã–±–æ—Ä–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            max_tokens: –ú–∞–∫—Å–∏–º—É–º —Ç–æ–∫–µ–Ω–æ–≤
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–¥–∞—á–∏ (–¥–ª—è —Ä–æ—É—Ç–∏–Ω–≥–∞)
        """
        import httpx
        from typing import List
        
        logger.info("[VICTORIA_CYCLE] extended_thinking _generate_response prompt_preview=%s category=%s",
                    (prompt or "")[:60], category)
        # –ö–†–ò–¢–ò–ß–ù–û: –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á–∏
        selected_model = self.model_name  # Fallback –Ω–∞ –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å
        
        if self.use_intelligent_routing and self.model_router:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ MLX Server
                available_models = await self._get_available_models()
                
                # –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                # –î–ª—è reasoning –∑–∞–¥–∞—á –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–∞—á–µ—Å—Ç–≤–∞ (–Ω—É–∂–Ω—ã –º–æ—â–Ω—ã–µ –º–æ–¥–µ–ª–∏)
                is_reasoning = category and category.lower() in ["reasoning", "–ª–æ–≥–∏–∫–∞", "–∞–Ω–∞–ª–∏–∑", "planning"]
                prioritize_quality = is_reasoning or "–ø–æ–¥—É–º–∞–π" in prompt.lower() or "–ª–æ–≥–∏–∫–∞" in prompt.lower()
                
                optimal_model, task_category, confidence = await self.model_router.select_optimal_model(
                    prompt=prompt,
                    category=category,
                    available_models=available_models,
                    prioritize_quality=prioritize_quality,
                    prioritize_speed=False
                )
                
                if optimal_model:
                    selected_model = optimal_model
                    logger.info(
                        f"üéØ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥: –≤—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å {selected_model} "
                        f"–¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ {task_category.value} (confidence: {confidence:.3f})"
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥ –Ω–µ –Ω–∞—à–µ–ª –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é: {self.model_name}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ä–æ—É—Ç–∏–Ω–≥–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        model_to_use = selected_model
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º MLX API Server —Å fallback –Ω–∞ Ollama
        mlx_url = os.getenv('MLX_API_URL', 'http://localhost:11435')
        is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
        ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        if is_docker:
            ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://host.docker.internal:11434')
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å MLX, Ollama –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏ rate limit
        urls_to_try = [mlx_url] if mlx_url else []
        
        try:
            # –ö–†–ò–¢–ò–ß–ù–û: –¢–∞–π–º–∞—É—Ç —É–≤–µ–ª–∏—á–µ–Ω –¥–æ 300 —Å–µ–∫—É–Ω–¥ (5 –º–∏–Ω—É—Ç)
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ deepseek-r1-distill-llama:70b –∑–∞–Ω–∏–º–∞—é—Ç 120-125 —Å–µ–∫—É–Ω–¥
            # –°—Ç–∞—Ä—ã–π —Ç–∞–π–º–∞—É—Ç 120 —Å–µ–∫—É–Ω–¥ –±—ã–ª —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–º
            async with httpx.AsyncClient(timeout=300.0) as client:
                for llm_url in urls_to_try:
                    try:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å (—á–µ—Ä–µ–∑ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥ –∏–ª–∏ –±–∞–∑–æ–≤—É—é)
                        # –ß–∞—Ç —Å –í–∏–∫—Ç–æ—Ä–∏–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç HIGH
                        headers = {"X-Request-Priority": "high"}
                        response = await client.post(
                            f"{llm_url}/api/generate",
                            json={
                                "model": model_to_use,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                                "prompt": prompt,
                                "stream": False,
                                "options": {
                                    "temperature": 0.5,  # –ù–∏–∑–∫–∞—è –¥–ª—è reasoning
                                    "num_predict": max_tokens
                                }
                            },
                            headers=headers
                        )
                        
                        if response.status_code == 200:
                            result = response.json().get('response', '')
                            if result:
                                source = "MLX"
                                logger.debug(f"‚úÖ ExtendedThinking –∏—Å–ø–æ–ª—å–∑—É–µ—Ç {source}: {llm_url} (–º–æ–¥–µ–ª—å: {model_to_use})")
                                return result
                        elif response.status_code == 429:
                            # Rate limit - –¥–ª—è MLX –ø—Ä–æ–±—É–µ–º Ollama fallback
                            is_mlx = "11435" in llm_url or "mlx" in llm_url.lower()
                            if is_mlx:
                                logger.warning(f"‚ö†Ô∏è [RATE LIMIT] MLX rate limit –Ω–∞ {llm_url}, –ø—Ä–æ–±—É–µ–º Ollama fallback...")
                                # –î–æ–±–∞–≤–ª—è–µ–º Ollama –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
                                ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
                                is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
                                if is_docker:
                                    ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://host.docker.internal:11434')
                                if ollama_url not in urls_to_try:
                                    urls_to_try.append(ollama_url)
                                    logger.info(f"üîÑ [FALLBACK] –î–æ–±–∞–≤–ª–µ–Ω Ollama –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ rate limit: {ollama_url}")
                            else:
                                logger.warning(f"‚ö†Ô∏è [RATE LIMIT] Rate limit –Ω–∞ {llm_url}, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π URL...")
                            # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                            await asyncio.sleep(2)
                            continue
                        elif response.status_code >= 500:
                            # –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ - –¥–ª—è MLX –ø—Ä–æ–±—É–µ–º Ollama fallback
                            is_mlx = "11435" in llm_url or "mlx" in llm_url.lower()
                            if is_mlx:
                                logger.warning(f"‚ö†Ô∏è [SERVER ERROR] MLX —Å–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {response.status_code} –Ω–∞ {llm_url}, –ø—Ä–æ–±—É–µ–º Ollama fallback...")
                                # –î–æ–±–∞–≤–ª—è–µ–º Ollama –≤ —Å–ø–∏—Å–æ–∫ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–∏
                                ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
                                is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
                                if is_docker:
                                    ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://host.docker.internal:11434')
                                if ollama_url not in urls_to_try:
                                    urls_to_try.append(ollama_url)
                                    logger.info(f"üîÑ [FALLBACK] –î–æ–±–∞–≤–ª–µ–Ω Ollama –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–µ—Ä–≤–µ—Ä–Ω–æ–π –æ—à–∏–±–∫–∏: {ollama_url}")
                            else:
                                logger.warning(f"‚ö†Ô∏è [SERVER ERROR] –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {response.status_code} –Ω–∞ {llm_url}")
                            continue
                        elif response.status_code == 404:
                            # –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π fallback
                            if self.use_intelligent_routing and self.model_router:
                                try:
                                    # –ü–æ–ª—É—á–∞–µ–º fallback –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ —Ä–æ—É—Ç–µ—Ä
                                    task_category = self.model_router.classify_task(prompt, category)
                                    fallback_models = self.model_router.get_fallback_models(
                                        model_to_use,
                                        task_category,
                                        max_fallbacks=5
                                    )
                                    logger.info(f"üîÑ –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π fallback –¥–ª—è {model_to_use}: {fallback_models}")
                                except Exception as e:
                                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è fallback –º–æ–¥–µ–ª–µ–π: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Å–ø–∏—Å–æ–∫")
                                    fallback_models = [
                                        "deepseek-r1-distill-llama:70b",
                                        "llama3.3:70b",
                                        "qwen2.5-coder:32b",
                                        "phi3.5:3.8b",
                                        "qwen2.5:3b",
                                        "phi3:mini-4k"
                                        # tinyllama –∏—Å–∫–ª—é—á–µ–Ω–∞ - —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
                                    ]
                            else:
                                # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π fallback —Å–ø–∏—Å–æ–∫
                                fallback_models = [
                                    "deepseek-r1-distill-llama:70b",
                                    "llama3.3:70b",
                                    "qwen2.5-coder:32b",
                                    "phi3.5:3.8b",
                                    "qwen2.5:3b",
                                    "phi3:mini-4k"
                                    # tinyllama –∏—Å–∫–ª—é—á–µ–Ω–∞ - —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
                                ]
                            
                            logger.warning(f"–ú–æ–¥–µ–ª—å {model_to_use} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ {llm_url}, –ø—Ä–æ–±—É–µ–º fallback –º–æ–¥–µ–ª–∏...")
                            for fallback_model in fallback_models:
                                if fallback_model == model_to_use:
                                    continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–∂–µ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                                try:
                                    # –ß–∞—Ç —Å –í–∏–∫—Ç–æ—Ä–∏–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç HIGH
                                    headers = {"X-Request-Priority": "high"}
                                    fallback_response = await client.post(
                                        f"{llm_url}/api/generate",
                                        json={
                                            "model": fallback_model,
                                            "prompt": prompt,
                                            "stream": False,
                                            "options": {
                                                "temperature": 0.5,
                                                "num_predict": max_tokens
                                            }
                                        },
                                        headers=headers,
                                        timeout=300.0  # –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–ª—è fallback –º–æ–¥–µ–ª–µ–π
                                    )
                                    if fallback_response.status_code == 200:
                                        source = "MLX"
                                        logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ {source} fallback –º–æ–¥–µ–ª—å: {fallback_model}")
                                        return fallback_response.json().get('response', '')
                                    elif fallback_response.status_code == 429:
                                        # Rate limit –Ω–∞ fallback –º–æ–¥–µ–ª–∏ - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        logger.warning(f"‚ö†Ô∏è [RATE LIMIT] Fallback –º–æ–¥–µ–ª—å {fallback_model} –Ω–∞ {llm_url} - rate limit")
                                        await asyncio.sleep(2)  # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                                        continue
                                    elif fallback_response.status_code >= 500:
                                        # –°–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ –Ω–∞ fallback - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                                        logger.warning(f"‚ö†Ô∏è [SERVER ERROR] Fallback –º–æ–¥–µ–ª—å {fallback_model} –Ω–∞ {llm_url} - —Å–µ—Ä–≤–µ—Ä–Ω–∞—è –æ—à–∏–±–∫–∞ {fallback_response.status_code}")
                                        continue
                                except Exception as e:
                                    logger.debug(f"Fallback –º–æ–¥–µ–ª—å {fallback_model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ {llm_url}: {e}")
                                    continue
                            
                            # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π URL (—Ç–æ–ª—å–∫–æ MLX)
                            continue
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ {llm_url}: {e}")
                        continue
                
                logger.error(f"‚ùå –í—Å–µ –º–æ–¥–µ–ª–∏ –∏ URL –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã")
                return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏: {e}")
            return ""


async def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
    engine = ExtendedThinkingEngine(
        model_name="deepseek-r1-distill-llama:70b",
        thinking_budget=10000
    )
    
    result = await engine.think(
        "–†–µ—à–∏ –∑–∞–¥–∞—á—É: –£ –ú–∞—à–∏ –±—ã–ª–æ 5 —è–±–ª–æ–∫, –æ–Ω–∞ –æ—Ç–¥–∞–ª–∞ 2 –¥—Ä—É–≥—É, –∑–∞—Ç–µ–º –∫—É–ø–∏–ª–∞ –µ—â–µ 3. –°–∫–æ–ª—å–∫–æ —è–±–ª–æ–∫ —É –ú–∞—à–∏ —Ç–µ–ø–µ—Ä—å?",
        use_iterative=True
    )
    
    print("–§–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç:", result.final_answer)
    print("–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å:", result.confidence)
    print("–®–∞–≥–æ–≤ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è:", len(result.thinking_steps))
    for step in result.thinking_steps:
        print(f"\n–®–∞–≥ {step.step_number}: {step.thought[:100]}...")


if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    asyncio.run(main())
