"""
Extended Thinking Mode - –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á
–û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ Anthropic Claude Extended Thinking
"""

import os
import asyncio
import logging
import time
import json
from typing import Dict, Optional, List, Tuple, Any
from dataclasses import dataclass
from datetime import datetime, timezone

logger = logging.getLogger(__name__)

# –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–û–õ–¨–ö–û MLX API Server (–ø–æ—Ä—Ç 11435)
MLX_URL = os.getenv('MLX_API_URL', 'http://localhost:11435')
DEFAULT_LLM_URL = MLX_URL

# –ö—ç—à –¥–ª—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π (—á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∞—Ç—å —á–∞—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ /api/tags)
_models_cache = {"data": None, "timestamp": 0}
_MODELS_CACHE_TTL = 120  # 2 –º–∏–Ω—É—Ç—ã –∫—ç—à –¥–ª—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π

# –ö—ç—à –¥–ª—è —Å–∫—Ä—ã—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π (Dual-channel reasoning)
# –•—Ä–∞–Ω–∏—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è Summary Reader
_hidden_thoughts_cache = {}  # {session_id: [thoughts]}
_MAX_HIDDEN_CACHE_SIZE = 100


@dataclass
class ThinkingStep:
    """–û–¥–∏–Ω —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
    step_number: int
    thought: str
    conclusion: Optional[str] = None
    timestamp: datetime = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now(timezone.utc)


@dataclass
class ExtendedThinkingResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
    final_answer: str
    thinking_steps: List[ThinkingStep]
    total_tokens_used: int
    thinking_time_seconds: float
    confidence: float


class ExtendedThinkingEngine:
    """
    Extended Thinking Mode - –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º
    
    –ú–æ–¥–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ —Ä–∞—Å—Å—É–∂–¥–∞–µ—Ç –ø–æ—à–∞–≥–æ–≤–æ (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ),
    –∑–∞—Ç–µ–º —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π.
    """
    
    def __init__(
        self,
        model_name: str = "qwq:32b",  # –°–∞–º–∞—è –º–æ—â–Ω–∞—è reasoning –º–æ–¥–µ–ª—å –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è 70B/104B
        thinking_budget: int = 10000,  # –¢–æ–∫–µ–Ω—ã –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        max_steps: int = 10,
        use_intelligent_routing: bool = True  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥
    ):
        self.model_name = model_name  # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (fallback)
        self.use_intelligent_routing = use_intelligent_routing
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ MLX
        self.llm_url = DEFAULT_LLM_URL
        self.thinking_budget = thinking_budget
        self.max_steps = max_steps
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω
        if self.use_intelligent_routing:
            try:
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–ø–æ—Ä—Ç–∞
                try:
                    from app.intelligent_model_router import get_intelligent_router
                except ImportError:
                    try:
                        from intelligent_model_router import get_intelligent_router
                    except ImportError:
                        import sys
                        import os
                        router_path = os.path.join(os.path.dirname(__file__), 'intelligent_model_router.py')
                        if os.path.exists(router_path):
                            sys.path.insert(0, os.path.dirname(__file__))
                            from intelligent_model_router import get_intelligent_router
                        else:
                            raise ImportError("intelligent_model_router.py not found")
                
                self.model_router = get_intelligent_router()
                logger.info(f"‚úÖ ExtendedThinkingEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–º —Ä–æ—É—Ç–∏–Ω–≥–æ–º: URL={self.llm_url}, –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å={self.model_name}")
            except (ImportError, Exception) as e:
                logger.warning(f"‚ö†Ô∏è Intelligent router –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({e}), –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å")
                self.use_intelligent_routing = False
                self.model_router = None
        else:
            self.model_router = None
            logger.info(f"‚úÖ ExtendedThinkingEngine –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: URL={self.llm_url}, –º–æ–¥–µ–ª—å={self.model_name}")

    @classmethod
    def get_hidden_thoughts(cls, session_id: str) -> Optional[List[Dict]]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–∫—Ä—ã—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è —Å–µ—Å—Å–∏–∏ (Summary Reader)"""
        return _hidden_thoughts_cache.get(session_id)

    async def think(
        self,
        prompt: str,
        context: Optional[str] = None,
        use_iterative: bool = True,
        category: Optional[str] = None
    ) -> ExtendedThinkingResult:
        """
        –†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –∑–∞–¥–∞—á–∏
        
        Args:
            prompt: –ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            use_iterative: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å —Ñ–∏–Ω–∞–ª—å–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º –∏ —à–∞–≥–∞–º–∏ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        """
        start_time = datetime.now(timezone.utc)
        
        if use_iterative:
            return await self._iterative_thinking(prompt, context, category)
        else:
            return await self._single_pass_thinking(prompt, context, category)
    
    async def _get_available_models(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –∏–∑ MLX API Server —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        """
        global _models_cache
        import httpx
        import os
        import time
        
        current_time = time.time()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –µ—Å–ª–∏ –µ—â–µ –≤–∞–ª–∏–¥–µ–Ω
        if _models_cache["data"] and (current_time - _models_cache["timestamp"]) < _MODELS_CACHE_TTL:
            logger.debug(f"üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π ({len(_models_cache['data'])} –º–æ–¥–µ–ª–µ–π)")
            return _models_cache["data"]
        
        try:
            mlx_url = os.getenv('MLX_API_URL', 'http://localhost:11435')
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{mlx_url}/api/tags")
                if response.status_code == 200:
                    models_data = response.json()
                    models = models_data.get("models", [])
                    available = [m.get("name") for m in models if m.get("exists", True)]
                    logger.debug(f"üìã –î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π –≤ MLX: {len(available)}")
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –∫—ç—à
                    _models_cache = {"data": available, "timestamp": current_time}
                    return available
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π: {e}")
            # –ï—Å–ª–∏ –µ—Å—Ç—å –∫—ç—à, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –¥–∞–∂–µ –µ—Å–ª–∏ –∏—Å—Ç–µ–∫
            if _models_cache["data"]:
                logger.debug("üìã –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏")
                return _models_cache["data"]
        
        # Fallback: –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ PLAN.md
        # –í–ê–ñ–ù–û: tinyllama –∏—Å–∫–ª—é—á–µ–Ω–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
        # –¢—è–∂—ë–ª—ã–µ 70B/104B —É–¥–∞–ª–µ–Ω—ã –∏–∑-–∑–∞ Apple Silicon Metal limits
        fallback_models = [
            "qwq:32b",
            "qwen2.5-coder:32b",
            "phi3.5:3.8b",
            "phi3:mini-4k",
            "qwen2.5:3b"
        ]
        return fallback_models
    
    async def _iterative_thinking(
        self,
        prompt: str,
        context: Optional[Any] = None,
        category: Optional[str] = None
    ) -> ExtendedThinkingResult:
        """–ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - –Ω–µ—Å–∫–æ–ª—å–∫–æ —à–∞–≥–æ–≤"""
        thinking_steps = []
        current_understanding = ""
        start_time = datetime.now(timezone.utc)
        
        # –ù–∞—á–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
        ctx_str = context.get("kb_context") if isinstance(context, dict) else context
        thinking_prompt = self._build_thinking_prompt(prompt, ctx_str, step=1)
        
        for step_num in range(1, self.max_steps + 1):
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è
            step_thought = await self._generate_thinking_step(
                thinking_prompt,
                step_num,
                current_understanding,
                category
            )
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –º—ã—Å–ª—å –∏ –≤—ã–≤–æ–¥
            thought, conclusion = self._parse_thinking_step(step_thought)
            
            step = ThinkingStep(
                step_number=step_num,
                thought=thought,
                conclusion=conclusion
            )
            thinking_steps.append(step)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ
            current_understanding += f"\n–®–∞–≥ {step_num}: {thought}\n"
            if conclusion:
                current_understanding += f"–í—ã–≤–æ–¥: {conclusion}\n"
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –º–æ–∂–Ω–æ –ª–∏ –∑–∞–≤–µ—Ä—à–∏—Ç—å
            if conclusion and self._is_final_conclusion(conclusion):
                break
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞
            thinking_prompt = self._build_thinking_prompt(
                prompt, ctx_str, step=step_num + 1, previous_steps=current_understanding
            )
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π
        final_answer = await self._synthesize_final_answer(prompt, thinking_steps, category)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫—Ä—ã—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –¥–ª—è Summary Reader (Dual-channel)
        session_id = context.get("session_id") if isinstance(context, dict) else None
        if session_id:
            try:
                global _hidden_thoughts_cache
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
                if len(_hidden_thoughts_cache) >= _MAX_HIDDEN_CACHE_SIZE:
                    oldest_key = next(iter(_hidden_thoughts_cache))
                    _hidden_thoughts_cache.pop(oldest_key)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ü–µ–ø–æ—á–∫—É –º—ã—Å–ª–µ–π
                _hidden_thoughts_cache[session_id] = [
                    {"step": s.step_number, "thought": s.thought, "conclusion": s.conclusion}
                    for s in thinking_steps
                ]
                logger.info(f"üß† [DUAL-CHANNEL] –°–∫—Ä—ã—Ç—ã–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è —Å–µ—Å—Å–∏–∏ {session_id}")
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–∫—Ä—ã—Ç—ã—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π: {e}")

        elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
        
        return ExtendedThinkingResult(
            final_answer=final_answer,
            thinking_steps=thinking_steps,
            total_tokens_used=sum(len(s.thought) for s in thinking_steps) + len(final_answer),
            thinking_time_seconds=elapsed,
            confidence=self._calculate_confidence(thinking_steps)
        )
    
    async def _single_pass_thinking(
        self,
        prompt: str,
        context: Optional[Any] = None,
        category: Optional[str] = None
    ) -> ExtendedThinkingResult:
        """–û–¥–Ω–æ—à–∞–≥–æ–≤–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ - –≤—Å–µ —Å—Ä–∞–∑—É"""
        ctx_str = context.get("kb_context") if isinstance(context, dict) else context
        thinking_prompt = self._build_thinking_prompt(prompt, ctx_str, step=1)
        
        t_start = time.perf_counter()
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω–æ–µ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–µ
        full_thinking = await self._generate_response(thinking_prompt, max_tokens=self.thinking_budget, category=category)
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
        final_answer = await self._extract_final_answer(full_thinking, prompt)
        # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —à–∞–≥–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å –Ω—É–º–µ—Ä–∞—Ü–∏—è)
        thinking_steps = self._parse_thinking_into_steps(full_thinking)
        elapsed = time.perf_counter() - t_start
        
        return ExtendedThinkingResult(
            final_answer=final_answer,
            thinking_steps=thinking_steps,
            total_tokens_used=len(full_thinking) + len(final_answer),
            thinking_time_seconds=elapsed,
            confidence=self._calculate_confidence(thinking_steps),
        )
    
    def _build_thinking_prompt(
        self,
        prompt: str,
        context: Optional[str],
        step: int,
        previous_steps: Optional[str] = None
    ) -> str:
        """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        thinking_prompt = f"""–¢—ã —Ä–µ—à–∞–µ—à—å —Å–ª–æ–∂–Ω—É—é –∑–∞–¥–∞—á—É. –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥—É–º–∞–π –ø–æ—à–∞–≥–æ–≤–æ, –∑–∞—Ç–µ–º –¥–∞–π –æ—Ç–≤–µ—Ç.

–ó–ê–î–ê–ß–ê: {prompt}

"""
        
        if context:
            thinking_prompt += f"–ö–û–ù–¢–ï–ö–°–¢:\n{context}\n\n"
        
        if previous_steps:
            thinking_prompt += f"–ü–†–ï–î–´–î–£–©–ò–ï –®–ê–ì–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–Ø:\n{previous_steps}\n\n"
        
        if step == 1:
            thinking_prompt += """–ù–ê–ß–ù–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:

–®–∞–≥ 1. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∑–∞–¥–∞—á—É:
"""
        else:
            thinking_prompt += f"""–ü–†–û–î–û–õ–ñ–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:

–®–∞–≥ {step}. –ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤, —á—Ç–æ –¥–∞–ª—å—à–µ?
"""
        
        thinking_prompt += """
–§–æ—Ä–º–∞—Ç:
1. –ú—ã—Å–ª—å/–∞–Ω–∞–ª–∏–∑
2. –í—ã–≤–æ–¥ (–µ—Å–ª–∏ –≥–æ—Ç–æ–≤ –¥–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç, –Ω–∞–ø–∏—à–∏ "–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢: ...")

–¢–í–û–ï –†–ê–°–°–£–ñ–î–ï–ù–ò–ï:"""
        
        return thinking_prompt
    
    async def _generate_thinking_step(
        self,
        prompt: str,
        step_num: int,
        current_understanding: str,
        category: Optional[str] = None
    ) -> str:
        """–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω —à–∞–≥ —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è"""
        return await self._generate_response(prompt, max_tokens=1000, category=category)
    
    def _parse_thinking_step(self, step_text: str) -> Tuple[str, Optional[str]]:
        """–†–∞–∑–æ–±—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç —à–∞–≥–∞ –Ω–∞ –º—ã—Å–ª—å –∏ –≤—ã–≤–æ–¥"""
        thought = step_text.strip()
        conclusion = None
        
        if "–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:" in thought:
            parts = thought.split("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:", 1)
            thought = parts[0].strip()
            conclusion = parts[1].strip()
        
        return thought, conclusion
    
    def _is_final_conclusion(self, conclusion: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –≤—ã–≤–æ–¥ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º"""
        return conclusion is not None and len(conclusion) > 0
    
    async def _synthesize_final_answer(
        self,
        prompt: str,
        thinking_steps: List[ThinkingStep],
        category: Optional[str] = None
    ) -> str:
        """–°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Å–µ—Ö —à–∞–≥–æ–≤"""
        if thinking_steps and thinking_steps[-1].conclusion:
            return thinking_steps[-1].conclusion
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –≤—ã–≤–æ–¥–∞, –ø—Ä–æ—Å–∏–º –º–æ–¥–µ–ª—å —Å—É–º–º–∏—Ä–æ–≤–∞—Ç—å
        steps_text = "\n".join([f"–®–∞–≥ {s.step_number}: {s.thought}" for s in thinking_steps])
        
        synthesis_prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ —Ç–≤–æ–∏—Ö —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π, –¥–∞–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –∑–∞–¥–∞—á—É.

–ó–ê–î–ê–ß–ê: {prompt}

–¢–í–û–ò –†–ê–°–°–£–ñ–î–ï–ù–ò–Ø:
{steps_text}

–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:"""
        
        return await self._generate_response(synthesis_prompt, max_tokens=2000, category=category)
    
    def _calculate_confidence(self, thinking_steps: List[ThinkingStep]) -> float:
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ"""
        if not thinking_steps:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —á–µ–º –±–æ–ª—å—à–µ —à–∞–≥–æ–≤ (–¥–æ –ø—Ä–µ–¥–µ–ª–∞), —Ç–µ–º –≤—ã—à–µ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        # –¢–∞–∫–∂–µ –Ω–∞–ª–∏—á–∏–µ –≤—ã–≤–æ–¥–∞ –≤ –ø–æ—Å–ª–µ–¥–Ω–µ–º —à–∞–≥–µ –ø–æ–≤—ã—à–∞–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
        base_confidence = min(0.5 + (len(thinking_steps) * 0.05), 0.9)
        
        if thinking_steps[-1].conclusion:
            base_confidence += 0.1
            
        return min(base_confidence, 1.0)

    def _parse_thinking_into_steps(self, full_text: str) -> List[ThinkingStep]:
        """–†–∞–∑–±–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –Ω–∞ —à–∞–≥–∏"""
        import re
        steps = []
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "–®–∞–≥ 1", "1.", "Step 1"
        raw_steps = re.split(r'(?:–®–∞–≥|Step|Step\s*#)?\s*(\d+)[\.:\)]', full_text)
        
        if len(raw_steps) > 1:
            for i in range(1, len(raw_steps), 2):
                step_num = int(raw_steps[i])
                step_content = raw_steps[i+1].strip()
                
                thought, conclusion = self._parse_thinking_step(step_content)
                steps.append(ThinkingStep(
                    step_number=step_num,
                    thought=thought,
                    conclusion=conclusion
                ))
        else:
            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–±–∏—Ç—å, —Å—á–∏—Ç–∞–µ–º –≤—Å–µ –æ–¥–Ω–∏–º —à–∞–≥–æ–º
            thought, conclusion = self._parse_thinking_step(full_text)
            steps.append(ThinkingStep(
                step_number=1,
                thought=thought,
                conclusion=conclusion
            ))
            
        return steps

    async def _extract_final_answer(self, full_text: str, original_prompt: str) -> str:
        """–ò–∑–≤–ª–µ—á—å —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞"""
        if "–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:" in full_text:
            return full_text.split("–§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–í–ï–¢:", 1)[1].strip()
        
        # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–≥–æ –º–∞—Ä–∫–µ—Ä–∞, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π –∞–±–∑–∞—Ü
        paragraphs = [p.strip() for p in full_text.split('\n\n') if p.strip()]
        if paragraphs:
            return paragraphs[-1]
            
        return full_text

    async def _generate_response(
        self,
        prompt: str,
        max_tokens: int = 2000,
        category: Optional[str] = None
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ MLX API Server –∏–ª–∏ Ollama (fallback)
        """
        import httpx
        
        # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å
        model_to_use = self.model_name
        if self.use_intelligent_routing and self.model_router:
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏
                # –ü–µ—Ä–µ–¥–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–∑ MLX
                available_models = await self._get_available_models()
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∑–∞–¥–∞—á—É –µ—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –Ω–µ –∑–∞–¥–∞–Ω–∞
                task_category = category or self.model_router.classify_task(prompt)
                
                # –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
                optimal_model, _task_cat, confidence = await self.model_router.select_optimal_model(
                    prompt=prompt,
                    category=task_category,
                    available_models=available_models,
                    optimize_for='quality'  # –î–ª—è —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏–π –≤–∞–∂–Ω–æ –∫–∞—á–µ—Å—Ç–≤–æ
                )
                
                if optimal_model and confidence > 0.5:
                    model_to_use = optimal_model
                    logger.info(f"üß† [INTELLIGENT ROUTER] –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model_to_use} (confidence: {confidence:.2f})")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ä–æ—É—Ç–µ—Ä–∞: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å {model_to_use}")

        # –°–ø–∏—Å–æ–∫ URL –¥–ª—è –ø–æ–ø—ã—Ç–æ–∫ (—Å–Ω–∞—á–∞–ª–∞ MLX, –∑–∞—Ç–µ–º Ollama)
        urls_to_try = [self.llm_url]
        
        # –î–æ–±–∞–≤–ª—è–µ–º Ollama –∫–∞–∫ fallback
        ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
        if is_docker:
            ollama_url = os.getenv('OLLAMA_BASE_URL', 'http://host.docker.internal:11434')
        
        if ollama_url not in urls_to_try:
            urls_to_try.append(ollama_url)

        async with httpx.AsyncClient() as client:
            for llm_url in urls_to_try:
                try:
                    # –ß–∞—Ç —Å –í–∏–∫—Ç–æ—Ä–∏–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç HIGH
                    headers = {"X-Request-Priority": "high"}
                    
                    # –î–ª—è MLX API Server –∏—Å–ø–æ–ª—å–∑—É–µ–º /api/generate —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º category
                    # –î–ª—è Ollama –∏—Å–ø–æ–ª—å–∑—É–µ–º /api/generate —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º model
                    is_mlx = "11435" in llm_url or "mlx" in llm_url.lower()
                    
                    if is_mlx:
                        payload = {
                            "category": "reasoning",  # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é reasoning –¥–ª—è MLX
                            "prompt": prompt,
                            "stream": False,
                            "options": {
                                "temperature": 0.5,
                                "num_predict": max_tokens
                            }
                        }
                    else:
                        payload = {
                            "model": model_to_use,
                            "prompt": prompt,
                            "stream": False,
                            "options": {
                                "temperature": 0.5,
                                "num_predict": max_tokens
                            }
                        }
                    
                    response = await client.post(
                        f"{llm_url.rstrip('/')}/api/generate",
                        json=payload,
                        headers=headers,
                        timeout=300.0  # 5 –º–∏–Ω—É—Ç –Ω–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
                    )
                    
                    if response.status_code == 200:
                        data = response.json()
                        return data.get('response', '')
                    elif response.status_code == 429:
                        # Rate limit - –ø—Ä–æ–±—É–µ–º –ø–æ–¥–æ–∂–¥–∞—Ç—å –∏–ª–∏ –¥—Ä—É–≥–æ–π —Å–µ—Ä–≤–µ—Ä
                        logger.warning(f"‚ö†Ô∏è [RATE LIMIT] –°–µ—Ä–≤–µ—Ä {llm_url} –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω (429)")
                        if llm_url == urls_to_try[-1]:
                            # –ï—Å–ª–∏ —ç—Ç–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ—Ä–≤–µ—Ä, –∂–¥–µ–º –∏ –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
                            await asyncio.sleep(5)
                            # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –≤—ã–∑–æ–≤ (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ —Å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–π —Ä–µ–∫—É—Ä—Å–∏–µ–π)
                            # return await self._generate_response(prompt, max_tokens, category)
                        continue
                    elif response.status_code >= 500:
                        logger.warning(f"‚ö†Ô∏è [SERVER ERROR] –û—à–∏–±–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞ {llm_url}: {response.status_code}")
                        continue
                    elif response.status_code == 404:
                        # –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–±—É–µ–º fallback –º–æ–¥–µ–ª–∏
                        logger.warning(f"‚ö†Ô∏è [NOT FOUND] –ú–æ–¥–µ–ª—å {model_to_use} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –Ω–∞ {llm_url}")
                        continue
                        
                except Exception as e:
                    logger.debug(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ {llm_url}: {e}")
                    continue
            
            logger.error(f"‚ùå –í—Å–µ LLM –±—ç–∫–µ–Ω–¥—ã –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
            return ""
