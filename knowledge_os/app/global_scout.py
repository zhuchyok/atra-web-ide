"""
[KNOWLEDGE OS] Global Scout Engine.
Integration with external APIs to validate knowledge relevance.
Part of the ATRA Singularity framework.
"""

import asyncio
import getpass
import json
import logging
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Any, Dict, List, Optional

# Third-party imports with fallback
try:
    import aiohttp
    AIOHTTP_AVAILABLE = True
except ImportError:
    aiohttp = None
    AIOHTTP_AVAILABLE = False

try:
    import asyncpg
    ASYNCPG_AVAILABLE = True
except ImportError:
    asyncpg = None
    ASYNCPG_AVAILABLE = False

# Local project imports with fallback
try:
    from ai_core import run_smart_agent_async
except ImportError:
    async def run_smart_agent_async(prompt, **kwargs):  # pylint: disable=unused-argument
        """Fallback for run_smart_agent_async."""
        return None

try:
    from semantic_cache import SemanticAICache
except ImportError:
    class SemanticAICache:
        """Fallback for SemanticAICache."""
        async def save_to_cache(self, *args, **kwargs): pass

logger = logging.getLogger(__name__)

USER_NAME = getpass.getuser()
# Priority: 1. env var, 2. local user (Mac), 3. fallback to admin (Server)
if USER_NAME == 'zhuchyok':
    DEFAULT_DB_URL = f'postgresql://{USER_NAME}@localhost:5432/knowledge_os'
else:
    DEFAULT_DB_URL = 'postgresql://admin:secret@localhost:5432/knowledge_os'

DB_URL = os.getenv('DATABASE_URL', DEFAULT_DB_URL)

# API Keys (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ¸Ñ‚ÑŒ Ğ² .env)
GITHUB_TOKEN = os.getenv('GITHUB_TOKEN', '')
STACK_OVERFLOW_KEY = os.getenv('STACK_OVERFLOW_KEY', '')


@dataclass
class ExternalValidation:
    """Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ğ¸Ğ· Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ñ… Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²"""
    source: str  # 'github', 'stackoverflow', 'arxiv', 'hackernews'
    relevance_score: float  # 0.0 - 1.0
    confidence: float  # 0.0 - 1.0
    evidence: str  # Ğ¡ÑÑ‹Ğ»ĞºĞ° Ğ¸Ğ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ°
    timestamp: datetime
    metadata: Dict[str, Any]


class GitHubScout:
    """Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ GitHub API Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ best practices"""

    BASE_URL = "https://api.github.com"

    def __init__(self, token: Optional[str] = None):
        self.token = token or GITHUB_TOKEN
        self.headers = {
            "Accept": "application/vnd.github.v3+json",
        }
        if self.token:
            self.headers["Authorization"] = f"token {self.token}"

    async def search_repositories(self, query: str, limit: int = 5) -> List[Dict]:
        """ĞŸĞ¾Ğ¸ÑĞº Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ² Ğ¿Ğ¾ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑÑƒ"""
        if not AIOHTTP_AVAILABLE:
            return []

        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.BASE_URL}/search/repositories"
                params = {
                    "q": query,
                    "sort": "stars",
                    "order": "desc",
                    "per_page": limit
                }
                async with session.get(url, headers=self.headers, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("items", [])

                    logger.warning("GitHub API error: %d", response.status)
                    return []
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error("GitHub search error: %s", exc)
            return []

    async def validate_knowledge(self, knowledge_content: str, domain: str) -> ExternalValidation:
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· GitHub"""
        # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸Ğ· Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ
        keywords = self._extract_keywords(knowledge_content, domain)
        query = " ".join(keywords[:3])  # ĞŸĞµÑ€Ğ²Ñ‹Ğµ 3 ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²Ğ°

        repositories = await self.search_repositories(query, limit=5)

        if not repositories:
            return ExternalValidation(
                source="github",
                relevance_score=0.0,
                confidence=0.0,
                evidence="No repositories found",
                timestamp=datetime.now(),
                metadata={}
            )

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ğ¾Ğ¿ÑƒĞ»ÑÑ€Ğ½Ğ¾ÑÑ‚Ğ¸ Ñ€ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ¾Ñ€Ğ¸ĞµĞ²
        total_stars = sum(r.get("stargazers_count", 0) for r in repositories)
        avg_stars = total_stars / len(repositories) if repositories else 0

        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ score (0-1)
        relevance_score = min(1.0, avg_stars / 10000.0)  # 10k stars = 1.0

        return ExternalValidation(
            source="github",
            relevance_score=relevance_score,
            confidence=0.8 if repositories else 0.0,
            evidence=f"Found {len(repositories)} repositories, avg {avg_stars:.0f} stars",
            timestamp=datetime.now(),
            metadata={
                "repositories": [
                    {
                        "name": r.get("full_name"),
                        "stars": r.get("stargazers_count"),
                        "url": r.get("html_url")
                    }
                    for r in repositories[:3]
                ]
            }
        )

    def _extract_keywords(self, content: str, domain: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ² Ğ¸Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğ°"""
        # ĞŸÑ€Ğ¾ÑÑ‚Ğ°Ñ ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ Ñ NLP)
        words = content.lower().split()
        # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ÑƒĞµĞ¼ ÑÑ‚Ğ¾Ğ¿-ÑĞ»Ğ¾Ğ²Ğ°
        stop_words = {
            "the", "a", "an", "is", "are", "was", "were",
            "be", "been", "to", "of", "and", "or"
        }
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ´Ğ¾Ğ¼ĞµĞ½
        if domain:
            keywords.insert(0, domain.lower())
        return keywords[:10]


class StackOverflowScout:
    """Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ Stack Overflow API Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ñ€ĞµÑˆĞµĞ½Ğ¸Ğ¹"""

    BASE_URL = "https://api.stackexchange.com/2.3"

    def __init__(self, key: Optional[str] = None):
        self.key = key or STACK_OVERFLOW_KEY

    async def search_questions(self, query: str, limit: int = 5) -> List[Dict]:
        """ĞŸĞ¾Ğ¸ÑĞº Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ¾Ğ² Ğ½Ğ° Stack Overflow"""
        if not AIOHTTP_AVAILABLE:
            return []

        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.BASE_URL}/search/advanced"
                params = {
                    "q": query,
                    "order": "desc",
                    "sort": "votes",
                    "site": "stackoverflow",
                    "pagesize": limit
                }
                if self.key:
                    params["key"] = self.key

                async with session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        return data.get("items", [])

                    logger.warning("Stack Overflow API error: %d", response.status)
                    return []
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error("Stack Overflow search error: %s", exc)
            return []

    async def validate_knowledge(self, knowledge_content: str, domain: str) -> ExternalValidation:
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Stack Overflow"""
        keywords = self._extract_keywords(knowledge_content, domain)
        query = " ".join(keywords[:3])

        questions = await self.search_questions(query, limit=5)

        if not questions:
            return ExternalValidation(
                source="stackoverflow",
                relevance_score=0.0,
                confidence=0.0,
                evidence="No questions found",
                timestamp=datetime.now(),
                metadata={}
            )

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ³Ğ¾Ğ»Ğ¾ÑĞ¾Ğ²
        total_votes = sum(q.get("score", 0) for q in questions)
        avg_votes = total_votes / len(questions) if questions else 0

        # ĞĞ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·ÑƒĞµĞ¼ score (0-1)
        relevance_score = min(1.0, avg_votes / 100.0)  # 100 votes = 1.0

        return ExternalValidation(
            source="stackoverflow",
            relevance_score=relevance_score,
            confidence=0.7 if questions else 0.0,
            evidence=f"Found {len(questions)} questions, avg {avg_votes:.1f} votes",
            timestamp=datetime.now(),
            metadata={
                "questions": [
                    {
                        "title": q.get("title"),
                        "votes": q.get("score"),
                        "answers": q.get("answer_count", 0),
                        "url": q.get("link")
                    }
                    for q in questions[:3]
                ]
            }
        )

    def _extract_keywords(self, content: str, domain: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²"""
        words = content.lower().split()
        stop_words = {
            "the", "a", "an", "is", "are", "was", "were",
            "be", "been", "to", "of", "and", "or"
        }
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        if domain:
            keywords.insert(0, domain.lower())
        return keywords[:10]


class ArxivScout:
    """Ğ˜Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ñ Ñ arXiv API Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¹"""

    BASE_URL = "http://export.arxiv.org/api/query"

    async def search_papers(self, query: str, limit: int = 5) -> List[Dict]:
        """ĞŸĞ¾Ğ¸ÑĞº ÑÑ‚Ğ°Ñ‚ĞµĞ¹ Ğ½Ğ° arXiv"""
        if not AIOHTTP_AVAILABLE:
            return []

        try:
            async with aiohttp.ClientSession() as session:
                params = {
                    "search_query": f"all:{query}",
                    "start": 0,
                    "max_results": limit,
                    "sortBy": "relevance",
                    "sortOrder": "descending"
                }
                async with session.get(self.BASE_URL, params=params) as response:
                    if response.status == 200:
                        text = await response.text()
                        # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ XML (Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ ÑƒĞ»ÑƒÑ‡ÑˆĞ¸Ñ‚ÑŒ)
                        # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½ÑƒĞ¶ĞµĞ½ XML Ğ¿Ğ°Ñ€ÑĞµÑ€
                        return self._parse_arxiv_response(text)

                    logger.warning("arXiv API error: %d", response.status)
                    return []
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error("arXiv search error: %s", exc)
            return []

    def _parse_arxiv_response(self, _xml_text: str) -> List[Dict]:
        """ĞŸÑ€Ğ¾ÑÑ‚Ğ¾Ğ¹ Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ° arXiv (ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹)"""
        # Ğ’ Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ½ÑƒĞ¶ĞµĞ½ XML Ğ¿Ğ°Ñ€ÑĞµÑ€ (xml.etree.ElementTree)
        # Ğ—Ğ´ĞµÑÑŒ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ´Ğ»Ñ ÑƒĞ¿Ñ€Ğ¾Ñ‰ĞµĞ½Ğ¸Ñ
        return []

    async def validate_knowledge(self, knowledge_content: str, domain: str) -> ExternalValidation:
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· arXiv"""
        keywords = self._extract_keywords(knowledge_content, domain)
        query = " ".join(keywords[:3])

        papers = await self.search_papers(query, limit=5)

        return ExternalValidation(
            source="arxiv",
            relevance_score=0.5 if papers else 0.0,
            confidence=0.6 if papers else 0.0,
            evidence=f"Found {len(papers)} papers" if papers else "No papers found",
            timestamp=datetime.now(),
            metadata={"papers": papers}
        )

    def _extract_keywords(self, content: str, domain: str) -> List[str]:
        """Ğ˜Ğ·Ğ²Ğ»ĞµÑ‡ĞµĞ½Ğ¸Ğµ ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… ÑĞ»Ğ¾Ğ²"""
        words = content.lower().split()
        stop_words = {
            "the", "a", "an", "is", "are", "was", "were",
            "be", "been", "to", "of", "and", "or"
        }
        keywords = [w for w in words if len(w) > 3 and w not in stop_words]
        if domain:
            keywords.insert(0, domain.lower())
        return keywords[:10]


class GlobalScout:
    """Ğ“Ğ»Ğ°Ğ²Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ´Ğ»Ñ Ğ¸Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ğ¸Ğ¸ ÑĞ¾ Ğ²ÑĞµĞ¼Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¼Ğ¸ API"""

    def __init__(self):
        self.github = GitHubScout()
        self.stackoverflow = StackOverflowScout()
        self.arxiv = ArxivScout()

    async def validate_knowledge_node(
        self,
        knowledge_id: int,
        content: str,
        domain: str
    ) -> Dict[str, Any]:
        """Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ ÑƒĞ·Ğ»Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞµ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸"""
        logger.info("ğŸ” Validating knowledge node %d via external APIs...", knowledge_id)

        # ĞŸĞ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ğ°Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ²ÑĞµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸
        validations = await asyncio.gather(
            self.github.validate_knowledge(content, domain),
            self.stackoverflow.validate_knowledge(content, domain),
            self.arxiv.validate_knowledge(content, domain),
            return_exceptions=True
        )

        # ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²
        results = []
        for validation in validations:
            if isinstance(validation, Exception):
                logger.error("Validation error: %s", validation)
                continue
            results.append({
                "source": validation.source,
                "relevance_score": validation.relevance_score,
                "confidence": validation.confidence,
                "evidence": validation.evidence,
                "timestamp": validation.timestamp.isoformat(),
                "metadata": validation.metadata
            })

        # Ğ’Ñ‹Ñ‡Ğ¸ÑĞ»ÑĞµĞ¼ Ğ¾Ğ±Ñ‰Ğ¸Ğ¹ score
        if results:
            avg_relevance = sum(r["relevance_score"] for r in results) / len(results)
            avg_confidence = sum(r["confidence"] for r in results) / len(results)
        else:
            avg_relevance = 0.0
            avg_confidence = 0.0

        return {
            "knowledge_id": str(knowledge_id),
            "overall_relevance": avg_relevance,
            "overall_confidence": avg_confidence,
            "validations": results,
            "validated_at": datetime.now().isoformat()
        }

    async def update_knowledge_validation(self, conn, knowledge_id: int, validation_result: Dict):
        """ĞĞ±Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ğ¸Ğµ ÑƒĞ·Ğ»Ğ° Ğ·Ğ½Ğ°Ğ½Ğ¸Ñ Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ°Ğ¼Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸"""
        if not ASYNCPG_AVAILABLE:
            return

        try:
            # ĞŸĞ¾Ğ»ÑƒÑ‡Ğ°ĞµĞ¼ Ñ‚ĞµĞºÑƒÑ‰Ğ¸Ğµ metadata
            current_metadata = await conn.fetchval(
                "SELECT metadata FROM knowledge_nodes WHERE id = $1",
                knowledge_id
            )

            if current_metadata is None:
                current_metadata = {}
            elif isinstance(current_metadata, str):
                current_metadata = json.loads(current_metadata)

            # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸
            current_metadata["external_validation"] = validation_result
            current_metadata["last_validated"] = datetime.now().isoformat()

            # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ‘Ğ”
            await conn.execute(
                "UPDATE knowledge_nodes SET metadata = $1 WHERE id = $2",
                json.dumps(current_metadata),
                knowledge_id
            )

            logger.info("âœ… Updated validation for knowledge node %d", knowledge_id)
        except Exception as exc:  # pylint: disable=broad-exception-caught
            logger.error("Error updating validation: %s", exc)


class PredictiveSynthesisScout:
    """Proactive knowledge synthesis based on emerging trends (Singularity v3.0)."""

    def __init__(self, db_url: str = DB_URL):
        self.db_url = db_url
        self.cache = SemanticAICache()

    async def run_predictive_cycle(self):
        """Monitor trends and pre-cache potential user queries."""
        if not ASYNCPG_AVAILABLE:
            return

        logger.info("ğŸ”­ Predictive Scout: Monitoring future trends...")

        # 1. Fetch recent knowledge gaps or news-like nodes
        conn = await asyncpg.connect(self.db_url)
        nodes = await conn.fetch("""
            SELECT content, d.name as domain
            FROM knowledge_nodes k
            JOIN domains d ON k.domain_id = d.id
            WHERE k.created_at > NOW() - INTERVAL '24 hours'
            ORDER BY RANDOM() LIMIT 3
        """)
        await conn.close()

        if not nodes:
            return

        for node in nodes:
            # 2. Synthesize a "Future Question"
            prompt = (
                "Ğ’Ğ« - Ğ“Ğ›ĞĞ‘ĞĞ›Ğ¬ĞĞ«Ğ™ Ğ¡ĞšĞ ĞĞ£Ğ¢ ATRA. "
                f"ĞĞ ĞĞ¡ĞĞĞ’Ğ• ĞĞĞ’ĞĞ“Ğ Ğ—ĞĞĞĞ˜Ğ¯: \"{node['content']}\" (Ğ”Ğ¾Ğ¼ĞµĞ½: {node['domain']})\n\n"
                "Ğ—ĞĞ”ĞĞ§Ğ: ĞŸÑ€ĞµĞ´ÑĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ 1 Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğ¹ Ğ’Ğ»Ğ°Ğ´ĞµĞ»ĞµÑ† Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ñ‚ÑŒ Ğ² Ğ±Ğ»Ğ¸Ğ¶Ğ°Ğ¹ÑˆĞµĞ¼ Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼ "
                "Ğ² ÑĞ²ÑĞ·Ğ¸ Ñ ÑÑ‚Ğ¸Ğ¼ Ñ‚Ñ€ĞµĞ½Ğ´Ğ¾Ğ¼. Ğ”Ğ°Ğ¹Ñ‚Ğµ Ğ¸Ğ´ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºÑĞ¿ĞµÑ€Ñ‚Ğ½Ñ‹Ğ¹ Ğ¾Ñ‚Ğ²ĞµÑ‚.\n\n"
                "Ğ’Ğ•Ğ ĞĞ˜Ğ¢Ğ• JSON:\n"
                "{\n"
                "    \"predicted_query\": \"Ğ’Ğ¾Ğ¿Ñ€Ğ¾Ñ...\",\n"
                "    \"expert_response\": \"ĞÑ‚Ğ²ĞµÑ‚...\"\n"
                "}"
            )

            response = await run_smart_agent_async(
                prompt,
                expert_name="Ğ’Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¸Ñ",
                category="predictive_scout"
            )

            if not response:
                continue

            try:
                if "```json" in response:
                    response = response.split("```json")[1].split("```")[0]
                elif "```" in response:
                    response = response.split("```")[1].split("```")[0]

                data = json.loads(response)

                # 3. Pre-cache the result
                await self.cache.save_to_cache(
                    data['predicted_query'],
                    data['expert_response'],
                    "Ğ’Ğ¸ĞºÑ‚Ğ¾Ñ€Ğ¸Ñ"
                )
                logger.info("âœ¨ Pre-cached future query: %s", data['predicted_query'])

            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Predictive synthesis error: %s", exc)


async def run_global_scout_cycle():
    """ĞÑĞ½Ğ¾Ğ²Ğ½Ğ¾Ğ¹ Ñ†Ğ¸ĞºĞ» Global Scout Ğ´Ğ»Ñ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ğ¸ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğ¹ Ğ¸ Ğ¿Ñ€Ğµ-ĞºÑÑˆĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"""
    if not ASYNCPG_AVAILABLE:
        logger.error("âŒ asyncpg is not installed. Global Scout aborted.")
        return

    logger.info("ğŸŒ Global Scout: Starting validation cycle...")

    conn = await asyncpg.connect(DB_URL)
    scout = GlobalScout()
    predictive = PredictiveSynthesisScout()

    try:
        # Run predictive pre-caching
        await predictive.run_predictive_cycle()

        # (Existing validation code...)
        # (Ñ‚Ğµ, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ ĞµÑ‰Ğµ Ğ½Ğµ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ¸Ğ»Ğ¸ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»Ğ¸ÑÑŒ Ğ´Ğ°Ğ²Ğ½Ğ¾)
        knowledge_nodes = await conn.fetch("""
            SELECT k.id, k.content, d.name as domain
            FROM knowledge_nodes k
            JOIN domains d ON k.domain_id = d.id
            WHERE k.metadata->>'last_validated' IS NULL
               OR (k.metadata->>'last_validated')::timestamp < NOW() - INTERVAL '30 days'
            ORDER BY k.created_at DESC
            LIMIT 10
        """)

        if not knowledge_nodes:
            logger.info("No knowledge nodes to validate")
            return

        logger.info("Found %d knowledge nodes to validate", len(knowledge_nodes))

        # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ¸Ñ€ÑƒĞµĞ¼ ĞºĞ°Ğ¶Ğ´Ğ¾Ğµ Ğ·Ğ½Ğ°Ğ½Ğ¸Ğµ
        for node in knowledge_nodes:
            try:
                validation_result = await scout.validate_knowledge_node(
                    node["id"],
                    node["content"],
                    node["domain"]
                )

                # ĞĞ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼ Ğ² Ğ‘Ğ”
                await scout.update_knowledge_validation(conn, node["id"], validation_result)

                # ĞĞµĞ±Ğ¾Ğ»ÑŒÑˆĞ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸ (rate limiting)
                await asyncio.sleep(1)
            except Exception as exc:  # pylint: disable=broad-exception-caught
                logger.error("Error validating node %d: %s", node['id'], exc)

        logger.info("âœ… Global Scout cycle completed")

    except Exception as exc:  # pylint: disable=broad-exception-caught
        logger.error("Global Scout error: %s", exc)
    finally:
        await conn.close()


if __name__ == "__main__":
    asyncio.run(run_global_scout_cycle())
