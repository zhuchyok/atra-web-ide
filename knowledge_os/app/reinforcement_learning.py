"""
Reinforcement Learning Framework –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤
Self-reward —Å–∏—Å—Ç–µ–º–∞, policy optimization, adaptive behavior
"""

import os
import asyncio
import logging
import json
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime, timezone
from collections import defaultdict
import random

logger = logging.getLogger(__name__)

OLLAMA_URL = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')


@dataclass
class Action:
    """–î–µ–π—Å—Ç–≤–∏–µ –∞–≥–µ–Ω—Ç–∞"""
    action_id: str
    action_type: str
    parameters: Dict[str, Any]
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


@dataclass
class Reward:
    """–ù–∞–≥—Ä–∞–¥–∞ –∑–∞ –¥–µ–π—Å—Ç–≤–∏–µ"""
    reward_id: str
    action_id: str
    reward_value: float  # -1.0 –¥–æ 1.0
    reward_type: str  # "success", "failure", "quality", "efficiency"
    feedback: Optional[str] = None
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


@dataclass
class Policy:
    """–ü–æ–ª–∏—Ç–∏–∫–∞ –∞–≥–µ–Ω—Ç–∞"""
    policy_id: str
    agent_name: str
    state_action_values: Dict[str, Dict[str, float]]  # state -> action -> value
    learning_rate: float = 0.1
    discount_factor: float = 0.9
    exploration_rate: float = 0.1
    updated_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


class ReinforcementLearning:
    """–§—Ä–µ–π–º–≤–æ—Ä–∫ Reinforcement Learning –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤"""
    
    def __init__(self, agent_name: str = "Victoria"):
        self.agent_name = agent_name
        self.policies: Dict[str, Policy] = {}
        self.action_history: List[Action] = []
        self.reward_history: List[Reward] = []
        self.episodes: List[Dict[str, Any]] = []
        
        # Q-learning –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        self.learning_rate = float(os.getenv("RL_LEARNING_RATE", "0.1"))
        self.discount_factor = float(os.getenv("RL_DISCOUNT_FACTOR", "0.9"))
        self.exploration_rate = float(os.getenv("RL_EXPLORATION_RATE", "0.1"))
    
    def _get_state_key(self, context: Dict[str, Any]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –∫–ª—é—á —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        task_type = context.get("task_type", "general")
        complexity = context.get("complexity", "medium")
        return f"{task_type}:{complexity}"
    
    def _get_policy(self, state: str) -> Policy:
        """–ü–æ–ª—É—á–∏—Ç—å –∏–ª–∏ —Å–æ–∑–¥–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if state not in self.policies:
            self.policies[state] = Policy(
                policy_id=f"policy_{state}_{self.agent_name}",
                agent_name=self.agent_name,
                state_action_values={state: {}},
                learning_rate=self.learning_rate,
                discount_factor=self.discount_factor,
                exploration_rate=self.exploration_rate
            )
        return self.policies[state]
    
    async def select_action(
        self,
        state: str,
        available_actions: List[str],
        context: Dict[str, Any] = None
    ) -> str:
        """
        –í—ã–±—Ä–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏–µ –∏—Å–ø–æ–ª—å–∑—É—è policy (epsilon-greedy)
        
        Args:
            state: –¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            available_actions: –î–æ—Å—Ç—É–ø–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        
        Returns:
            –í—ã–±—Ä–∞–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        """
        if not available_actions:
            return None
        
        policy = self._get_policy(state)
        
        # Epsilon-greedy: exploration vs exploitation
        if random.random() < policy.exploration_rate:
            # Exploration: —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            action = random.choice(available_actions)
            logger.debug(f"üîç Exploration: –≤—ã–±—Ä–∞–Ω–æ —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ {action}")
        else:
            # Exploitation: –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ –ø–æ Q-values
            state_values = policy.state_action_values.get(state, {})
            if state_values:
                # –í—ã–±–∏—Ä–∞–µ–º –¥–µ–π—Å—Ç–≤–∏–µ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º Q-value
                action = max(available_actions, key=lambda a: state_values.get(a, 0.0))
                logger.debug(f"üéØ Exploitation: –≤—ã–±—Ä–∞–Ω–æ –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ {action}")
            else:
                # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö - —Å–ª—É—á–∞–π–Ω–æ–µ
                action = random.choice(available_actions)
                logger.debug(f"‚ùì –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö, —Å–ª—É—á–∞–π–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ {action}")
        
        return action
    
    async def record_action(
        self,
        action_type: str,
        parameters: Dict[str, Any],
        state: str
    ) -> Action:
        """–ó–∞–ø–∏—Å–∞—Ç—å –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ"""
        action_id = f"action_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        
        action = Action(
            action_id=action_id,
            action_type=action_type,
            parameters=parameters
        )
        
        self.action_history.append(action)
        return action
    
    async def assign_reward(
        self,
        action_id: str,
        reward_value: float,
        reward_type: str = "success",
        feedback: Optional[str] = None
    ) -> Reward:
        """
        –ù–∞–∑–Ω–∞—á–∏—Ç—å –Ω–∞–≥—Ä–∞–¥—É –∑–∞ –¥–µ–π—Å—Ç–≤–∏–µ
        
        Args:
            action_id: ID –¥–µ–π—Å—Ç–≤–∏—è
            reward_value: –ó–Ω–∞—á–µ–Ω–∏–µ –Ω–∞–≥—Ä–∞–¥—ã (-1.0 –¥–æ 1.0)
            reward_type: –¢–∏–ø –Ω–∞–≥—Ä–∞–¥—ã
            feedback: –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å
        
        Returns:
            Reward –æ–±—ä–µ–∫—Ç
        """
        reward_id = f"reward_{datetime.now().strftime('%Y%m%d_%H%M%S_%f')}"
        
        reward = Reward(
            reward_id=reward_id,
            action_id=action_id,
            reward_value=reward_value,
            reward_type=reward_type,
            feedback=feedback
        )
        
        self.reward_history.append(reward)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º Q-values
        await self._update_q_values(action_id, reward_value)
        
        logger.info(f"üéÅ –ù–∞–≥—Ä–∞–¥–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∞: {reward_value:.2f} ({reward_type})")
        
        return reward
    
    async def _update_q_values(self, action_id: str, reward: float):
        """–û–±–Ω–æ–≤–∏—Ç—å Q-values –∏—Å–ø–æ–ª—å–∑—É—è Q-learning"""
        # –ù–∞—Ö–æ–¥–∏–º –¥–µ–π—Å—Ç–≤–∏–µ
        action = next((a for a in self.action_history if a.action_id == action_id), None)
        if not action:
            return
        
        # –ù–∞—Ö–æ–¥–∏–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ (—É–ø—Ä–æ—â–µ–Ω–Ω–æ - –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è)
        if len(self.action_history) < 2:
            return
        
        prev_action = self.action_history[-2]
        state = self._get_state_key(prev_action.parameters)
        
        policy = self._get_policy(state)
        action_type = action.action_type
        
        # Q-learning update: Q(s,a) = Q(s,a) + Œ±[r + Œ≥*max(Q(s',a')) - Q(s,a)]
        current_q = policy.state_action_values.get(state, {}).get(action_type, 0.0)
        
        # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ Q-value –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (—É–ø—Ä–æ—â–µ–Ω–Ω–æ)
        next_state_values = policy.state_action_values.get(state, {})
        max_next_q = max(next_state_values.values()) if next_state_values else 0.0
        
        # –û–±–Ω–æ–≤–ª—è–µ–º Q-value
        new_q = current_q + policy.learning_rate * (
            reward + policy.discount_factor * max_next_q - current_q
        )
        
        if state not in policy.state_action_values:
            policy.state_action_values[state] = {}
        policy.state_action_values[state][action_type] = new_q
        
        policy.updated_at = datetime.now(timezone.utc)
        
        logger.debug(f"üìà Q-value –æ–±–Ω–æ–≤–ª–µ–Ω: {state}:{action_type} = {new_q:.3f}")
    
    async def self_reward(
        self,
        action_id: str,
        result: Any,
        expected_result: Optional[Any] = None
    ) -> Reward:
        """
        –°–∞–º–æ–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        
        Args:
            action_id: ID –¥–µ–π—Å—Ç–≤–∏—è
            result: –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            expected_result: –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–µ—Å–ª–∏ –µ—Å—Ç—å)
        
        Returns:
            Reward –æ–±—ä–µ–∫—Ç
        """
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Å–∞–º–æ–Ω–∞–≥—Ä–∞–∂–¥–µ–Ω–∏—è
        reward_value = 0.5  # –ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        
        # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —É—Å–ø–µ—à–Ω—ã–π
        if result and not isinstance(result, Exception):
            reward_value = 0.8
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
        if expected_result:
            if result == expected_result:
                reward_value = 1.0
            else:
                reward_value = 0.3
        
        # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞
        if isinstance(result, Exception):
            reward_value = -0.5
        
        return await self.assign_reward(
            action_id=action_id,
            reward_value=reward_value,
            reward_type="self_reward"
        )
    
    async def optimize_policy(self, state: str) -> Dict[str, Any]:
        """
        –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª–∏—Ç–∏–∫—É –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏—è
        
        Args:
            state: –°–æ—Å—Ç–æ—è–Ω–∏–µ
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        """
        policy = self._get_policy(state)
        state_values = policy.state_action_values.get(state, {})
        
        if not state_values:
            return {"message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"}
        
        # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–µ–µ –¥–µ–π—Å—Ç–≤–∏–µ
        best_action = max(state_values.items(), key=lambda x: x[1])
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π
        avg_value = sum(state_values.values()) / len(state_values)
        max_value = max(state_values.values())
        min_value = min(state_values.values())
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        if max_value - min_value < 0.1:
            recommendations.append("–ù–∏–∑–∫–∞—è –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞—Ü–∏—è - —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª—å—à–µ exploration")
        if policy.exploration_rate > 0.2:
            recommendations.append("–í—ã—Å–æ–∫–∏–π exploration rate - –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å")
        
        return {
            "state": state,
            "best_action": best_action[0],
            "best_value": best_action[1],
            "average_value": avg_value,
            "value_range": max_value - min_value,
            "total_actions": len(state_values),
            "recommendations": recommendations
        }
    
    def get_statistics(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è"""
        total_actions = len(self.action_history)
        total_rewards = len(self.reward_history)
        
        if total_rewards == 0:
            return {"message": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"}
        
        avg_reward = sum(r.reward_value for r in self.reward_history) / total_rewards
        positive_rewards = sum(1 for r in self.reward_history if r.reward_value > 0)
        success_rate = positive_rewards / total_rewards if total_rewards > 0 else 0
        
        return {
            "total_actions": total_actions,
            "total_rewards": total_rewards,
            "average_reward": avg_reward,
            "success_rate": success_rate,
            "policies_count": len(self.policies),
            "exploration_rate": self.exploration_rate
        }

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ —ç–∫–∑–µ–º–ø–ª—è—Ä—ã –ø–æ –∞–≥–µ–Ω—Ç–∞–º
_rl_instances: Dict[str, ReinforcementLearning] = {}

def get_rl(agent_name: str = "Victoria") -> ReinforcementLearning:
    """–ü–æ–ª—É—á–∏—Ç—å —ç–∫–∑–µ–º–ø–ª—è—Ä RL –¥–ª—è –∞–≥–µ–Ω—Ç–∞"""
    if agent_name not in _rl_instances:
        _rl_instances[agent_name] = ReinforcementLearning(agent_name=agent_name)
    return _rl_instances[agent_name]
