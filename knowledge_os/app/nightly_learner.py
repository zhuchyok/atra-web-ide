import asyncio
import logging
import os
import sys
import json
import asyncpg
import subprocess
from datetime import datetime, timedelta
import time
import random
from typing import Optional
from resource_manager import acquire_resource_lock

logger = logging.getLogger(__name__)


def _log_step(msg: str) -> None:
    """–ü–µ—á–∞—Ç—å –∏ —Å–±—Ä–æ—Å –±—É—Ñ–µ—Ä–∞ ‚Äî –ø—Ä–∏ OOM/Killed –≤ –ª–æ–≥–∞—Ö –±—É–¥–µ—Ç –≤–∏–¥–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥."""
    print(msg)
    sys.stdout.flush()
    sys.stderr.flush()


# –ü—É—Ç–∏ –¥–ª—è Mac Studio –∏ Linux (–±–µ–∑ /root/)
_APP_DIR = os.path.dirname(os.path.abspath(__file__))
_KNOWLEDGE_OS_ROOT = os.path.dirname(_APP_DIR)
from contextual_learner import ContextualMemory, AdaptiveLearner, PersonalizationEngine, NeedPredictor

DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')


def _node_type(url: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —É–∑–ª–∞ –ø–æ URL: ollama (11434) –∏–ª–∏ mlx (11435)."""
    u = (url or "").rstrip("/")
    if ":11435" in u or "11435/" in u:
        return "mlx"
    if ":11434" in u or "11434/" in u:
        return "ollama"
    return "ollama"  # default


async def run_local_model(prompt: str, model: Optional[str] = None) -> Optional[str]:
    """–ó–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–±–µ–∑ —Ç–æ–∫–µ–Ω–æ–≤). –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –±–µ—Ä—ë—Ç—Å—è –∏–∑ available_models_scanner ‚Äî Ollama –∏ MLX —Ä–∞–∑–¥–µ–ª—å–Ω–æ."""
    import httpx

    ollama_url = os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_URL") or "http://localhost:11434"
    mlx_url = os.getenv("MAC_LLM_URL") or os.getenv("MLX_API_URL") or "http://localhost:11435"
    raw_nodes = [
        os.getenv("MAC_LLM_URL") or mlx_url,
        os.getenv("OLLAMA_BASE_URL") or os.getenv("OLLAMA_URL") or ollama_url,
        os.getenv("SERVER_LLM_URL") or ollama_url,
    ]
    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ URL –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–æ –ø–æ—Ä—Ç—É (11434=ollama, 11435=mlx)
    seen = set()
    unique_nodes = []
    for url in raw_nodes:
        u = (url or "").rstrip("/")
        if u in seen:
            continue
        seen.add(u)
        kind = _node_type(u)
        unique_nodes.append((u, kind))

    logger.info("[NIGHTLY_LEARNER] run_local_model nodes=%s", [n[0] for n in unique_nodes])

    try:
        from available_models_scanner import get_available_models, pick_best_ollama, pick_best_mlx
    except ImportError:
        try:
            from app.available_models_scanner import get_available_models, pick_best_ollama, pick_best_mlx
        except ImportError:
            logger.warning("[NIGHTLY_LEARNER] available_models_scanner not found, cannot get model lists")
            return None

    mlx_list, ollama_list = await get_available_models(mlx_url, ollama_url, force_refresh=False)
    logger.info("[NIGHTLY_LEARNER] scanner: mlx=%s ollama=%s", len(mlx_list), len(ollama_list))

    for node_url, node_kind in unique_nodes:
        selected_model = None
        if node_kind == "ollama":
            selected_model = pick_best_ollama(ollama_list) if ollama_list else None
        else:
            selected_model = pick_best_mlx(mlx_list) if mlx_list else None

        if not selected_model:
            logger.debug("[NIGHTLY_LEARNER] skip node=%s kind=%s no model", node_url, node_kind)
            continue

        logger.info("[NIGHTLY_LEARNER] trying node_url=%s selected_model=%s kind=%s", node_url, selected_model, node_kind)

        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                health = await client.get(f"{node_url}/api/tags", timeout=3.0)
                if health.status_code != 200:
                    logger.info("[NIGHTLY_LEARNER] node %s /api/tags status_code=%s", node_url, health.status_code)
                    continue
                tags_response = health.json()
                available_tags = [m.get("name", "") for m in tags_response.get("models", [])]
                logger.info("[NIGHTLY_LEARNER] node=%s /api/tags 200 models_count=%s names=%s", node_url, len(available_tags), available_tags[:15])

                if selected_model not in available_tags:
                    selected_model = available_tags[0] if available_tags else None
                if not selected_model:
                    continue

                logger.info("[NIGHTLY_LEARNER] POST %s/api/generate model=%s", node_url, selected_model)
                response = await client.post(
                    f"{node_url}/api/generate",
                    json={"model": selected_model, "prompt": prompt, "stream": False},
                    timeout=120.0,
                )

                if response.status_code == 200:
                    result = response.json()
                    out = result.get("response", "").strip()
                    logger.info("[NIGHTLY_LEARNER] success node=%s model=%s response_len=%s", node_url, selected_model, len(out))
                    return out
                if response.status_code == 404:
                    body = (response.text or "")[:200]
                    logger.warning(
                        "[NIGHTLY_LEARNER] Ollama 404: node=%s model=%s; available_models=%s body=%s",
                        node_url, selected_model, available_tags, body,
                    )
                    continue
                logger.warning("[NIGHTLY_LEARNER] node=%s /api/generate status_code=%s body=%s", node_url, response.status_code, (response.text or "")[:200])
        except httpx.TimeoutException:
            logger.debug("[NIGHTLY_LEARNER] timeout node=%s", node_url)
            continue
        except Exception as e:
            logger.warning("[NIGHTLY_LEARNER] error node=%s: %s", node_url, e)
            continue

    return None

async def run_cursor_agent(prompt: str) -> Optional[str]:
    """–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –æ–±–ª–∞—á–Ω–æ–π –º–æ–¥–µ–ª–∏ —á–µ—Ä–µ–∑ Cursor Agent (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å cursor-agent
    cursor_agent_paths = [
        os.path.expanduser('~/.local/bin/cursor-agent'),
        '/usr/local/bin/cursor-agent',
        '/root/.local/bin/cursor-agent'
    ]
    
    cursor_agent_path = None
    for path in cursor_agent_paths:
        if os.path.exists(path) and os.access(path, os.X_OK):
            cursor_agent_path = path
            break
    
    if not cursor_agent_path:
        result = await run_local_model(prompt)
        return result
    
    try:
        env = os.environ.copy()
        result = subprocess.run(
            [cursor_agent_path, '--print', prompt],
            capture_output=True, text=True, check=True, timeout=300, env=env
        )
        return result.stdout.strip()
    except subprocess.TimeoutExpired:
        print(f"‚ö†Ô∏è Cursor agent timeout –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞: {prompt[:50]}...")
        return None
    except subprocess.CalledProcessError as e:
        print(f"‚ö†Ô∏è Cursor agent error (code {e.returncode}): {e.stderr[:200]}")
        return None
    except Exception as e:
        print(f"‚ö†Ô∏è Cursor agent exception: {e}")
        return None

async def get_nightly_context(conn):
    """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è OKR –∏ –ø–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ—à–∏–±–æ–∫ –∑–∞ –¥–µ–Ω—å (Phase 0)."""
    # 1. OKR
    okrs = await conn.fetch("SELECT objective FROM okrs WHERE created_at > NOW() - INTERVAL '30 days'")
    okr_text = "\n".join([f"- {o['objective']}" for o in okrs])
    
    # 2. –û–®–ò–ë–ö–ò –ò –ü–õ–û–•–û–ô FEEDBACK (Phase 0: Error Analysis)
    bad_interactions = await conn.fetch("""
        SELECT user_query, assistant_response, metadata->>'error' as error
        FROM interaction_logs 
        WHERE (feedback_score < 3 OR metadata->>'error' IS NOT NULL)
          AND created_at > NOW() - INTERVAL '24 hours'
        LIMIT 10
    """)
    error_context = ""
    if bad_interactions:
        error_context = "\n".join([f"Q: {i['user_query'][:100]} | Error: {i['error'] or 'Low feedback'}" for i in bad_interactions])
        print(f"‚ö†Ô∏è Phase 0: –ù–∞–π–¥–µ–Ω–æ {len(bad_interactions)} –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")
    
    return okr_text, error_context


async def sync_okrs(conn):
    """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ OKR —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ –ë–î."""
    print("Syncing OKR metrics...")
    try:
        await conn.execute("""
            UPDATE key_results 
            SET current_value = (SELECT count(*) FROM knowledge_nodes)
            WHERE description ILIKE '%–û–±—ä–µ–º –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π%' OR description ILIKE '%—É–∑–ª–æ–≤%'
        """)
        await conn.execute("""
            UPDATE key_results 
            SET current_value = (SELECT COALESCE(sum(usage_count), 0) FROM knowledge_nodes)
            WHERE description ILIKE '%–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ%' OR description ILIKE '%ROI%'
        """)
        print("OKR Sync completed.")
    except Exception as e:
        print(f"OKR Sync error: {e}")

async def create_debate_for_hypothesis(conn, knowledge_node_id, content, domain_id=None):
    """–°–æ–∑–¥–∞—ë—Ç –¥–µ–±–∞—Ç –ø–æ –≥–∏–ø–æ—Ç–µ–∑–µ: –Ω–∞—Ö–æ–¥–∏—Ç —ç–∫—Å–ø–µ—Ä—Ç–∞ –ø–æ –¥–æ–º–µ–Ω—É –∏ –≤—ã–∑—ã–≤–∞–µ—Ç run_expert_council."""
    expert = None
    if domain_id:
        try:
            expert = await conn.fetchrow(
                "SELECT id FROM experts WHERE domain_id = $1 ORDER BY RANDOM() LIMIT 1",
                domain_id
            )
        except Exception:
            pass
    if not expert and domain_id:
        try:
            domain = await conn.fetchrow("SELECT name FROM domains WHERE id = $1", domain_id)
            if domain:
                expert = await conn.fetchrow(
                    "SELECT id FROM experts WHERE department = $1 ORDER BY RANDOM() LIMIT 1",
                    domain['name']
                )
        except Exception:
            pass
    if not expert:
        try:
            expert = await conn.fetchrow("SELECT id FROM experts ORDER BY RANDOM() LIMIT 1")
        except Exception:
            pass
    if expert:
        await run_expert_council(conn, knowledge_node_id, content, expert['id'])
    else:
        logger.warning("No experts found for hypothesis debate, skipping")


async def run_expert_council(conn, knowledge_id, content, original_expert_id):
    """
    –ò–Ω–∏—Ü–∏–∏—Ä—É–µ—Ç –º–Ω–æ–≥–æ—Ä–∞—É–Ω–¥–æ–≤—ã–µ –¥–µ–±–∞—Ç—ã (Red Team Pattern) –º–µ–∂–¥—É —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏.
    –†–∞—É–Ω–¥ 1: –ö—Ä–∏—Ç–∏–∫–∞ –∏–Ω—Å–∞–π—Ç–∞.
    –†–∞—É–Ω–¥ 2: –û—Ç–≤–µ—Ç –∞–≤—Ç–æ—Ä–∞ –Ω–∞ –∫—Ä–∏—Ç–∏–∫—É.
    –†–∞—É–Ω–¥ 3: –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∏–Ω—Ç–µ–∑ –∏ –≤–µ—Ä–¥–∏–∫—Ç.
    """
    _log_step(f"[NIGHTLY] Enhanced Expert Council (Red Team) starting for knowledge_id={knowledge_id}")
    try:
        import gc
        gc.collect()
        # 1. –í—ã–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ—Ä–∞ –∏ –æ–ø–ø–æ–Ω–µ–Ω—Ç–æ–≤
        author = await conn.fetchrow("SELECT name, role FROM experts WHERE id = $1", original_expert_id)
        opponents = await conn.fetch("""
            SELECT id, name, role, system_prompt 
            FROM experts 
            WHERE id != $1 
            ORDER BY RANDOM() LIMIT 2
        """, original_expert_id)
        
        if not opponents or not author: return

        debate_log = []
        debate_log.append(f"üìù **–ê–≤—Ç–æ—Ä ({author['name']}):** {content}")

        # –†–ê–£–ù–î 1: –ö–†–ò–¢–ò–ö–ê (RED TEAM)
        criticisms = []
        for opp in opponents:
            prompt = f"""
            –í–´ - RED TEAM –≠–ö–°–ü–ï–†–¢. 
            –†–û–õ–¨: {opp['name']}, {opp['role']}.
            –ó–ê–î–ê–ß–ê: –ù–∞–π–¥–∏—Ç–µ 3 –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —É—è–∑–≤–∏–º–æ—Å—Ç–∏, –ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –æ—à–∏–±–∫–∏ –∏–ª–∏ –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º –∏–Ω—Å–∞–π—Ç–µ:
            "{content}"
            
            –û–¢–í–ï–¢–¨–¢–ï –ñ–ï–°–¢–ö–û –ò –ü–û –°–£–©–ï–°–¢–í–£ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
            """
            comment = await run_local_model(prompt) or await run_cursor_agent(prompt)
            if comment:
                criticisms.append(f"üßê {opp['name']} ({opp['role']}): {comment}")
                debate_log.append(f"‚ùå **–ö—Ä–∏—Ç–∏–∫–∞ –æ—Ç {opp['name']}:** {comment}")

        # –†–ê–£–ù–î 2: –û–¢–í–ï–¢ –ê–í–¢–û–†–ê
        if criticisms:
            rebuttal_prompt = f"""
            –í–´ - {author['name']}, {author['role']}.
            –í–ê–® –ò–ù–°–ê–ô–¢: "{content}"
            –ö–†–ò–¢–ò–ö–ê:
            {chr(10).join(criticisms)}
            
            –ó–ê–î–ê–ß–ê: –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –∫—Ä–∏—Ç–∏–∫—É. –ü—Ä–∏–∑–Ω–∞–π—Ç–µ –æ—à–∏–±–∫–∏, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, –∏–ª–∏ –æ–±–æ—Å–Ω—É–π—Ç–µ —Å–≤–æ—é –ø–æ–∑–∏—Ü–∏—é.
            –û–¢–í–ï–¢–¨–¢–ï –ö–†–ê–¢–ö–û (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).
            """
            rebuttal = await run_local_model(rebuttal_prompt) or await run_cursor_agent(rebuttal_prompt)
            if rebuttal:
                debate_log.append(f"üõ°Ô∏è **–û—Ç–≤–µ—Ç –∞–≤—Ç–æ—Ä–∞ ({author['name']}):** {rebuttal}")

        # –†–ê–£–ù–î 3: –§–ò–ù–ê–õ–¨–ù–´–ô –°–ò–ù–¢–ï–ó (–ö–û–ù–°–ï–ù–°–£–°)
        synthesis_prompt = f"""
        –í–´ - –ù–ï–ô–¢–†–ê–õ–¨–ù–´–ô –ê–†–ë–ò–¢–† –ö–û–†–ü–û–†–ê–¶–ò–ò.
        –•–û–î –û–ë–°–£–ñ–î–ï–ù–ò–Ø:
        {chr(10).join(debate_log)}
        
        –ó–ê–î–ê–ß–ê: –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∏—Ç–æ–≥–æ–≤—ã–π –∫–æ–Ω—Å–µ–Ω—Å—É—Å. –ù–∞—Å–∫–æ–ª—å–∫–æ –∏–Ω—Å–∞–π—Ç –ø–æ–ª–µ–∑–µ–Ω –¥–ª—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏? 
        –£–∫–∞–∂–∏—Ç–µ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (0.0 - 1.0).
        """
        consensus = await run_local_model(synthesis_prompt) or await run_cursor_agent(synthesis_prompt)
        
        if consensus:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ–±–∞—Ç—ã
            full_summary = "\n\n".join(debate_log) + f"\n\nüèÅ **–ò–¢–û–ì–û–í–´–ô –ö–û–ù–°–ï–ù–°–£–°:**\n{consensus}"
            await conn.execute("""
                INSERT INTO expert_discussions (knowledge_node_id, expert_ids, topic, consensus_summary, status)
                VALUES ($1, $2, $3, $4, 'closed')
            """, knowledge_id, [original_expert_id] + [o['id'] for o in opponents], content[:100], full_summary)
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —É–∑–ª–∞
            await conn.execute("""
                UPDATE knowledge_nodes 
                SET metadata = COALESCE(metadata, '{}'::jsonb) || jsonb_build_object('council_review', $1::text, 'red_team_status', 'passed')
                WHERE id = $2
            """, consensus, knowledge_id)
            _log_step("‚úÖ Enhanced Expert Council finished successfully.")

    except Exception as e:
        print(f"‚ùå Enhanced Expert Council error: {e}")
        import traceback
        traceback.print_exc()

async def nightly_learning_cycle():
    async with acquire_resource_lock("nightly_learner"):
        start_time = datetime.now()
        _log_step(f"[{start_time}] Total Nightly Learning Cycle (Council Phase enabled) starting...")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º
        try:
            try:
                from corporation_knowledge_system import update_all_agents_knowledge
            except ImportError:
                from app.corporation_knowledge_system import update_all_agents_knowledge
            await update_all_agents_knowledge()
            print("‚úÖ –ó–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –ø–µ—Ä–µ–¥ –æ–±—É—á–µ–Ω–∏–µ–º")
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –∑–Ω–∞–Ω–∏—è –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏: {e}")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º get_pool –∏–∑ evaluator –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ –ë–î
        from evaluator import get_pool
        pool = await get_pool()
        conn = await pool.acquire()
        
        # Phase 0: –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç OKR –∏ –æ—à–∏–±–æ–∫
        okr_context, error_context = await get_nightly_context(conn)
        await sync_okrs(conn)
        
        # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏–ª–∏ —Ç–µ—Ö, –∫—Ç–æ –Ω–µ –æ–±—É—á–∞–ª—Å—è –¥–∞–≤–Ω–æ
        # –û–±—É—á–∞–µ–º –≤—Å–µ—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –Ω–æ —Å —É—á–µ—Ç–æ–º –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
        # –ï—Å–ª–∏ —ç–∫—Å–ø–µ—Ä—Ç –æ–±—É—á–∞–ª—Å—è –Ω–µ–¥–∞–≤–Ω–æ (< 24 —á–∞—Å–æ–≤), –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
        experts = await conn.fetch("""
            SELECT id, name, role, department, system_prompt, last_learned_at
            FROM experts
            ORDER BY 
                CASE 
                    WHEN last_learned_at IS NULL THEN 0
                    WHEN last_learned_at < NOW() - INTERVAL '24 hours' THEN 1
                    ELSE 2
                END,
                RANDOM()
        """)
        
        total_learned = 0
        total_experts = len(experts)
        learned_today = 0
        skipped_recent = 0

        print(f"üìö –ù–∞–π–¥–µ–Ω–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {total_experts}")
        
        for idx, expert in enumerate(experts):
            if idx % 10 == 0:
                import gc
                gc.collect()
            _log_step(f"[NIGHTLY] Expert {idx + 1}/{total_experts}: {expert.get('name', '?')}")
            expert_name = expert['name']
            expert_role = expert['role']
            last_learned = expert.get('last_learned_at')
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—É—á–∞–ª–∏—Å—å –º–µ–Ω–µ–µ 24 —á–∞—Å–æ–≤ –Ω–∞–∑–∞–¥
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É–∂–µ —Å–¥–µ–ª–∞–Ω–∞ –≤ SQL –∑–∞–ø—Ä–æ—Å–µ, –Ω–æ –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É
            if last_learned:
                # last_learned –∏–∑ –ë–î - —ç—Ç–æ timezone-aware datetime
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤ SQL, –Ω–æ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
                print(f"\n>>> Learning session for: {expert_name} ({expert_role})")
                
                # –í—ã—á–∏—Å–ª—è–µ–º –≤—Ä–µ–º—è —Å —É—á–µ—Ç–æ–º timezone
                if hasattr(last_learned, 'replace'):
                    # –≠—Ç–æ datetime –æ–±—ä–µ–∫—Ç
                    if last_learned.tzinfo:
                        # timezone-aware, –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ naive –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
                        last_learned_utc = last_learned.astimezone().replace(tzinfo=None)
                    else:
                        last_learned_utc = last_learned
                    
                    hours_ago = (datetime.now() - last_learned_utc).total_seconds() / 3600
                    print(f"   –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ: {hours_ago:.1f} —á–∞—Å–æ–≤ –Ω–∞–∑–∞–¥")
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–ª–∏–µ–Ω—Ç–µ (–æ—Å–Ω–æ–≤–Ω–∞—è —É–∂–µ –≤ SQL)
                    if hours_ago < 24:
                        print(f"‚è≠Ô∏è  –ü—Ä–æ–ø—É—Å–∫ {expert_name} - —É–∂–µ –æ–±—É—á–∞–ª—Å—è {hours_ago:.1f} —á–∞—Å–æ–≤ –Ω–∞–∑–∞–¥")
                        skipped_recent += 1
                        continue
                else:
                    print(f"\n>>> Learning session for: {expert_name} ({expert_role})")
                    print(f"   –ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±—É—á–µ–Ω–∏–µ: {last_learned} (—Ñ–æ—Ä–º–∞—Ç –Ω–µ datetime)")
            else:
                print(f"\n>>> Learning session for: {expert_name} ({expert_role})")
                print(f"   –ü–µ—Ä–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è (run_local_model –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–∫–∞–Ω–µ—Ä Ollama/MLX)
            # Adversarial Self-Play: –≠–∫—Å–ø–µ—Ä—Ç –¥–æ–ª–∂–µ–Ω —É—á–∏—Ç—ã–≤–∞—Ç—å OKR –∏ –ø—Ä–æ—à–ª—ã–µ –æ—à–∏–±–∫–∏
            gap_prompt = f"""–í–´ - {expert_name}, {expert_role}.
            –¶–ï–õ–¨ –ö–û–†–ü–û–†–ê–¶–ò–ò (OKR):
            {okr_context}
            
            –ü–†–û–ë–õ–ï–ú–´ –ó–ê –î–ï–ù–¨ (Phase 0):
            {error_context if error_context else "–û—à–∏–±–æ–∫ –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ."}
            
            –ó–ê–î–ê–ß–ê: –ö–∞–∫–∞—è –æ–¥–Ω–∞ —Å–∞–º–∞—è –≤–∞–∂–Ω–∞—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –∏–ª–∏ —Ç—Ä–µ–Ω–¥ 2026 –≥–æ–¥–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ {expert['department']} 
            –ø–æ–º–æ–∂–µ—Ç —Ä–µ—à–∏—Ç—å —É–∫–∞–∑–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –¥–æ—Å—Ç–∏—á—å —Ü–µ–ª–µ–π? 
            –û–¢–í–ï–¢–¨–¢–ï –û–î–ù–û–ô –§–†–ê–ó–û–ô.
            """

            topic = await run_local_model(gap_prompt)
            if not topic or len(topic.strip()) < 5:
                topic = await run_cursor_agent(gap_prompt)
            if not topic or len(topic.strip()) < 5:
                dept = expert.get('department', 'General')
                current_year = datetime.now().year
                topic = f"–ê–∫—Ç—É–∞–ª—å–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ç—Ä–µ–Ω–¥—ã {current_year} –≥–æ–¥–∞ –≤ –æ–±–ª–∞—Å—Ç–∏ {dept}"

            # –†–ï–§–õ–ï–ö–°–ò–Ø (Adversarial Self-Play Phase 2)
            search_prompt = f"""–ò—Å—Å–ª–µ–¥—É–π '{topic}'. 
            –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 1-2 –≥–ª—É–±–æ–∫–∏—Ö –∏–Ω—Å–∞–π—Ç–∞. 
            –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ô –§–ò–õ–¨–¢–†: –ù–∞–π–¥–∏ 1 –ø—Ä–∏—á–∏–Ω—É, –ø–æ—á–µ–º—É —ç—Ç–æ—Ç –∏–Ω—Å–∞–π—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—à–∏–±–æ—á–Ω—ã–º –∏–ª–∏ –±–µ—Å–ø–æ–ª–µ–∑–Ω—ã–º.
            
            –í–µ—Ä–Ω–∏ JSON: 
            {{
                "topic": "{topic}", 
                "summary": "...", 
                "insights": [ {{"content": "...", "confidence": 0.95}} ],
                "self_criticism": "..."
            }} 
            –û–¢–í–ï–¢–¨ –¢–û–õ–¨–ö–û –ß–ò–°–¢–´–ú JSON.
            """

            search_output = await run_local_model(search_prompt)
            if not search_output or ('insights' not in search_output and '{' not in search_output):
                search_output = await run_cursor_agent(search_prompt)
            
            if search_output:
                try:
                    data_str = search_output.strip()
                    if '```' in data_str:
                        data_str = data_str.split('```')[1].replace('json', '').strip()
                    
                    learning_data = json.loads(data_str)

                    domain_id = await conn.fetchval('SELECT id FROM domains WHERE name = $1', expert['department'])
                    if not domain_id:
                        domain_id = await conn.fetchval('INSERT INTO domains (name) VALUES ($1) RETURNING id', expert['department'])
                    
                    for insight in learning_data.get('insights', []):
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–Ω–∏–µ (–ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å embedding ‚Äî VERIFICATION ¬ß5)
                        content_kn = insight['content']
                        meta_kn = json.dumps({
                            "expert": expert_name, 
                            "cycle": "nightly_council_v2",
                            "self_criticism": learning_data.get('self_criticism', '')
                        })
                        embedding = None
                        try:
                            from semantic_cache import get_embedding
                            embedding = await get_embedding(content_kn[:8000])
                        except Exception:
                            pass
                        if embedding is not None:
                            k_id = await conn.fetchval("""
                                INSERT INTO knowledge_nodes (domain_id, content, confidence_score, metadata, is_verified, embedding)
                                VALUES ($1, $2, $3, $4, $5, $6::vector)
                                RETURNING id
                            """, domain_id, content_kn, insight['confidence'], meta_kn, True, str(embedding))
                        else:
                            k_id = await conn.fetchval("""
                                INSERT INTO knowledge_nodes (domain_id, content, confidence_score, metadata, is_verified)
                                VALUES ($1, $2, $3, $4, $5)
                                RETURNING id
                            """, domain_id, content_kn, insight['confidence'], meta_kn, True)
                        
                        total_learned += 1
                        
                        # –ï—Å–ª–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–∞—è, –∑–∞–ø—É—Å–∫–∞–µ–º –°–æ–≤–µ—Ç –≠–∫—Å–ø–µ—Ä—Ç–æ–≤
                        if insight['confidence'] >= 0.9:
                            await run_expert_council(conn, k_id, insight['content'], expert['id'])
                    
                    await conn.execute("INSERT INTO expert_learning_logs (expert_id, topic, summary) VALUES ($1, $2, $3)", 
                                     expert['id'], learning_data.get('topic', topic), learning_data.get('summary', ''))
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–∞
                    await conn.execute('UPDATE experts SET last_learned_at = CURRENT_TIMESTAMP WHERE id = $1', expert['id'])
                    learned_today += 1
                    
                except Exception as e:
                    print(f"Error for {expert_name}: {e}")
            
            await asyncio.sleep(5) # –£–≤–µ–ª–∏—á–µ–Ω–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞

        # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print(f"\n{'='*60}")
        print(f"üìä –ò–¢–û–ì–ò –û–ë–£–ß–ï–ù–ò–Ø:")
        print(f"   –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: {total_experts}")
        print(f"   –û–±—É—á–∏–ª–æ—Å—å —Å–µ–≥–æ–¥–Ω—è: {learned_today}")
        print(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ –æ–±—É—á–∞–ª–∏—Å—å < 24—á): {skipped_recent}")
        print(f"   –ù–æ–≤—ã—Ö –∑–Ω–∞–Ω–∏–π –¥–æ–±–∞–≤–ª–µ–Ω–æ: {total_learned}")
        print(f"{'='*60}\n")
        
        if total_learned > 0:
            # –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—â–µ–µ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
            await conn.execute('UPDATE experts SET last_learned_at = CURRENT_TIMESTAMP WHERE last_learned_at IS NULL')
            await sync_okrs(conn)
            
            # --- –§–ê–ó–ê 4: LM JUDGE (–í–ï–†–ò–§–ò–ö–ê–¶–ò–Ø) ---
            try:
                _log_step("‚öñÔ∏è Running LM Judge...")
                subprocess.run([sys.executable, os.path.join(_APP_DIR, "evaluator.py")], cwd=_APP_DIR)
            except Exception as e:
                logger.warning("LM Judge phase failed: %s", e)

            # --- –§–ê–ó–ê 5: CORPORATE IMMUNITY (–°–¢–†–ï–°–°-–¢–ï–°–¢) ---
            try:
                _log_step("üõ°Ô∏è Running Adversarial Critic...")
                subprocess.run([sys.executable, os.path.join(_APP_DIR, "adversarial_critic.py")], cwd=_APP_DIR)
            except Exception as e:
                logger.warning("Adversarial Critic phase failed: %s", e)
        
        # --- –§–ê–ó–ê 6: CONTEXTUAL LEARNING (–ö–û–ù–¢–ï–ö–°–¢–ù–ê–Ø –ü–ê–ú–Ø–¢–¨) ---
        _log_step("üéì Running Contextual Learning...")
        try:
            from contextual_learner import run_contextual_learning_cycle
            await run_contextual_learning_cycle()
        except Exception as e:
            print(f"‚ö†Ô∏è Contextual Learning error: {e}")
        
        # --- –§–ê–ó–ê 7: ENHANCED EXPERT EVOLUTION (–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ê–Ø –≠–í–û–õ–Æ–¶–ò–Ø) ---
        _log_step("üß¨ Running Autonomous Talent Management...")
        try:
            # –¢–µ–ø–µ—Ä—å —ç–∫—Å–ø–µ—Ä—Ç—ã —Å–∞–º–∏ —Ä–µ—à–∞—é—Ç, –∫–∞–∫–∏–µ —Å–∫–∏–ª–ª—ã –∏–º –Ω—É–∂–Ω—ã
            from expert_evolver import evolve_experts
            await evolve_experts()
        except Exception as e:
            print(f"‚ö†Ô∏è Talent Management error: {e}")
        
        # --- –§–ê–ó–ê 10: ADAPTIVE LEARNING (–ê–î–ê–ü–¢–ò–í–ù–û–ï –û–ë–£–ß–ï–ù–ò–ï) ---
        _log_step("üéì Running Adaptive Learning...")
        try:
            from adaptive_learner import run_adaptive_learning_cycle
            await run_adaptive_learning_cycle()
        except Exception as e:
            print(f"‚ö†Ô∏è Adaptive Learning error: {e}")
        
        # --- –§–ê–ó–ê 8: AUTO-TRANSLATION (–ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –ü–ï–†–ï–í–û–î) ---
        _log_step("üåç Running Auto-Translation...")
        try:
            from translator import run_auto_translation_cycle
            await run_auto_translation_cycle()
        except Exception as e:
            print(f"‚ö†Ô∏è Auto-Translation error: {e}")
        
        # --- –§–ê–ó–ê 9: UPDATE CURSORRULES (–û–ë–ù–û–í–õ–ï–ù–ò–ï .CURSORRULES) ---
        _log_step("üìù Updating .cursorrules from database...")
        try:
            from cursorrules_generator import update_cursorrules_file
            await update_cursorrules_file()
        except Exception as e:
            print(f"‚ö†Ô∏è .cursorrules update error: {e}")
        
        # --- –§–ê–ó–ê 10: DEBATE PROCESSING (–û–ë–†–ê–ë–û–¢–ö–ê –î–ï–ë–ê–¢–û–í) ---
        _log_step("üí¨ Processing debates and creating tasks...")
        try:
            from debate_processor import DebateProcessor
            processor = DebateProcessor()
            stats = await processor.process_new_debates()
            if stats['processed'] > 0:
                print(f"‚úÖ Processed {stats['processed']} debates:")
                print(f"   Created {stats['tasks_created']} tasks")
                print(f"   Prioritized {stats['knowledge_prioritized']} knowledge nodes")
                print(f"   Sent {stats['notifications_sent']} notifications")
        except Exception as e:
            print(f"‚ö†Ô∏è Debate processing error: {e}")
            import traceback
            traceback.print_exc()

        # --- –§–ê–ó–ê 11: APPLY ALL KNOWLEDGE (SINGULARITY 10.0) ---
        _log_step("üß† Applying knowledge (lessons ‚Üí guidance, retrospectives ‚Üí knowledge_nodes, insights ‚Üí tasks)...")
        try:
            import gc
            gc.collect()
            from pathlib import Path
            _app_dir = Path(__file__).resolve().parent
            _ko_root = _app_dir.parent
            if str(_ko_root) not in sys.path:
                sys.path.insert(0, str(_ko_root))
            from observability.knowledge_applicator import apply_all_knowledge_async
            results = await apply_all_knowledge_async()
            if any(results.values()):
                print(f"‚úÖ Knowledge applied: guidance={results.get('guidance_updated')}, knowledge_base={results.get('knowledge_base_updated')}, prompts_evolved={results.get('prompts_evolved')}, code_tasks={results.get('code_tasks_created')}")
        except Exception as e:
            print(f"‚ö†Ô∏è Knowledge application error: {e}")
            import traceback
            traceback.print_exc()

        # --- –§–ê–ó–ê 12: DASHBOARD DAILY IMPROVEMENT (SINGULARITY 10.0) ---
        _log_step("üìä Running dashboard improvement cycle...")
        try:
            import gc
            gc.collect()
            from dashboard_daily_improver import run_dashboard_improvement_cycle
            dash_result = await run_dashboard_improvement_cycle()
            if dash_result.get("tasks_created", 0) > 0:
                print(f"‚úÖ Dashboard improvement: {dash_result['tasks_created']} tasks created")
        except Exception as e:
            print(f"‚ö†Ô∏è Dashboard improvement error: {e}")
            import traceback
            traceback.print_exc()

        # --- –§–ê–ó–ê 13: AUTONOMOUS TESTS (Living Brain) ---
        _log_step("üß™ Running autonomous test phase...")
        try:
            tests_dir = os.path.join(_KNOWLEDGE_OS_ROOT, "tests")
            if os.path.exists(tests_dir):
                result = subprocess.run(
                    [sys.executable, "-m", "pytest", "tests/test_json_fast_http_client.py", "tests/test_rest_api.py", "-v", "--tb=no", "-q"],
                    cwd=_KNOWLEDGE_OS_ROOT,
                    capture_output=True,
                    text=True,
                    timeout=120,
                )
                passed = result.returncode == 0
                summary = (result.stdout or "")[-500:] + (result.stderr or "")[-300:]
                content_kn = f"Autonomous tests run at {datetime.now().isoformat()}: passed={passed}, returncode={result.returncode}\n\n{summary}"
                domain_id = await conn.fetchval("SELECT id FROM domains WHERE name = $1 LIMIT 1", "QA") or await conn.fetchval("SELECT id FROM domains LIMIT 1")
                embedding = None
                try:
                    from semantic_cache import get_embedding
                    embedding = await get_embedding(content_kn[:8000])
                except Exception:
                    pass
                if embedding is not None:
                    await conn.execute("""
                        INSERT INTO knowledge_nodes (domain_id, content, confidence_score, source_ref, metadata, embedding)
                        VALUES ($1, $2, $3, $4, $5, $6::vector)
                    """, domain_id or 1, content_kn, 1.0 if passed else 0.5, "autonomous_tests", json.dumps({"passed": passed, "returncode": result.returncode}), str(embedding))
                else:
                    await conn.execute("""
                        INSERT INTO knowledge_nodes (domain_id, content, confidence_score, source_ref, metadata)
                        VALUES ($1, $2, $3, $4, $5)
                    """, domain_id or 1, content_kn, 1.0 if passed else 0.5, "autonomous_tests", json.dumps({"passed": passed, "returncode": result.returncode}))
                if not passed:
                    await conn.execute("""
                        INSERT INTO tasks (title, description, status, priority, metadata)
                        VALUES ($1, $2, 'pending', 'high', $3::jsonb)
                    """, "üîß –ò—Å–ø—Ä–∞–≤–∏—Ç—å –ø–∞–¥–∞—é—â–∏–µ –∞–≤—Ç–æ—Ç–µ—Å—Ç—ã (Nightly Learner)", content_kn[:2000], json.dumps({"source": "nightly_learner", "assignee_hint": "QA"}))
                    print(f"‚ö†Ô∏è Tests failed, task created for QA")
                else:
                    print(f"‚úÖ Autonomous tests passed")
        except Exception as e:
            logger.warning("Autonomous tests phase failed: %s", e)

        # --- –§–ê–ó–ê 14: GIT DIFF ‚Üí –ó–ê–î–ê–ß–ò –ù–ê –ì–ï–ù–ï–†–ê–¶–ò–Æ –¢–ï–°–¢–û–í (Living Brain ¬ß6.1) ---
        _log_step("üìù Running git diff ‚Üí test generation tasks...")
        try:
            repo_root = os.path.dirname(_KNOWLEDGE_OS_ROOT)
            if os.path.exists(os.path.join(repo_root, ".git")):
                r = subprocess.run(
                    ["git", "log", "--since=24 hours ago", "--name-only", "--pretty=format:"],
                    cwd=repo_root,
                    capture_output=True,
                    text=True,
                    timeout=10,
                )
                changed = [f.strip() for f in (r.stdout or "").splitlines() if f.strip() and f.strip().endswith(".py")]
                changed = list(dict.fromkeys(changed))  # dedupe
                tests_dir_rel = "knowledge_os/tests" if repo_root != _KNOWLEDGE_OS_ROOT else "tests"
                created = 0
                for path in changed[:10]:
                    if "knowledge_os/app/" not in path and "knowledge_os/" not in path:
                        continue
                    mod = path.replace("knowledge_os/", "").replace(".py", "").replace("/", ".")
                    test_name = f"test_{mod.split('.')[-1]}.py"
                    if any(test_name in p for p in changed):
                        continue
                    test_path = os.path.join(repo_root, tests_dir_rel, test_name)
                    if os.path.exists(test_path):
                        continue
                    exists = await conn.fetchval(
                        "SELECT 1 FROM tasks WHERE metadata->>'module' = $1 AND status NOT IN ('completed','cancelled') AND created_at > NOW() - INTERVAL '7 days' LIMIT 1",
                        mod,
                    )
                    if exists:
                        continue
                    meta = json.dumps({"source": "nightly_learner", "assignee_hint": "QA", "module": mod})
                    await conn.execute("""
                        INSERT INTO tasks (title, description, status, priority, metadata)
                        VALUES ($1, $2, 'pending', 'medium', $3::jsonb)
                    """, f"üß™ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å pytest –¥–ª—è {mod}", f"–ú–æ–¥—É–ª—å –∏–∑–º–µ–Ω—ë–Ω –∑–∞ 24—á. –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç—ã –≤ {tests_dir_rel}/{test_name}. –ú–æ–¥—É–ª—å: {path}", meta)
                    created += 1
                    if created >= 3:
                        break
                if created > 0:
                    print(f"‚úÖ Phase 14: {created} test generation task(s) created")
            else:
                logger.debug("Phase 14 skipped: no .git in repo root")
        except Exception as e:
            logger.debug("Phase 14 (git diff ‚Üí test tasks) failed: %s", e)

        # --- –§–ê–ó–ê 15: –ê–í–¢–û-–ü–†–û–§–ò–õ–ò–†–û–í–ê–ù–ò–ï (Living Brain ¬ß6.3, AUTO_PROFILING_GUIDE) ---
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ä–∞–∑ –≤ –Ω–µ–¥–µ–ª—é (–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ) —á—Ç–æ–±—ã –Ω–µ –∑–∞–º–µ–¥–ª—è—Ç—å –∫–∞–∂–¥—ã–π Nightly
        if datetime.now().weekday() == 6:  # 0=Mon, 6=Sun
            _log_step("üìä Running auto-profiling phase (cProfile)...")
            try:
                import cProfile
                import pstats
                import io
                try:
                    from app.json_fast import loads, dumps
                except ImportError:
                    try:
                        from json_fast import loads, dumps
                    except ImportError:
                        loads, dumps = __import__("json").loads, __import__("json").dumps
                prof = cProfile.Profile()
                prof.enable()
                for _ in range(500):
                    loads(dumps({"test": "value", "n": 42}))
                prof.disable()
                s = io.StringIO()
                pstats.Stats(prof, stream=s).sort_stats("cumulative").print_stats(15)
                report = s.getvalue()
                domain_id = await conn.fetchval("SELECT id FROM domains WHERE name = $1 LIMIT 1", "Performance") or await conn.fetchval("SELECT id FROM domains LIMIT 1")
                content = f"Auto-profiling at {datetime.now().isoformat()} (json_fast roundtrip x500)\n\n{report[:3000]}"
                embedding = None
                try:
                    from semantic_cache import get_embedding
                    embedding = await get_embedding(content[:8000])
                except Exception:
                    pass
                if embedding is not None:
                    await conn.execute("""
                        INSERT INTO knowledge_nodes (domain_id, content, confidence_score, source_ref, metadata, embedding)
                        VALUES ($1, $2, 0.8, $3, $4, $5::vector)
                    """, domain_id or 1, content, "auto_profiling", json.dumps({"phase": 15, "workload": "json_roundtrip"}), str(embedding))
                else:
                    await conn.execute("""
                        INSERT INTO knowledge_nodes (domain_id, content, confidence_score, source_ref, metadata)
                        VALUES ($1, $2, 0.8, $3, $4)
                    """, domain_id or 1, content, "auto_profiling", json.dumps({"phase": 15, "workload": "json_roundtrip"}))
                print("‚úÖ Phase 15: profiling result saved to knowledge_nodes")
            except Exception as e:
                logger.debug("Phase 15 (auto-profiling) failed: %s", e)

        # --- –§–ê–ó–ê 16: –ó–ê–î–ê–ß–ê –ù–ê –°–ò–ù–•–†–û–ù–ò–ó–ê–¶–ò–Æ –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–ò (Living Organism ¬ß8, –¢–∞—Ç—å—è–Ω–∞) ---
        # –ü—Ä–∏ merge –≤ main –∑–∞ 24—á ‚Äî —Å–æ–∑–¥–∞—Ç—å –∑–∞–¥–∞—á—É –¥–ª—è Technical Writer –æ–±–Ω–æ–≤–∏—Ç—å MASTER_REFERENCE/docs
        _log_step("üìÑ Checking for documentation sync task...")
        try:
            repo_root = os.path.dirname(_KNOWLEDGE_OS_ROOT)
            if os.path.exists(os.path.join(repo_root, ".git")):
                r = subprocess.run(
                    ["git", "log", "--since=24 hours ago", "--merges", "--oneline"],
                    cwd=repo_root,
                    capture_output=True,
                    text=True,
                    timeout=5,
                )
                merge_count = len([l for l in (r.stdout or "").strip().splitlines() if l.strip()])
                if merge_count > 0:
                    exists = await conn.fetchval("""
                        SELECT 1 FROM tasks
                        WHERE title LIKE '%–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é%'
                        AND created_at > NOW() - INTERVAL '7 days'
                        AND status NOT IN ('completed', 'cancelled')
                        LIMIT 1
                    """)
                    if not exists:
                        meta = json.dumps({"source": "nightly_learner", "assignee_hint": "Technical Writer", "phase": 16})
                        await conn.execute("""
                            INSERT INTO tasks (title, description, status, priority, metadata)
                            VALUES ($1, $2, 'pending', 'low', $3::jsonb)
                        """, "üìù –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é —Å –ø–æ—Å–ª–µ–¥–Ω–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏",
                            f"–í main –∑–∞ 24—á –±—ã–ª–æ {merge_count} merge(–æ–≤). –ü—Ä–æ–≤–µ—Ä–∏—Ç—å MASTER_REFERENCE, CHANGES_FROM_OTHER_CHATS –∏ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫–∏ (–ø—Ä–∞–≤–∏–ª–æ –±–∏–±–ª–∏–∏).", meta)
                        print("‚úÖ Phase 16: documentation sync task created")
        except Exception as e:
            logger.debug("Phase 16 (doc sync task) failed: %s", e)

        # --- –§–ê–ó–ê 17: –ê–í–¢–û–û–ß–ò–°–¢–ö–ê –°–¢–ê–†–´–• –ó–ê–î–ê–ß (completed > 30 –¥–Ω–µ–π, cancelled) ---
        _log_step("üóëÔ∏è Running tasks cleanup (completed >30 days, cancelled)...")
        try:
            deleted_completed = await conn.fetchval("""
                WITH d AS (
                    DELETE FROM tasks
                    WHERE status = 'completed' AND updated_at < NOW() - INTERVAL '30 days'
                    RETURNING id
                )
                SELECT count(*)::int FROM d
            """) or 0
            deleted_cancelled = await conn.fetchval("""
                WITH d AS (DELETE FROM tasks WHERE status = 'cancelled' RETURNING id)
                SELECT count(*)::int FROM d
            """) or 0
            if deleted_completed or deleted_cancelled:
                print(f"‚úÖ Phase 17: deleted {deleted_completed} old completed, {deleted_cancelled} cancelled")
            else:
                print("‚úÖ Phase 17: nothing to clean")
        except Exception as e:
            logger.debug("Phase 17 (tasks cleanup) failed: %s", e)

        await pool.release(conn)
        await pool.close()
        _log_step(f"[{datetime.now()}] Total cycle with Council Review finished.")


if __name__ == '__main__':
    asyncio.run(nightly_learning_cycle())
