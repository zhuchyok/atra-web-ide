import os
import httpx
import logging
import asyncio
import time
import hashlib
import asyncpg
import json
from typing import Optional, List, Dict, Tuple, AsyncGenerator
from functools import lru_cache

# ML Router Data Collector
try:
    from ml_router_data_collector import MLRouterDataCollector, get_collector
except ImportError:
    MLRouterDataCollector = None
    get_collector = None

# ML Router Model
try:
    from ml_router_model import MLRouterModel
except ImportError:
    MLRouterModel = None

# ML Router A/B Test
try:
    from ml_router_ab_test import get_ab_test
except ImportError:
    get_ab_test = None

# Load Balancer
try:
    from load_balancer import get_load_balancer
except ImportError:
    get_load_balancer = None

logger = logging.getLogger(__name__)

# Debug mode: VICTORIA_DEBUG=true enables verbose logging
VICTORIA_DEBUG = os.getenv("VICTORIA_DEBUG", "false").lower() in ("true", "1", "yes")
if VICTORIA_DEBUG:
    logger.setLevel(logging.DEBUG)
    logging.getLogger().setLevel(logging.DEBUG)

# Config
MAC_LLM_URL = os.getenv('MAC_LLM_URL', 'http://localhost:11435')  # MacBook —á–µ—Ä–µ–∑ SSH reverse tunnel (11435 -> MacBook:11434)
SERVER_LLM_URL = os.getenv('SERVER_LLM_URL', 'http://localhost:11434')
USE_LOCAL_LLM = os.getenv('USE_LOCAL_LLM', 'true').lower() == 'true'
DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')

# Health check cache (120 seconds TTL - —É–≤–µ–ª–∏—á–µ–Ω –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ /api/tags)
_health_cache = {"nodes": [], "timestamp": 0}
_HEALTH_CACHE_TTL = 120  # 2 –º–∏–Ω—É—Ç—ã –≤–º–µ—Å—Ç–æ 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è rate limiting

# Model mapping with environment overrides for different hardware (Mac Studio)
MODEL_MAP = {
    "coding": os.getenv('MODEL_CODING', "qwen2.5-coder:32b"),
    "reasoning": os.getenv('MODEL_REASONING', "deepseek-r1-distill-llama:70b"),
    "fast": os.getenv('MODEL_FAST', "phi3.5:3.8b"),
    "vision": "moondream",
    "vision_pdf": "llava:7b",
    "default": os.getenv('MODEL_DEFAULT', "phi3.5:3.8b")
}

# Ollama –º–æ–¥–µ–ª–∏
OLLAMA_MODELS = {
    "fast": "phi3.5:3.8b",
    "vision": "moondream",
    "vision_pdf": "llava:7b",
    "coding": "glm-4.7-flash:q8_0",
    "reasoning": "glm-4.7-flash:q8_0",
    "default": "phi3.5:3.8b"
}

# List of task categories that can be handled locally (L1)
LOCAL_TASK_CATEGORIES = [
    "code_audit",
    "log_analysis",
    "unit_test_generation",
    "text_summarization",
    "simple_query",
    "grammar_correction",
    "logic_check"
]


class LocalAIRouter:
    def __init__(self):
        self.use_local = USE_LOCAL_LLM
        import os
        is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
        if is_docker:
            ollama_url = "http://host.docker.internal:11434"
            mlx_url = "http://host.docker.internal:11435"
        else:
            ollama_url = "http://localhost:11434"
            mlx_url = "http://localhost:11435"
        self.nodes = [
            {"name": "Mac Studio (MLX)", "url": mlx_url, "priority": 0, "routing_key": "mlx_studio"},
            {"name": "MacBook (Ollama)", "url": ollama_url, "priority": 0, "routing_key": "local_mac"},
            {"name": "Server (Light)", "url": SERVER_LLM_URL, "priority": 0, "routing_key": "local_server"}
        ]
        self._active_node = None
        self._performance_cache = {}  # Cache for node performance metrics
        self._cache_ttl = 300  # 5 minutes
        
        # ML Model for intelligent routing
        self.ml_model = None
        self.ml_model_path = os.path.join(os.path.dirname(__file__), 'ml_router_model.pkl')
        self._load_ml_model()
        
        # Model Memory Manager –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
        self._memory_manager = None
        self._memory_manager_url = SERVER_LLM_URL
        self._tunnel_checked = False
        # –ö—ç—à –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ (prompt, category, model) ‚Äî —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–ø—Ä–æ—Å–æ–≤
        self._prompt_cache: Dict[str, Tuple[str, str]] = {}
        self._prompt_cache_meta: Dict[str, float] = {}
        self._prompt_cache_max = 500
        self._prompt_cache_ttl = 1800  # 30 –º–∏–Ω
        self._prompt_cache_hits = 0
        self._prompt_cache_misses = 0
    
    @property
    def memory_manager(self):
        """–î–æ—Å—Ç—É–ø –∫ memory_manager –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        return self._memory_manager
    
    def _evict_prompt_cache_if_needed(self) -> None:
        """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞ –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–∏ (LRU –ø–æ timestamp)."""
        if len(self._prompt_cache) < self._prompt_cache_max:
            return
        now = time.time()
        # –£–¥–∞–ª—è–µ–º –∏—Å—Ç—ë–∫—à–∏–µ
        expired = [k for k, ts in self._prompt_cache_meta.items() if (now - ts) >= self._prompt_cache_ttl]
        for k in expired:
            self._prompt_cache.pop(k, None)
            self._prompt_cache_meta.pop(k, None)
        if len(self._prompt_cache) >= self._prompt_cache_max:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –ø–æ timestamp
            sorted_keys = sorted(self._prompt_cache_meta.keys(), key=lambda x: self._prompt_cache_meta[x])
            for k in sorted_keys[: max(0, len(self._prompt_cache) - self._prompt_cache_max + 1)]:
                self._prompt_cache.pop(k, None)
                self._prompt_cache_meta.pop(k, None)
    
    def _load_ml_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ML-–º–æ–¥–µ–ª—å –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞"""
        if MLRouterModel and os.path.exists(self.ml_model_path):
            try:
                self.ml_model = MLRouterModel()
                self.ml_model.load(self.ml_model_path)
                logger.info("‚úÖ [ML ROUTER] ML model loaded successfully")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [ML ROUTER] Failed to load ML model: {e}")
                self.ml_model = None
        else:
            logger.debug("‚ÑπÔ∏è [ML ROUTER] ML model not available, using heuristic routing")
    
    async def predict_optimal_route(
        self,
        prompt: str,
        category: Optional[str] = None
    ) -> tuple:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç –∏—Å–ø–æ–ª—å–∑—É—è ML-–º–æ–¥–µ–ª—å.
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–¥–∞—á–∏
        
        Returns:
            (predicted_route, confidence) –∏–ª–∏ (None, None) –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        """
        if self.ml_model is None:
            return None, None
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏
            task_type = self._determine_task_type(prompt, category)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —É–∑–ª–æ–≤
            node_metrics = await self._get_node_performance_metrics()
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–∞—Ä—à—Ä—É—Ç
            predicted_route, confidence = self.ml_model.predict(
                task_type=task_type,
                prompt_length=len(prompt),
                category=category,
                node_metrics=node_metrics
            )
            
            logger.info(f"ü§ñ [ML ROUTER] Predicted route: {predicted_route} (confidence: {confidence:.2f})")
            return predicted_route, confidence
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [ML ROUTER] Prediction error: {e}")
            return None, None

    async def check_health(self, force_refresh: bool = False) -> List[Dict]:
        """Check which nodes are alive and return their latency. Uses caching."""
        global _health_cache
        current_time = time.time()
        
        # Return cached result if still valid
        if not force_refresh and (current_time - _health_cache["timestamp"]) < _HEALTH_CACHE_TTL:
            return _health_cache["nodes"]
        
        healthy_nodes = []
        async with httpx.AsyncClient() as client:
            for node in self.nodes:
                try:
                    start_time = time.time()
                    # –ü—Ä–æ–±—É–µ–º –ª–µ–≥–∫–∏–π /health endpoint —Å–Ω–∞—á–∞–ª–∞ (–±—ã—Å—Ç—Ä–µ–µ, –±–µ–∑ rate limiting)
                    health_url = f"{node['url']}/health"
                    try:
                        response = await client.get(health_url, timeout=2.0)
                        if response.status_code == 200:
                            latency = time.time() - start_time
                            healthy_nodes.append({
                                **node,
                                "latency": latency,
                                "status": "online"
                            })
                            continue  # –£—Å–ø–µ—à–Ω–æ, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —É–∑–ª—É
                    except Exception:
                        pass  # /health –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–±—É–µ–º /api/tags
                    
                    # Fallback –Ω–∞ /api/tags (–µ—Å–ª–∏ /health –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
                    response = await client.get(f"{node['url']}/api/tags", timeout=2.0)
                    latency = time.time() - start_time
                    if response.status_code == 200:
                        healthy_nodes.append({
                            **node,
                            "latency": latency,
                            "status": "online"
                        })
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Node {node['name']} is offline: {e}")
        
        # Get performance metrics from cache for each node
        performance_metrics = await self._get_node_performance_metrics()
        
        # Enhance nodes with performance data
        for node in healthy_nodes:
            routing_key = node.get('routing_key', '')
            if routing_key in performance_metrics:
                node['performance_score'] = performance_metrics[routing_key].get('avg_performance', 0.8)
                node['success_rate'] = performance_metrics[routing_key].get('success_rate', 0.9)
            else:
                node['performance_score'] = 0.8  # Default
                node['success_rate'] = 0.9  # Default
        
        # Sort by: performance_score (higher is better), then priority, then latency
        sorted_nodes = sorted(
            healthy_nodes, 
            key=lambda x: (-x.get('performance_score', 0.8), x['priority'], x['latency'])
        )
        
        # Update cache
        _health_cache = {"nodes": sorted_nodes, "timestamp": current_time}
        
        return sorted_nodes
    
    async def _get_node_performance_metrics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–∑–ª–æ–≤ –∏–∑ semantic_ai_cache"""
        try:
            # Check cache first
            current_time = time.time()
            if hasattr(self, '_performance_cache') and self._performance_cache:
                cache_time = self._performance_cache.get('timestamp', 0)
                if (current_time - cache_time) < self._cache_ttl:
                    return self._performance_cache.get('metrics', {})
            
            # Try to connect to DB
            try:
                conn = await asyncpg.connect(DB_URL, timeout=2)
            except (asyncpg.PostgresError, OSError, ValueError):
                return {}
            
            try:
                # Check if columns exist
                columns_exist = await conn.fetchval("""
                    SELECT COUNT(*) FROM information_schema.columns 
                    WHERE table_name = 'semantic_ai_cache' 
                    AND column_name IN ('routing_source', 'performance_score')
                """) == 2
                
                if not columns_exist:
                    await conn.close()
                    return {}
                
                # Get performance metrics for each routing source (last 24 hours)
                metrics = await conn.fetch("""
                    SELECT 
                        routing_source,
                        AVG(performance_score) as avg_performance,
                        COUNT(*) as total_requests,
                        COUNT(*) FILTER (WHERE performance_score >= 0.7) as successful_requests
                    FROM semantic_ai_cache
                    WHERE routing_source IS NOT NULL
                    AND routing_source IN ('local_mac', 'local_server')
                    AND last_used_at > NOW() - INTERVAL '24 hours'
                    GROUP BY routing_source
                """)
                
                result = {}
                for row in metrics:
                    routing_key = row['routing_source']
                    total = row['total_requests'] or 0
                    successful = row['successful_requests'] or 0
                    result[routing_key] = {
                        'avg_performance': float(row['avg_performance'] or 0.8),
                        'success_rate': (successful / total) if total > 0 else 0.9,
                        'total_requests': total
                    }
                
                await conn.close()
                
                # Update cache
                self._performance_cache = {
                    'metrics': result,
                    'timestamp': current_time
                }
                
                return result
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error getting performance metrics: {e}")
                await conn.close()
                return {}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error in _get_node_performance_metrics: {e}")
            return {}

    def _is_simple_task(self, prompt: str, category: Optional[str] = None) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–¥–∞—á–∞ –ø—Ä–æ—Å—Ç–æ–π (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama –ø—Ä–∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–µ MLX)"""
        # –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏:
        # - –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã (< 500 —Å–∏–º–≤–æ–ª–æ–≤)
        # - –ü—Ä–æ—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (fast, simple_query, text_summarization)
        # - –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç (–±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–∏)
        # - –ù–µ —Ç—Ä–µ–±—É–µ—Ç reasoning
        
        prompt_lower = prompt.lower()
        
        # –ü—Ä–æ—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        simple_categories = ["fast", "simple_query", "text_summarization", "grammar_correction"]
        if category in simple_categories:
            return True
        
        # –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã
        if len(prompt) < 500:
            return True
        
        # –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç (–±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)
        complex_keywords = ["–ø–æ–¥—É–º–∞–π", "–ª–æ–≥–∏–∫–∞", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è", "–∞–Ω–∞–ª–∏–∑", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"]
        if not any(keyword in prompt_lower for keyword in complex_keywords):
            return True
        
        return False
    
    def _select_model(self, prompt: str, category: Optional[str] = None, use_ollama: bool = False, node_type: Optional[str] = None) -> str:
        """Select the best local model for the task.
        –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏, –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (MLX/Ollama).
        
        Args:
            prompt: User prompt
            category: Task category
            use_ollama: If True, use Ollama models (deprecated - —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–∞ –≤—ã–±–µ—Ä–µ—Ç)
            node_type: –¢–∏–ø —É–∑–ª–∞ ('mlx' –∏–ª–∏ 'ollama') - –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª–∏
        """
        prompt_lower = prompt.lower()
        
        # –í—ã–±–∏—Ä–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á–∏, –∞ –Ω–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
        # –û–±–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ (MLX –∏ Ollama) –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –º–æ–¥–µ–ª–∏
        
        # Reasoning –∑–∞–¥–∞—á–∏ - –Ω—É–∂–Ω–∞ –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å
        if category == "reasoning" or "–ø–æ–¥—É–º–∞–π" in prompt_lower or "–ª–æ–≥–∏–∫–∞" in prompt_lower or "–ø–ª–∞–Ω–∏—Ä" in prompt_lower:
            # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ reasoning –º–æ–¥–µ–ª—å –≤ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–∞—Ö
            if node_type == "mlx":
                return "deepseek-r1-distill-llama:70b"  # MLX –º–æ–¥–µ–ª—å
            else:
                return OLLAMA_MODELS.get("reasoning", "command-r-plus:104b")  # Ollama –º–æ–¥–µ–ª—å
        
        # Coding –∑–∞–¥–∞—á–∏ - –Ω—É–∂–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–¥–∞
        if "–∫–æ–¥" in prompt_lower or "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–π" in prompt_lower or category == "coding":
            if node_type == "mlx":
                return "qwen2.5-coder:32b"  # MLX –º–æ–¥–µ–ª—å
            else:
                return OLLAMA_MODELS.get("coding", "glm-4.7-flash:q8_0")  # Ollama –º–æ–¥–µ–ª—å
        
        # –ë—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏ - –ª–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å
        if category == "fast" or len(prompt) < 300:
            if node_type == "mlx":
                return "phi3.5:3.8b"  # MLX –º–æ–¥–µ–ª—å
            else:
                return OLLAMA_MODELS.get("fast", "phi3.5:3.8b")  # Ollama –º–æ–¥–µ–ª—å
        
        # Vision –∑–∞–¥–∞—á–∏
        if category == "vision" or "–∏–∑–æ–±—Ä–∞–∂–µ–Ω" in prompt_lower or "–∫–∞—Ä—Ç–∏–Ω–∫" in prompt_lower:
            return OLLAMA_MODELS.get("vision", "moondream")  # Vision —Ç–æ–ª—å–∫–æ –≤ Ollama
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
        if node_type == "mlx":
            return "phi3.5:3.8b"  # MLX –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        else:
            return OLLAMA_MODELS.get("default", "phi3.5:3.8b")  # Ollama –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    async def _is_mlx_overloaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω –ª–∏ MLX API Server"""
        try:
            from app.mlx_request_queue import get_request_queue
            queue = get_request_queue()
            stats = queue.get_stats()
            
            # MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω –µ—Å–ª–∏:
            # - –í—Å–µ —Å–ª–æ—Ç—ã –∑–∞–Ω—è—Ç—ã (active_requests >= max_concurrent)
            # - –ï—Å—Ç—å –æ—á–µ—Ä–µ–¥—å (queue_size > 0)
            is_overloaded = (
                stats.get("active_requests", 0) >= stats.get("max_concurrent", 5) or
                stats.get("queue_size", 0) > 0
            )
            
            if is_overloaded:
                logger.debug(
                    f"‚ö†Ô∏è MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω: –∞–∫—Ç–∏–≤–Ω—ã—Ö={stats.get('active_requests')}/"
                    f"{stats.get('max_concurrent')}, –æ—á–µ—Ä–µ–¥—å={stats.get('queue_size')}"
                )
            
            return is_overloaded
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É MLX: {e}")
            return False  # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω

    def should_use_local(self, prompt: str, category: Optional[str] = None, images: Optional[list] = None) -> bool:
        """Determine if the task should be routed to local LLM. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Ollama/MLX)."""
        if not self.use_local:
            logger.debug("[LOCAL ROUTER] should_use_local=False: USE_LOCAL_LLM –æ—Ç–∫–ª—é—á–µ–Ω")
            return False
        
        # If images are provided, we MUST use local vision model (e.g., moondream)
        if images:
            return True
        
        # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–¥–∞–Ω–∞ ‚Äî –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä, –≤–æ—Ä–∫–µ—Ä, –∫–æ–¥–∏–Ω–≥ –∏ —Ç.–¥.)
        if category in LOCAL_TASK_CATEGORIES or category in MODEL_MAP:
            return True
        if category in ("autonomous_worker", "orchestrator", "general", "research", "reasoning", "coding", "fast"):
            return True
        
        # Heuristic based on prompt content
        prompt_lower = prompt.lower()
        if any(keyword in prompt_lower for keyword in ["–∞–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤", "–ø—Ä–æ–≤–µ—Ä—å –∫–æ–¥", "–Ω–∞–ø–∏—à–∏ —Ç–µ—Å—Ç", "—Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π", "–∏—Å–ø—Ä–∞–≤—å –æ–ø–µ—á–∞—Ç–∫—É"]):
            return True
            
        # If the prompt is very large (context-heavy) and doesn't require high-level reasoning
        if len(prompt) > 2000 and "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞" not in prompt_lower and "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è" not in prompt_lower:
            return True
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏)
        if category is not None:
            return True
            
        return False

    async def run_local_llm(self, prompt: str, system_prompt: str = "", category: Optional[str] = None, images: Optional[list] = None, max_retries: int = 2, model: Optional[str] = None) -> Optional[tuple]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é LLM –º–æ–¥–µ–ª—å.
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: MLX API Server (HTTP) –∏ Ollama ‚Äî –æ–±–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è (–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞).
        model: –µ—Å–ª–∏ –∑–∞–¥–∞–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç—É –º–æ–¥–µ–ª—å –∏ –ø–µ—Ä–µ–±–∏—Ä–∞–µ–º —É–∑–ª—ã (MLX/Ollama) –ø–æ–∫–∞ –æ–¥–∏–Ω –Ω–µ –æ—Ç–≤–µ—Ç–∏—Ç.
        
        Returns:
            tuple: (response, routing_source)
        """
        logger.info("[ROUTER] ========== LocalAIRouter.run_local_llm() ==========")
        logger.info("[ROUTER] Input model: %s", model)
        logger.info("[ROUTER] Category: %s", category)
        logger.info("[ROUTER] Prompt length: %d chars", len(prompt))
        logger.info("[ROUTER] Prompt preview: %s...", prompt[:150])
        
        # –ü–†–ò–û–†–ò–¢–ï–¢: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MLX API Server –∏ Ollama —á–µ—Ä–µ–∑ HTTP —Ä–æ—É—Ç–∏–Ω–≥
        # MLX Router –Ω–∞–ø—Ä—è–º—É—é –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ (—Ç—Ä–µ–±—É–µ—Ç –º–æ–¥—É–ª—å mlx)
        # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º MLX API Server —á–µ—Ä–µ–∑ HTTP (—É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ nodes)
        # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –≤–∫–ª—é—á–∞–µ–º —Ç—É–Ω–Ω–µ–ª—å –¥–ª—è MacBook –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        if ("localhost:11435" in MAC_LLM_URL or "127.0.0.1:11435" in MAC_LLM_URL) and not self._tunnel_checked:
            try:
                from tunnel_manager import ensure_tunnel, get_tunnel_status
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–µ–º —Ç—É–Ω–Ω–µ–ª—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                status_before = get_tunnel_status()
                logger.info(f"üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ SSH tunnel –¥–ª—è MacBook (—Å—Ç–∞—Ç—É—Å: {status_before})...")
                await ensure_tunnel()
                status_after = get_tunnel_status()
                if status_after == "–∞–∫—Ç–∏–≤–µ–Ω" and status_before != "–∞–∫—Ç–∏–≤–µ–Ω":
                    logger.info("‚úÖ SSH tunnel –¥–ª—è MacBook –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–∫–ª—é—á–µ–Ω!")
                elif status_after == "–∞–∫—Ç–∏–≤–µ–Ω":
                    logger.debug("‚úÖ SSH tunnel –¥–ª—è MacBook —É–∂–µ –∞–∫—Ç–∏–≤–µ–Ω")
                self._tunnel_checked = True
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Tunnel check failed: {e}")
                self._tunnel_checked = True  # –ü–æ–º–µ—á–∞–µ–º –∫–∞–∫ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–π, —á—Ç–æ–±—ã –Ω–µ –ø–æ–≤—Ç–æ—Ä—è—Ç—å
        """Call local LLM (Ollama style) with automatic failover, retry logic and node selection."""
        if not model:
            model = self._select_model(prompt, category)
            logger.info("[ROUTER] Model selected by _select_model(): %s", model)
        if images and MODEL_MAP.get("vision"):
            model = MODEL_MAP["vision"]
            logger.info("[ROUTER] Using vision model: %s", model)
        
        # –ö—ç—à: —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        prompt_cache_key = None
        if len(prompt) <= 1000 and not images:
            raw_key = f"{prompt}|{category or ''}|{model or ''}"
            prompt_cache_key = hashlib.sha256(raw_key.encode()).hexdigest()[:32]
            now = time.time()
            if prompt_cache_key in self._prompt_cache:
                ts = self._prompt_cache_meta.get(prompt_cache_key, 0)
                if (now - ts) < self._prompt_cache_ttl:
                    self._prompt_cache_hits += 1
                    logger.debug("‚úÖ [CACHE HIT] LocalAIRouter prompt cache")
                    return self._prompt_cache[prompt_cache_key]
            self._prompt_cache_misses += 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        task_type = self._determine_task_type(prompt, category)
        
        # A/B Testing: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ ML-—Ä–æ—É—Ç–∏–Ω–≥
        use_ml_routing = False
        if get_ab_test and self.ml_model:
            ab_test = await get_ab_test(ml_ratio=0.5)  # 50% ML, 50% heuristic
            use_ml_routing = ab_test.should_use_ml()
        
        # ML Prediction: –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω A/B —Ç–µ—Å—Ç)
        ml_predicted_route = None
        ml_confidence = None
        if use_ml_routing and self.ml_model:
            ml_predicted_route, ml_confidence = await self.predict_optimal_route(prompt, category)
            if ml_predicted_route and ml_confidence > 0.7:
                logger.info(f"ü§ñ [ML ROUTER] Using ML prediction: {ml_predicted_route} (confidence: {ml_confidence:.2f})")
        elif self.ml_model:
            logger.debug("üìä [HEURISTIC ROUTER] A/B test: using heuristic routing")
        
        # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è memory_manager (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        if self._memory_manager is None and self._memory_manager_url:
            try:
                from model_memory_manager import get_memory_manager
                self._memory_manager = get_memory_manager(self._memory_manager_url)
                logger.debug("‚úÖ ModelMemoryManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)")
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è ModelMemoryManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self._memory_manager = None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –≤—ã–±–æ—Ä–æ–º —É–∑–ª–∞ (–¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞)
        if self._memory_manager:
            available_mb = await self._memory_manager.get_available_memory_mb()
            if available_mb < 200:  # MIN_FREE_MEMORY_MB
                logger.warning(f"‚ö†Ô∏è [MEMORY] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏: {available_mb}MB, –∑–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É...")
                await self._memory_manager.emergency_memory_cleanup()
        
        # Discover healthy nodes (with caching)
        healthy_nodes = await self.check_health()
        if not healthy_nodes:
            logger.error("‚ùå No healthy local nodes found!")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏–µ –æ fallback –≤ –æ–±–ª–∞–∫–æ
            if get_collector:
                collector = await get_collector()
                await collector.collect_routing_decision(
                    task_type=task_type,
                    prompt_length=len(prompt),
                    category=category,
                    selected_route="cloud",  # Fallback –≤ –æ–±–ª–∞–∫–æ
                    success=False,
                    features={"reason": "no_healthy_nodes", "ml_predicted": ml_predicted_route}
                )
            return None
        
        # –£–ú–ù–´–ô –í–´–ë–û–† –£–ó–õ–ê: —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á–∏
        # 1. Load Balancing: –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —É–∑–µ–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if get_load_balancer:
            load_balancer = get_load_balancer()
            best_node = load_balancer.select_best_node(healthy_nodes)
            if best_node and best_node != healthy_nodes[0]:
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ª—É—á—à–∏–π —É–∑–µ–ª –≤ –Ω–∞—á–∞–ª–æ
                healthy_nodes.remove(best_node)
                healthy_nodes.insert(0, best_node)
                logger.info(f"‚öñÔ∏è [LOAD BALANCER] –í—ã–±—Ä–∞–Ω —É–∑–µ–ª: {best_node['name']} –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # 2. ML Prediction: –µ—Å–ª–∏ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª –º–∞—Ä—à—Ä—É—Ç, —É—á–∏—Ç—ã–≤–∞–µ–º –µ–≥–æ
        if ml_predicted_route and ml_predicted_route != "cloud":
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —É–∑–ª—ã, —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –±—ã–ª –ø–µ—Ä–≤—ã–º (–Ω–æ –ø–æ—Å–ª–µ load balancer)
            predicted_node = None
            other_nodes = []
            for node in healthy_nodes:
                if node.get('routing_key') == ml_predicted_route:
                    predicted_node = node
                else:
                    other_nodes.append(node)
            
            if predicted_node and predicted_node != healthy_nodes[0]:
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —É–∑–µ–ª –Ω–∞ –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ (–ø–æ—Å–ª–µ load balancer)
                healthy_nodes.remove(predicted_node)
                healthy_nodes.insert(1, predicted_node)
                logger.info(f"ü§ñ [ML ROUTER] –£—á–∏—Ç—ã–≤–∞–µ–º ML-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_node['name']}")
        
        # 3. –í—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏ (reasoning ‚Üí MLX, fast ‚Üí Ollama, –∏ —Ç.–¥.)
        # –≠—Ç–æ —É–∂–µ –¥–µ–ª–∞–µ—Ç—Å—è –≤ _select_model –Ω–∞ –æ—Å–Ω–æ–≤–µ node_type
        
        # 4. –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —É–∑–ª—ã –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MLX –∏ Ollama
        mlx_nodes = [n for n in healthy_nodes if "11435" in n['url'] or "mlx" in n['url'].lower()]
        ollama_nodes = [n for n in healthy_nodes if "11434" in n['url'] or "ollama" in n['url'].lower()]
        other_nodes = [n for n in healthy_nodes if n not in mlx_nodes and n not in ollama_nodes]
        prefer_ollama_due_to_mlx_overload = False
        
        # 4.1 –ï—Å–ª–∏ MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω (–æ—á–µ—Ä–µ–¥—å/rate limit) ‚Äî –ø—Ä–æ–±—É–µ–º Ollama –ø–µ—Ä–≤—ã–º
        if mlx_nodes and ollama_nodes:
            try:
                if await self._is_mlx_overloaded():
                    prefer_ollama_due_to_mlx_overload = True
                    healthy_nodes = ollama_nodes + mlx_nodes + other_nodes
                    logger.info(
                        "üîÑ [ROUTER] MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç Ollama –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ "
                        "(–º–µ–Ω—å—à–µ 429, –±—ã—Å—Ç—Ä–µ–µ –æ—Ç–≤–µ—Ç)"
                    )
            except Exception as e:
                logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ MLX: {e}")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ —Ç–∏–ø–∞ —É–∑–ª–æ–≤ –∏ –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª–∏ Ollama –ø–µ—Ä–≤—ã–º ‚Äî —á–µ—Ä–µ–¥—É–µ–º –∏—Ö
        if mlx_nodes and ollama_nodes and not prefer_ollama_due_to_mlx_overload:
            balanced_nodes = []
            max_len = max(len(mlx_nodes), len(ollama_nodes))
            for i in range(max_len):
                if i < len(mlx_nodes):
                    balanced_nodes.append(mlx_nodes[i])
                if i < len(ollama_nodes):
                    balanced_nodes.append(ollama_nodes[i])
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É–∑–ª—ã –≤ –∫–æ–Ω–µ—Ü
            balanced_nodes.extend(other_nodes)
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π —É–∑–µ–ª –æ—Ç load balancer –µ—Å–ª–∏ –æ–Ω –±—ã–ª –≤—ã–±—Ä–∞–Ω
            if healthy_nodes and balanced_nodes:
                first_node = healthy_nodes[0]
                if first_node in balanced_nodes:
                    balanced_nodes.remove(first_node)
                    balanced_nodes.insert(0, first_node)
                healthy_nodes = balanced_nodes
                logger.info(f"‚öñÔ∏è [BALANCED] –ü–µ—Ä–µ–º–µ—à–∞–Ω—ã —É–∑–ª—ã –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MLX ({len(mlx_nodes)}) –∏ Ollama ({len(ollama_nodes)})")

        # –£–º–Ω—ã–π –≤—ã–±–æ—Ä —É–∑–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏
        # –°–∏—Å—Ç–µ–º–∞ —Å–∞–º–∞ –≤—ã–±–µ—Ä–µ—Ç –ª—É—á—à–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ (MLX –∏–ª–∏ Ollama) –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        # 1. –¢–∏–ø–∞ –∑–∞–¥–∞—á–∏ (reasoning ‚Üí –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å, fast ‚Üí –ª–µ–≥–∫–∞—è)
        # 2. –ó–∞–≥—Ä—É–∑–∫–∏ —É–∑–ª–∞ (load balancing)
        # 3. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
        # 4. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –º–µ–∂–¥—É MLX –∏ Ollama
        
        # –£–õ–£–ß–®–ï–ù–ò–ï: –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ (–∏–∑ worker'–∞), –ø—Ä–æ–±—É–µ–º –µ–≥–æ –ø–µ—Ä–≤—ã–º
        preferred_source = getattr(self, '_preferred_source', None)
        if preferred_source:
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —É–∑–µ–ª –≤ –Ω–∞—á–∞–ª–æ
            preferred_nodes = [n for n in healthy_nodes if 
                              (preferred_source == 'mlx' and ("11435" in n['url'] or "mlx" in n['url'].lower())) or
                              (preferred_source == 'ollama' and ("11434" in n['url'] or "ollama" in n['url'].lower()))]
            if preferred_nodes:
                for node in preferred_nodes:
                    if node in healthy_nodes:
                        healthy_nodes.remove(node)
                        healthy_nodes.insert(0, node)
                logger.info(f"üéØ [PREFERRED] –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {preferred_source}")
        
        # Try each node with retry logic
        start_time = time.time()
        for node in healthy_nodes:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —ç—Ç–æ Ollama –∏–ª–∏ MLX API Server
            is_ollama = "11434" in node['url'] or "ollama" in node['url'].lower()
            is_mlx = "11435" in node['url'] or "mlx" in node['url'].lower()
            
            # –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –í–´–ë–û–† –ú–û–î–ï–õ–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∏—Ä–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫ (–∏–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å ‚Äî Victoria)
            node_type = "mlx" if is_mlx else "ollama" if is_ollama else "unknown"
            
            # 1. –ü–µ—Ä–µ–¥–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å (Victoria: –ª—É—á—à–∞—è –∏–∑ Ollama+MLX) ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è –≤—Å–µ—Ö —É–∑–ª–æ–≤
            current_model = model
            # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø–æ–ø—ã—Ç–æ–∫)
            recommended_model = getattr(self, '_recommended_model', None)
            if recommended_model and not current_model:
                current_model = recommended_model
                logger.info(f"üéØ [ROUTER] –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å: {current_model}")
            if not current_model:
                # 3. –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∏—Ä–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
                try:
                    from intelligent_model_router import get_intelligent_router
                    intelligent_router = get_intelligent_router()
                    
                    # –ü–æ–ª—É—á–∞–µ–º –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç—Ç–æ–≥–æ —É–∑–ª–∞
                    available_models = []
                    if is_mlx:
                        # MLX –º–æ–¥–µ–ª–∏
                        available_models = ['qwen2.5-coder:32b', 'deepseek-r1-distill-llama:70b', 'phi3.5:3.8b']
                    elif is_ollama:
                        # Ollama –º–æ–¥–µ–ª–∏
                        available_models = ['glm-4.7-flash:q8_0', 'phi3.5:3.8b']
                    
                    if available_models:
                        # –í—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç model, TaskCategory, confidence)
                        optimal_model, _task_cat, confidence = await intelligent_router.select_optimal_model(
                            prompt=prompt,
                            category=category or "",
                            available_models=available_models,
                            optimize_for='balanced'  # –ë–∞–ª–∞–Ω—Å –∫–∞—á–µ—Å—Ç–≤–∞, —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
                        )
                        
                        if optimal_model and confidence > 0.5:
                            current_model = optimal_model
                            logger.info(f"üß† [INTELLIGENT ROUTER] –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {current_model} (confidence: {confidence:.2f})")
                        else:
                            current_model = self._select_model(prompt, category, node_type=node_type)
                            logger.debug(f"Intelligent router confidence too low ({confidence:.2f}), using fallback: {current_model}")
                    else:
                        current_model = self._select_model(prompt, category, node_type=node_type)
                except Exception as e:
                    logger.debug(f"Intelligent router failed: {e}, using fallback")
                    current_model = self._select_model(prompt, category, node_type=node_type)
            
            model = current_model
            logger.info(f"üéØ [SMART SELECTION] –£–∑–µ–ª: {node['name']} | –ú–æ–¥–µ–ª—å: {model} | –¢–∏–ø –∑–∞–¥–∞—á–∏: {category or 'auto'}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º /api/chat –¥–ª—è Ollama (–±–æ–ª–µ–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π endpoint)
            if is_ollama or is_mlx:
                node_url = f"{node['url']}/api/chat"
                logger.info(f"üè† [LOCAL ROUTE] Node: {node['name']} | Model: {model} | Endpoint: /api/chat")
                
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                payload = {
                    "model": model,
                    "messages": messages,
                    "stream": False
                }
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º /api/generate
                node_url = f"{node['url']}/api/generate"
                logger.info(f"üè† [LOCAL ROUTE] Node: {node['name']} | Model: {model} | Endpoint: /api/generate")
                
                full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
                payload = {
                    "model": model,
                    "prompt": full_prompt,
                    "stream": False
                }
            if images:
                payload["images"] = images
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è MLX)
            if is_mlx:
                try:
                    from resource_monitor import get_resource_monitor
                    monitor = get_resource_monitor()
                    mlx_health = await monitor.get_mlx_health()
                    
                    # –ï—Å–ª–∏ MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
                    if monitor.should_throttle_mlx(mlx_health):
                        logger.warning(
                            f"‚ö†Ô∏è [RESOURCE] MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω: "
                            f"RAM={mlx_health.get('system', {}).get('ram', {}).get('used_percent', 0):.1f}%, "
                            f"CPU={mlx_health.get('system', {}).get('cpu', {}).get('percent', 0):.1f}%, "
                            f"Active={mlx_health.get('active_requests', 0)}/{mlx_health.get('max_concurrent', 5)}"
                        )
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —É–∑–µ–ª, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π
                except Exception as e:
                    logger.debug(f"Resource monitoring failed: {e}")
            
            # Load Balancing: –æ—Ç–º–µ—á–∞–µ–º –Ω–∞—á–∞–ª–æ –∑–∞–ø—Ä–æ—Å–∞
            if get_load_balancer:
                load_balancer = get_load_balancer()
                load_balancer.start_request(node.get('routing_key', ''))
            
            # Retry logic with exponential backoff
            for attempt in range(max_retries + 1):
                try:
                    async with httpx.AsyncClient() as client:
                        request_start = time.time()
                        # –ß–∞—Ç —Å –í–∏–∫—Ç–æ—Ä–∏–µ–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç HIGH
                        headers = {"X-Request-Priority": "high"}
                        response = await client.post(
                            node_url,
                            json=payload,
                            headers=headers,
                            timeout=120.0  # Reduced from 300 to 120 seconds
                        )
                        latency_ms = (time.time() - request_start) * 1000
                        
                        # Load Balancing: –æ–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
                        if get_load_balancer:
                            load_balancer = get_load_balancer()
                            load_balancer.update_node_load(
                                node['name'],
                                node.get('routing_key', ''),
                                latency_ms / 1000.0,  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–µ–∫—É–Ω–¥—ã
                                success=(response.status_code == 200)
                            )
                            load_balancer.end_request(node.get('routing_key', ''))
                        
                        logger.info("[ROUTER] HTTP response status: %d from %s", response.status_code, node['name'])
                        
                        if response.status_code == 200:
                            result_data = response.json()
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤
                            if "message" in result_data:
                                # –§–æ—Ä–º–∞—Ç /api/chat
                                result = result_data["message"].get("content", "")
                                logger.info("[ROUTER] Response format: /api/chat, content length: %d", len(result))
                            elif "response" in result_data:
                                # –§–æ—Ä–º–∞—Ç /api/generate
                                result = result_data.get("response", "")
                                logger.info("[ROUTER] Response format: /api/generate, content length: %d", len(result))
                            else:
                                result = str(result_data)
                                logger.info("[ROUTER] Response format: unknown, raw data length: %d", len(result))
                            
                            logger.info("[ROUTER] Response preview: %s...", result[:200] if result else "(empty)")
                            
                            if result:
                                routing_source = node.get('routing_key', 'local_mac' if node['name'].startswith("MacBook") else 'local_server')
                                performance_score = node.get('performance_score', 0.8)
                                logger.info("[ROUTER] ‚úÖ [SUCCESS] Node: %s, Model: %s, Latency: %.2fms, Performance: %.2f", 
                                           node['name'], model, latency_ms, performance_score)
                                
                                # –û—Ç–º–µ—á–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–∞–º—è—Ç–∏
                                if self._memory_manager and node.get('routing_key') == 'local_server' and model:
                                    await self._memory_manager.mark_model_used(model)
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏–µ —Ä–æ—É—Ç–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML
                                if get_collector:
                                    collector = await get_collector()
                                    await collector.collect_routing_decision(
                                        task_type=task_type,
                                        prompt_length=len(prompt),
                                        category=category,
                                        selected_route=routing_source,
                                        performance_score=performance_score,
                                        latency_ms=latency_ms,
                                        success=True,
                                        features={
                                            "model": model,
                                            "node_name": node['name'],
                                            "node_priority": node.get('priority', 0),
                                            "attempt": attempt + 1
                                        }
                                    )
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
                                # –≠—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ worker'–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                                if hasattr(self, '_current_task_id'):
                                    try:
                                        from model_performance_tracker import get_performance_tracker
                                        tracker = get_performance_tracker()
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ metadata –∑–∞–¥–∞—á–∏ (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ worker'–µ)
                                        self._used_model = model
                                    except:
                                        pass
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –ø—Ä–∏ —É—Å–ø–µ—Ö–µ (–∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç)
                                if prompt_cache_key and len(result) < 5000:
                                    self._evict_prompt_cache_if_needed()
                                    self._prompt_cache[prompt_cache_key] = (result, routing_source)
                                    self._prompt_cache_meta[prompt_cache_key] = time.time()
                                # Return (response, routing_source) tuple
                                return result, routing_source
                            else:
                                logger.warning("[ROUTER] ‚ö†Ô∏è Node %s returned empty response for model %s", node['name'], model)
                        else:
                            # Log error response body for debugging
                            try:
                                error_text = response.text[:500]
                                logger.error("[ROUTER] ‚ùå Node %s returned HTTP %d: %s", node['name'], response.status_code, error_text)
                            except:
                                logger.error("[ROUTER] ‚ùå Node %s returned HTTP %d", node['name'], response.status_code)
                except asyncio.TimeoutError:
                    logger.warning("[ROUTER] ‚è±Ô∏è Timeout: Node %s, Model %s (attempt %d/%d)", 
                                  node['name'], model, attempt + 1, max_retries + 1)
                    if attempt < max_retries:
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                    continue
                except httpx.ConnectError as e:
                    logger.error("[ROUTER] ‚ùå Connection failed to %s: %s", node_url, e)
                    if attempt < max_retries:
                        await asyncio.sleep(2 ** attempt)
                    continue
                except Exception as e:
                    logger.error("[ROUTER] ‚ùå Exception calling Node %s: %s: %s", node['name'], type(e).__name__, e)
                    if attempt < max_retries:
                        await asyncio.sleep(2 ** attempt)
                    continue
            
            # If we exhausted retries for this node, try next
            logger.warning(f"‚ö†Ô∏è Node {node['name']} failed after {max_retries + 1} attempts, trying next...")

        # –í—Å–µ —É–∑–ª—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ—É–¥–∞—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        total_latency = (time.time() - start_time) * 1000
        if get_collector:
            collector = await get_collector()
            await collector.collect_routing_decision(
                task_type=task_type,
                prompt_length=len(prompt),
                category=category,
                selected_route="cloud",  # Fallback –≤ –æ–±–ª–∞–∫–æ
                latency_ms=total_latency,
                success=False,
                features={"reason": "all_nodes_failed"}
            )
        
        return None, None
    
    async def run_local_llm_streaming(
        self,
        prompt: str,
        system_prompt: str = "",
        category: Optional[str] = None,
        images: Optional[list] = None,
        model: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        Call local LLM with streaming support.
        Returns AsyncGenerator that yields response chunks.
        
        Args:
            prompt: User prompt
            system_prompt: System prompt
            category: Task category
            images: List of images (for vision models)
            model: Specific model to use (if None, selected automatically)
        
        Yields:
            Response chunks as strings
        """
        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Ç—É–Ω–Ω–µ–ª—å –∞–∫—Ç–∏–≤–µ–Ω –ø–µ—Ä–µ–¥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MacBook
        if "localhost:11435" in MAC_LLM_URL or "127.0.0.1:11435" in MAC_LLM_URL:
            try:
                from tunnel_manager import ensure_tunnel
                await ensure_tunnel()
            except Exception as e:
                logger.debug(f"Tunnel check failed: {e}")
        
        # Select model
        if model is None:
            model = self._select_model(prompt, category)
        if images and MODEL_MAP.get("vision"):
            model = MODEL_MAP["vision"]
        
        # Discover healthy nodes
        healthy_nodes = await self.check_health()
        if not healthy_nodes:
            logger.error("‚ùå [STREAMING] No healthy local nodes found!")
            return
        
        # Select best node (use first one, load balancer would be ideal but keep it simple for streaming)
        node = healthy_nodes[0]
        node_url = f"{node['url']}/api/generate"
        logger.info(f"üåä [STREAMING] Node: {node['name']} | Model: {model}")
        
        full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
        payload = {
            "model": model,
            "prompt": full_prompt,
            "stream": True
        }
        if images:
            payload["images"] = images
        
        try:
            async with httpx.AsyncClient(timeout=300.0) as client:
                async with client.stream(
                    'POST',
                    node_url,
                    json=payload
                ) as response:
                    if response.status_code != 200:
                        logger.error(f"‚ùå [STREAMING] Error: {response.status_code}")
                        return
                    
                    async for line in response.aiter_lines():
                        if not line:
                            continue
                        
                        try:
                            chunk_data = json.loads(line)
                            if 'response' in chunk_data:
                                yield chunk_data['response']
                            if chunk_data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            logger.error(f"‚ùå [STREAMING] Error: {e}")
            return
    
    def _determine_task_type(self, prompt: str, category: Optional[str] = None) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
        prompt_lower = prompt.lower()
        
        if category == "coding" or "–∫–æ–¥" in prompt_lower or "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–π" in prompt_lower:
            return "coding"
        elif category == "reasoning" or "–ø–æ–¥—É–º–∞–π" in prompt_lower or "–ª–æ–≥–∏–∫–∞" in prompt_lower:
            return "reasoning"
        else:
            return "general"
