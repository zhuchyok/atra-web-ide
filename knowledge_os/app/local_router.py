import os
import httpx
import logging
import asyncio
import time
import hashlib
import asyncpg
import json
from typing import Optional, List, Dict, Tuple, AsyncGenerator
from functools import lru_cache

# ML Router Data Collector
try:
    from ml_router_data_collector import MLRouterDataCollector, get_collector
except ImportError:
    MLRouterDataCollector = None
    get_collector = None

# ML Router Model
try:
    from ml_router_model import MLRouterModel
except ImportError:
    MLRouterModel = None

# ML Router A/B Test
try:
    from ml_router_ab_test import get_ab_test
except ImportError:
    get_ab_test = None

# Load Balancer
try:
    from load_balancer import get_load_balancer
except ImportError:
    get_load_balancer = None

logger = logging.getLogger(__name__)

# Debug mode: VICTORIA_DEBUG=true enables verbose logging
VICTORIA_DEBUG = os.getenv("VICTORIA_DEBUG", "false").lower() in ("true", "1", "yes")
if VICTORIA_DEBUG:
    logger.setLevel(logging.DEBUG)
    logging.getLogger().setLevel(logging.DEBUG)

# Config - Mac Studio (–ª–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞). OLLAMA_BASE_URL –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Victoria/Veronica
_is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
_default_ollama = 'http://host.docker.internal:11434' if _is_docker else 'http://localhost:11434'
def _valid_http_url(url: str) -> bool:
    """True –µ—Å–ª–∏ url ‚Äî –≤–∞–ª–∏–¥–Ω—ã–π http(s) URL (–Ω–µ 'disabled' –∏ –Ω–µ –ø—É—Å—Ç–æ–π)."""
    if not url or not isinstance(url, str):
        return False
    u = url.strip().lower()
    return u.startswith("http://") or u.startswith("https://")


OLLAMA_API_URL = os.getenv('OLLAMA_API_URL') or os.getenv('OLLAMA_BASE_URL') or os.getenv('SERVER_LLM_URL') or _default_ollama
_raw_mlx = os.getenv('MLX_API_URL') or os.getenv('MAC_LLM_URL') or ('http://host.docker.internal:11435' if _is_docker else 'http://localhost:11435')
MLX_API_URL = _raw_mlx if _valid_http_url(_raw_mlx) else None  # MLX_API_URL=disabled ‚Üí None, —Ç–æ–ª—å–∫–æ Ollama


def _get_keep_alive(model_name: Optional[str] = None):
    """
    Ollama keep_alive –∏–∑ env –∏–ª–∏ –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞—Å—á–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ (Singularity 10.0).
    
    –õ–æ–≥–∏–∫–∞ (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä—Ç–æ–≤):
    - Light (< 5GB): 3600—Å (1—á) ‚Äî –¥–µ—à–µ–≤–æ –¥–µ—Ä–∂–∞—Ç—å, –±—ã—Å—Ç—Ä–æ –æ—Ç–≤–µ—á–∞—Ç—å.
    - Medium (5-15GB): 600—Å (10–º).
    - Heavy (15-30GB): 300—Å (5–º).
    - Monster (> 30GB): 60—Å (1–º) ‚Äî –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–∞—è –≤—ã–≥—Ä—É–∑–∫–∞ –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è VRAM.
    """
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º —è–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ env
    raw = os.getenv("VICTORIA_OLLAMA_KEEP_ALIVE") or os.getenv("OLLAMA_KEEP_ALIVE")
    if raw:
        try:
            return int(raw) if str(raw).strip().lstrip("-").isdigit() else raw
        except (ValueError, AttributeError):
            return raw

    # 2. –ê–¥–∞–ø—Ç–∏–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ –µ—Å–ª–∏ –∏–º—è –º–æ–¥–µ–ª–∏ –∏–∑–≤–µ—Å—Ç–Ω–æ
    if model_name:
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏–∑ –∫—ç—à–∞ —Å–∫–∞–Ω–µ—Ä–∞
            from available_models_scanner import _scan_cache
            if _scan_cache and "ollama_sizes" in _scan_cache:
                size_bytes = _scan_cache["ollama_sizes"].get(model_name, 0)
                if size_bytes > 0:
                    size_gb = size_bytes / (1024**3)
                    if size_gb > 30: return 60
                    if size_gb > 15: return 300
                    if size_gb > 5: return 600
                    return 3600
        except Exception:
            pass

        # Fallback –Ω–∞ —ç–≤—Ä–∏—Å—Ç–∏–∫—É –ø–æ –∏–º–µ–Ω–∏
        key = model_name.lower()
        if "70b" in key or "104b" in key or "next" in key: return 60
        if "32b" in key or "30b" in key or "qwq" in key: return 300
        if "7b" in key or "8b" in key or "14b" in key: return 600
        if "3b" in key or "1b" in key or "tiny" in key or "embedding" in key: return 3600

    return 300  # –î–µ—Ñ–æ–ª—Ç 5 –º–∏–Ω
# –û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å (legacy)
MAC_LLM_URL = MLX_API_URL
SERVER_LLM_URL = OLLAMA_API_URL
USE_LOCAL_LLM = os.getenv('USE_LOCAL_LLM', 'true').lower() == 'true'
DB_URL = os.getenv('DATABASE_URL', 'postgresql://admin:secret@localhost:5432/knowledge_os')

# Health check cache (120 seconds TTL - —É–≤–µ–ª–∏—á–µ–Ω –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ /api/tags)
_health_cache = {"nodes": [], "timestamp": 0}
_HEALTH_CACHE_TTL = 120  # 2 –º–∏–Ω—É—Ç—ã –≤–º–µ—Å—Ç–æ 30 —Å–µ–∫—É–Ω–¥ –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è rate limiting

# ========== –î–ò–ù–ê–ú–ò–ß–ï–°–ö–û–ï –û–ë–ù–ê–†–£–ñ–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô ==========
# –ú–æ–¥–µ–ª–∏ —Å–∫–∞–Ω–∏—Ä—É—é—Ç—Å—è —Å —Å–µ—Ä–≤–µ—Ä–æ–≤ –∫–∞–∂–¥—ã–µ 2 –º–∏–Ω—É—Ç—ã —á–µ—Ä–µ–∑ available_models_scanner
# –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∑–∞–º–µ—á–∞—Ç—å —É–¥–∞–ª–µ–Ω–Ω—ã–µ

try:
    from available_models_scanner import (
        get_available_models, 
        pick_ollama_for_category, 
        pick_mlx_for_category,
        OLLAMA_PRIORITY_BY_CATEGORY,
        MLX_PRIORITY_BY_CATEGORY
    )
    _HAS_MODEL_SCANNER = True
except ImportError:
    _HAS_MODEL_SCANNER = False
    logger.warning("‚ö†Ô∏è available_models_scanner –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")

# –ö—ç—à –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏)
_cached_mlx_models: list = []
_cached_ollama_models: list = []
_models_cache_time: float = 0
_MODELS_CACHE_TTL = 120  # 2 –º–∏–Ω—É—Ç—ã

# Fallback –º–æ–¥–µ–ª–∏ (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –µ—Å–ª–∏ scanner –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω).
# MLX: —Ç–æ–ª—å–∫–æ –ª—ë–≥–∫–∏–µ ‚Äî 70b/104b/32b —É–¥–∞–ª–µ–Ω—ã (Metal/–ø–∞–º—è—Ç—å); –Ω–µ –ø–æ–¥—Å—Ç–∞–≤–ª—è—Ç—å —É–¥–∞–ª—ë–Ω–Ω—ã–µ.
MLX_MODELS_FALLBACK = {
    "reasoning": "phi3.5:3.8b",
    "coding": "phi3.5:3.8b",
    "chat": "phi3.5:3.8b",
    "fast": "phi3.5:3.8b",
    "default": "phi3.5:3.8b",
}

OLLAMA_MODELS_FALLBACK = {
    "reasoning": "deepseek-r1:32b",
    "coding": "qwen2.5-coder:32b",
    "chat": "deepseek-r1:32b",
    "fast": "deepseek-r1:14b",
    "vision": "moondream",
    "vision_pdf": "llava:7b",
    "default": "qwen2.5-coder:32b",
    "vip": "deepseek-r1:32b"
}

# –î–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
MLX_MODELS = MLX_MODELS_FALLBACK
OLLAMA_MODELS = OLLAMA_MODELS_FALLBACK

# Legacy MODEL_MAP –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
MODEL_MAP = {
    "coding": os.getenv('MODEL_CODING', OLLAMA_MODELS["coding"]),
    "reasoning": os.getenv('MODEL_REASONING', MLX_MODELS["reasoning"]),
    "fast": os.getenv('MODEL_FAST', OLLAMA_MODELS["fast"]),
    "vision": OLLAMA_MODELS["vision"],
    "vision_pdf": OLLAMA_MODELS["vision_pdf"],
    "default": os.getenv('MODEL_DEFAULT', OLLAMA_MODELS["default"])
}

# List of task categories that can be handled locally (L1)
LOCAL_TASK_CATEGORIES = [
    "code_audit",
    "log_analysis",
    "unit_test_generation",
    "text_summarization",
    "simple_query",
    "grammar_correction",
    "logic_check"
]


class LocalAIRouter:
    def __init__(self):
        self.use_local = USE_LOCAL_LLM
        is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º URL –∏–∑ env (docker-compose). MLX_API_URL=disabled ‚Üí —Ç–æ–ª—å–∫–æ Ollama (–Ω–µ –¥–æ–±–∞–≤–ª—è–µ–º MLX node)
        ollama_url = OLLAMA_API_URL or ("http://host.docker.internal:11434" if is_docker else "http://localhost:11434")
        if not _valid_http_url(ollama_url):
            ollama_url = "http://host.docker.internal:11434" if is_docker else "http://localhost:11434"
        mlx_url = MLX_API_URL or ("http://host.docker.internal:11435" if is_docker else "http://localhost:11435")
        self.nodes = []
        if _valid_http_url(mlx_url):
            self.nodes.append({"name": "Mac Studio (MLX)", "url": mlx_url, "priority": 0, "routing_key": "mlx_studio"})
        self.nodes.append({"name": "Mac Studio (Ollama)", "url": ollama_url, "priority": 1, "routing_key": "ollama_studio"})
        self._active_node = None
        self._performance_cache = {}  # Cache for node performance metrics
        self._cache_ttl = 300  # 5 minutes
        
        # ML Model for intelligent routing
        self.ml_model = None
        self.ml_model_path = os.path.join(os.path.dirname(__file__), 'ml_router_model.pkl')
        self._load_ml_model()
        
        # Model Memory Manager –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø–∞–º—è—Ç–∏ (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)
        self._memory_manager = None
        self._memory_manager_url = OLLAMA_API_URL
        self._tunnel_checked = False
        # –ö—ç—à –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ (prompt, category, model) ‚Äî —É—Å–∫–æ—Ä–µ–Ω–∏–µ –ø–æ–≤—Ç–æ—Ä—è—é—â–∏—Ö—Å—è –∑–∞–ø—Ä–æ—Å–æ–≤
        self._prompt_cache: Dict[str, Tuple[str, str]] = {}
        self._prompt_cache_meta: Dict[str, float] = {}
        self._prompt_cache_max = 500
        self._prompt_cache_ttl = 1800  # 30 –º–∏–Ω
        self._prompt_cache_hits = 0
        self._prompt_cache_misses = 0
    
    @property
    def memory_manager(self):
        """–î–æ—Å—Ç—É–ø –∫ memory_manager –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏"""
        return self._memory_manager
    
    def _evict_prompt_cache_if_needed(self) -> None:
        """–£–¥–∞–ª—è–µ—Ç —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏ –∫—ç—à–∞ –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–∏ (LRU –ø–æ timestamp)."""
        if len(self._prompt_cache) < self._prompt_cache_max:
            return
        now = time.time()
        # –£–¥–∞–ª—è–µ–º –∏—Å—Ç—ë–∫—à–∏–µ
        expired = [k for k, ts in self._prompt_cache_meta.items() if (now - ts) >= self._prompt_cache_ttl]
        for k in expired:
            self._prompt_cache.pop(k, None)
            self._prompt_cache_meta.pop(k, None)
        if len(self._prompt_cache) >= self._prompt_cache_max:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –ø–æ timestamp
            sorted_keys = sorted(self._prompt_cache_meta.keys(), key=lambda x: self._prompt_cache_meta[x])
            for k in sorted_keys[: max(0, len(self._prompt_cache) - self._prompt_cache_max + 1)]:
                self._prompt_cache.pop(k, None)
                self._prompt_cache_meta.pop(k, None)
    
    _cached_ml_model = None  # –∫–ª–∞—Å—Å-—É—Ä–æ–≤–µ–Ω—å: –æ–¥–∏–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å (–ø–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ LocalAIRouter)
    _cached_ml_model_path = None

    def _load_ml_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ML-–º–æ–¥–µ–ª—å –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–∞. –ü–µ—Ä–µ–∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫—ç—à –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–ª–∞—Å—Å–∞ (–æ–¥–∏–Ω —Ä–∞–∑ –Ω–∞ –ø—Ä–æ—Ü–µ—Å—Å)."""
        if MLRouterModel and os.path.exists(self.ml_model_path):
            if LocalAIRouter._cached_ml_model_path == self.ml_model_path and LocalAIRouter._cached_ml_model is not None:
                self.ml_model = LocalAIRouter._cached_ml_model
                return
            try:
                self.ml_model = MLRouterModel()
                self.ml_model.load(self.ml_model_path)
                LocalAIRouter._cached_ml_model = self.ml_model
                LocalAIRouter._cached_ml_model_path = self.ml_model_path
                logger.info("‚úÖ [ML ROUTER] ML model loaded successfully (cached for process)")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [ML ROUTER] Failed to load ML model: {e}")
                self.ml_model = None
        else:
            logger.debug("‚ÑπÔ∏è [ML ROUTER] ML model not available, using heuristic routing")
    
    async def predict_optimal_route(
        self,
        prompt: str,
        category: Optional[str] = None
    ) -> tuple:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç –∏—Å–ø–æ–ª—å–∑—É—è ML-–º–æ–¥–µ–ª—å.
        
        Args:
            prompt: –ü—Ä–æ–º–ø—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–¥–∞—á–∏
        
        Returns:
            (predicted_route, confidence) –∏–ª–∏ (None, None) –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
        """
        if self.ml_model is None:
            return None, None
        
        try:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏
            task_type = self._determine_task_type(prompt, category)
            
            # –ü–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —É–∑–ª–æ–≤
            node_metrics = await self._get_node_performance_metrics()
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –º–∞—Ä—à—Ä—É—Ç
            predicted_route, confidence = self.ml_model.predict(
                task_type=task_type,
                prompt_length=len(prompt),
                category=category,
                node_metrics=node_metrics
            )
            
            logger.info(f"ü§ñ [ML ROUTER] Predicted route: {predicted_route} (confidence: {confidence:.2f})")
            return predicted_route, confidence
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è [ML ROUTER] Prediction error: {e}")
            return None, None

    async def check_health(self, force_refresh: bool = False) -> List[Dict]:
        """Check which nodes are alive and return their latency. Uses caching."""
        global _health_cache
        current_time = time.time()
        
        # Return cached result if still valid
        if not force_refresh and (current_time - _health_cache["timestamp"]) < _HEALTH_CACHE_TTL:
            return _health_cache["nodes"]
        
        healthy_nodes = []
        async with httpx.AsyncClient() as client:
            for node in self.nodes:
                try:
                    start_time = time.time()
                    # –ü—Ä–æ–±—É–µ–º –ª–µ–≥–∫–∏–π /health endpoint —Å–Ω–∞—á–∞–ª–∞ (–±—ã—Å—Ç—Ä–µ–µ, –±–µ–∑ rate limiting)
                    health_url = f"{node['url']}/health"
                    try:
                        response = await client.get(health_url, timeout=2.0)
                        if response.status_code == 200:
                            latency = time.time() - start_time
                            healthy_nodes.append({
                                **node,
                                "latency": latency,
                                "status": "online"
                            })
                            continue  # –£—Å–ø–µ—à–Ω–æ, –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —É–∑–ª—É
                    except Exception:
                        pass  # /health –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–±—É–µ–º /api/tags
                    
                    # Fallback –Ω–∞ /api/tags (–µ—Å–ª–∏ /health –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)
                    response = await client.get(f"{node['url']}/api/tags", timeout=2.0)
                    latency = time.time() - start_time
                    if response.status_code == 200:
                        healthy_nodes.append({
                            **node,
                            "latency": latency,
                            "status": "online"
                        })
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Node {node['name']} is offline: {e}")
        
        # –ù–µ –∫—ç—à–∏—Ä—É–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç ‚Äî —Å–ª–µ–¥—É—é—â–∞—è –ø–æ–ø—ã—Ç–∫–∞ —Å—Ä–∞–∑—É –ø–µ—Ä–µ–ø—Ä–æ–≤–µ—Ä–∏—Ç
        if not healthy_nodes:
            logger.warning("‚ö†Ô∏è [HEALTH] –ù–µ—Ç –∑–¥–æ—Ä–æ–≤—ã—Ö —É–∑–ª–æ–≤, –Ω–µ –∫—ç—à–∏—Ä—É–µ–º (–ø–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏ —Å–ª–µ–¥—É—é—â–µ–º –∑–∞–ø—Ä–æ—Å–µ)")
            return []
        
        # Get performance metrics from cache for each node
        performance_metrics = await self._get_node_performance_metrics()
        
        # Enhance nodes with performance data
        for node in healthy_nodes:
            routing_key = node.get('routing_key', '')
            if routing_key in performance_metrics:
                node['performance_score'] = performance_metrics[routing_key].get('avg_performance', 0.8)
                node['success_rate'] = performance_metrics[routing_key].get('success_rate', 0.9)
            else:
                node['performance_score'] = 0.8  # Default
                node['success_rate'] = 0.9  # Default
        
        # Sort by: performance_score (higher is better), then priority, then latency
        sorted_nodes = sorted(
            healthy_nodes, 
            key=lambda x: (-x.get('performance_score', 0.8), x['priority'], x['latency'])
        )
        
        # Update cache
        _health_cache = {"nodes": sorted_nodes, "timestamp": current_time}
        
        return sorted_nodes
    
    async def _get_node_performance_metrics(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —É–∑–ª–æ–≤ –∏–∑ semantic_ai_cache"""
        try:
            # Check cache first
            current_time = time.time()
            if hasattr(self, '_performance_cache') and self._performance_cache:
                cache_time = self._performance_cache.get('timestamp', 0)
                if (current_time - cache_time) < self._cache_ttl:
                    return self._performance_cache.get('metrics', {})
            
            # Try to connect to DB
            try:
                conn = await asyncpg.connect(DB_URL, timeout=2)
            except (asyncpg.PostgresError, OSError, ValueError):
                return {}
            
            try:
                # Check if columns exist
                columns_exist = await conn.fetchval("""
                    SELECT COUNT(*) FROM information_schema.columns 
                    WHERE table_name = 'semantic_ai_cache' 
                    AND column_name IN ('routing_source', 'performance_score')
                """) == 2
                
                if not columns_exist:
                    await conn.close()
                    return {}
                
                # Get performance metrics for each routing source (last 24 hours)
                metrics = await conn.fetch("""
                    SELECT 
                        routing_source,
                        AVG(performance_score) as avg_performance,
                        COUNT(*) as total_requests,
                        COUNT(*) FILTER (WHERE performance_score >= 0.7) as successful_requests
                    FROM semantic_ai_cache
                    WHERE routing_source IS NOT NULL
                    AND routing_source IN ('local_mac', 'local_server')
                    AND last_used_at > NOW() - INTERVAL '24 hours'
                    GROUP BY routing_source
                """)
                
                result = {}
                for row in metrics:
                    routing_key = row['routing_source']
                    total = row['total_requests'] or 0
                    successful = row['successful_requests'] or 0
                    result[routing_key] = {
                        'avg_performance': float(row['avg_performance'] or 0.8),
                        'success_rate': (successful / total) if total > 0 else 0.9,
                        'total_requests': total
                    }
                
                await conn.close()
                
                # Update cache
                self._performance_cache = {
                    'metrics': result,
                    'timestamp': current_time
                }
                
                return result
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Error getting performance metrics: {e}")
                await conn.close()
                return {}
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error in _get_node_performance_metrics: {e}")
            return {}

    def _is_echo_response(self, result: str, prompt: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –æ—Ç–≤–µ—Ç —ç—Ö–æ–º –ø—Ä–æ–º–ø—Ç–∞ (–º–æ–¥–µ–ª—å/—Å–µ—Ä–≤–µ—Ä –≤–µ—Ä–Ω—É–ª–∞ –∑–∞–ø—Ä–æ—Å –∫–∞–∫ –æ—Ç–≤–µ—Ç).
        –£—Å–ª–æ–≤–∏–µ –æ—Å–ª–∞–±–ª–µ–Ω–æ: –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç (<200 —Å–∏–º–≤–æ–ª–æ–≤) —Å—á–∏—Ç–∞–µ–º —ç—Ö–æ —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —è–≤–Ω–æ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–∏
        –∏–ª–∏ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –ø–æ—á—Ç–∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –Ω–∞—á–∞–ª–æ–º –ø—Ä–æ–º–ø—Ç–∞ (‚â•80% –¥–ª–∏–Ω—ã –æ—Ç–≤–µ—Ç–∞), —á—Ç–æ–±—ã
        –Ω–µ –æ—Ç–±—Ä–∞—Å—ã–≤–∞—Ç—å –ª–µ–≥–∏—Ç–∏–º–Ω—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã (¬´–î–∞¬ª, ¬´–ì–æ—Ç–æ–≤–æ¬ª)."""
        if not result or not prompt:
            return False
        r = result.strip()
        p = prompt.strip()
        if r == p:
            return True
        # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —ç—Ö–æ: —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ—Ç–≤–µ—Ç –∫–æ—Ä–æ—Ç–∫–∏–π –ò —è–≤–Ω–æ –∫–æ–ø–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç (–ø—Ä–æ–º–ø—Ç –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –æ—Ç–≤–µ—Ç–∞
        # –∏ –æ—Ç–≤–µ—Ç –Ω–µ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–ª—è –ª–µ–≥–∏—Ç–∏–º–Ω–æ–≥–æ ¬´–î–∞¬ª/¬´–û–∫¬ª ‚Äî –º–∏–Ω–∏–º—É–º 50 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —á–∞—Å—Ç–∏—á–Ω–æ–≥–æ —ç—Ö–æ)
        if len(r) < 200:
            if p.startswith(r) and len(r) >= 50:
                return True
            if r.startswith(p) and len(p) >= 50:
                return True
        return False
    
    def _is_simple_task(self, prompt: str, category: Optional[str] = None) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∑–∞–¥–∞—á–∞ –ø—Ä–æ—Å—Ç–æ–π (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama –ø—Ä–∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–µ MLX)"""
        # –ü—Ä–æ—Å—Ç—ã–µ –∑–∞–¥–∞—á–∏:
        # - –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã (< 500 —Å–∏–º–≤–æ–ª–æ–≤)
        # - –ü—Ä–æ—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (fast, simple_query, text_summarization)
        # - –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç (–±–µ–∑ —Å–ª–æ–∂–Ω–æ–π –ª–æ–≥–∏–∫–∏)
        # - –ù–µ —Ç—Ä–µ–±—É–µ—Ç reasoning
        
        prompt_lower = prompt.lower()
        
        # –ü—Ä–æ—Å—Ç—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        simple_categories = ["fast", "simple_query", "text_summarization", "grammar_correction"]
        if category in simple_categories:
            return True
        
        # –ö–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–æ–º–ø—Ç—ã
        if len(prompt) < 500:
            return True
        
        # –ü—Ä–æ—Å—Ç–æ–π —á–∞—Ç (–±–µ–∑ —Å–ª–æ–∂–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤)
        complex_keywords = ["–ø–æ–¥—É–º–∞–π", "–ª–æ–≥–∏–∫–∞", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞", "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è", "–∞–Ω–∞–ª–∏–∑", "–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ"]
        if not any(keyword in prompt_lower for keyword in complex_keywords):
            return True
        
        return False
    
    async def _refresh_available_models(self, force: bool = False) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫—ç—à –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å —Å–µ—Ä–≤–µ—Ä–æ–≤ (–µ—Å–ª–∏ –∏—Å—Ç—ë–∫ TTL –∏–ª–∏ force=True)"""
        global _cached_mlx_models, _cached_ollama_models, _models_cache_time
        
        now = time.time()
        if not force and (now - _models_cache_time) < _MODELS_CACHE_TTL:
            return  # –ö—ç—à –µ—â—ë –∞–∫—Ç—É–∞–ª–µ–Ω
        
        if _HAS_MODEL_SCANNER:
            try:
                mlx_url = MLX_API_URL
                ollama_url = OLLAMA_API_URL
                mlx_models, ollama_models = await get_available_models(mlx_url, ollama_url, force_refresh=force)
                _cached_mlx_models = mlx_models
                _cached_ollama_models = ollama_models
                _models_cache_time = now
                logger.info(f"üîÑ [MODEL SCAN] –û–±–Ω–æ–≤–ª–µ–Ω—ã –º–æ–¥–µ–ª–∏: MLX={len(mlx_models)}, Ollama={len(ollama_models)}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
    
    def _select_model(self, prompt: str, category: Optional[str] = None, use_ollama: bool = False, node_type: Optional[str] = None) -> str:
        """Select the best local model for the task.
        
        –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –í–´–ë–û–†: –ï—Å–ª–∏ available_models_scanner –¥–æ—Å—Ç—É–ø–µ–Ω, –≤—ã–±–∏—Ä–∞–µ—Ç –∏–∑ —Ä–µ–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.
        –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–¥–∞–ª–µ–Ω–∞/–¥–æ–±–∞–≤–ª–µ–Ω–∞, —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∞–¥–∞–ø—Ç–∏—Ä—É–µ—Ç—Å—è (–∫—ç—à –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∫–∞–∂–¥—ã–µ 2 –º–∏–Ω).
        
        Args:
            prompt: User prompt
            category: Task category
            use_ollama: If True, use Ollama models (deprecated)
            node_type: –¢–∏–ø —É–∑–ª–∞ ('mlx' –∏–ª–∏ 'ollama') - –¥–ª—è –≤—ã–±–æ—Ä–∞ –ø–æ–¥—Ö–æ–¥—è—â–µ–π –º–æ–¥–µ–ª–∏
        """
        prompt_lower = prompt.lower()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏–∑ –ø—Ä–æ–º–ø—Ç–∞ –µ—Å–ª–∏ –Ω–µ –∑–∞–¥–∞–Ω–∞ —è–≤–Ω–æ
        effective_category = category
        if not effective_category:
            if "–ø–æ–¥—É–º–∞–π" in prompt_lower or "–ª–æ–≥–∏–∫–∞" in prompt_lower or "–ø–ª–∞–Ω–∏—Ä" in prompt_lower:
                effective_category = "reasoning"
            elif "–ø—Ä–∏–≤–µ—Ç" in prompt_lower or "–∑–¥—Ä–∞–≤—Å—Ç–≤" in prompt_lower:
                effective_category = "general"
            elif "–∫–æ–¥" in prompt_lower or "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–π" in prompt_lower:
                effective_category = "coding"
            elif "–∏–∑–æ–±—Ä–∞–∂–µ–Ω" in prompt_lower or "–∫–∞—Ä—Ç–∏–Ω–∫" in prompt_lower:
                effective_category = "vision"
            elif len(prompt) < 300:
                effective_category = "fast"
            else:
                effective_category = "default"
        
        # ========== –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ô –í–´–ë–û–† –ú–û–î–ï–õ–ò ==========
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º scanner –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ fallback
        
        if _HAS_MODEL_SCANNER and (_cached_mlx_models or _cached_ollama_models):
            # –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –∏–∑ —Ä–µ–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            if node_type == "mlx" and _cached_mlx_models:
                model = pick_mlx_for_category(effective_category, _cached_mlx_models)
                if model:
                    logger.debug(f"üéØ [DYNAMIC] MLX: {model} –¥–ª—è {effective_category}")
                    return model
            elif node_type == "ollama" and _cached_ollama_models:
                model = pick_ollama_for_category(effective_category, _cached_ollama_models)
                if model:
                    logger.debug(f"üéØ [DYNAMIC] Ollama: {model} –¥–ª—è {effective_category}")
                    return model
        
        # ========== FALLBACK: –°—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä ==========
        # –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –µ—Å–ª–∏ scanner –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –∏–ª–∏ –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã
        
        if effective_category == "reasoning":
            if node_type == "mlx":
                return MLX_MODELS_FALLBACK["reasoning"]
            else:
                return OLLAMA_MODELS_FALLBACK["reasoning"]
        
        if effective_category == "general":
            if node_type == "mlx":
                return MLX_MODELS_FALLBACK["chat"]
            else:
                return OLLAMA_MODELS_FALLBACK["chat"]
        
        if effective_category == "coding":
            if node_type == "mlx":
                return MLX_MODELS_FALLBACK["coding"]
            else:
                return OLLAMA_MODELS_FALLBACK["coding"]
        
        if effective_category == "fast":
            if node_type == "mlx":
                return MLX_MODELS_FALLBACK["fast"]
            else:
                return OLLAMA_MODELS_FALLBACK["fast"]
        
        if effective_category == "vision":
            return OLLAMA_MODELS_FALLBACK["vision"]
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if node_type == "mlx":
            return MLX_MODELS_FALLBACK["default"]
        else:
            return OLLAMA_MODELS_FALLBACK["default"]
    
    async def _is_mlx_overloaded(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω –ª–∏ MLX API Server"""
        try:
            from app.mlx_request_queue import get_request_queue
            queue = get_request_queue()
            stats = queue.get_stats()
            
            # MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω –µ—Å–ª–∏:
            # - –í—Å–µ —Å–ª–æ—Ç—ã –∑–∞–Ω—è—Ç—ã (active_requests >= max_concurrent)
            # - –ï—Å—Ç—å –æ—á–µ—Ä–µ–¥—å (queue_size > 0)
            is_overloaded = (
                stats.get("active_requests", 0) >= stats.get("max_concurrent", 5) or
                stats.get("queue_size", 0) > 0
            )
            
            if is_overloaded:
                logger.debug(
                    f"‚ö†Ô∏è MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω: –∞–∫—Ç–∏–≤–Ω—ã—Ö={stats.get('active_requests')}/"
                    f"{stats.get('max_concurrent')}, –æ—á–µ—Ä–µ–¥—å={stats.get('queue_size')}"
                )
            
            return is_overloaded
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∑–∞–≥—Ä—É–∑–∫—É MLX: {e}")
            return False  # –ï—Å–ª–∏ –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å—á–∏—Ç–∞–µ–º —á—Ç–æ –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω

    def should_use_local(self, prompt: str, category: Optional[str] = None, images: Optional[list] = None) -> bool:
        """Determine if the task should be routed to local LLM. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Ollama/MLX)."""
        if not self.use_local:
            logger.debug("[LOCAL ROUTER] should_use_local=False: USE_LOCAL_LLM –æ—Ç–∫–ª—é—á–µ–Ω")
            return False
        
        # If images are provided, we MUST use local vision model (e.g., moondream)
        if images:
            return True
        
        # –ï—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞–¥–∞–Ω–∞ ‚Äî –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–æ—Ä–∫–µ—Å—Ç—Ä–∞—Ç–æ—Ä, –≤–æ—Ä–∫–µ—Ä, –∫–æ–¥–∏–Ω–≥ –∏ —Ç.–¥.)
        if category in LOCAL_TASK_CATEGORIES or category in MODEL_MAP:
            return True
        if category in ("autonomous_worker", "orchestrator", "general", "research", "reasoning", "coding", "fast"):
            return True
        
        # Heuristic based on prompt content
        prompt_lower = prompt.lower()
        if any(keyword in prompt_lower for keyword in ["–∞–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤", "–ø—Ä–æ–≤–µ—Ä—å –∫–æ–¥", "–Ω–∞–ø–∏—à–∏ —Ç–µ—Å—Ç", "—Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–π", "–∏—Å–ø—Ä–∞–≤—å –æ–ø–µ—á–∞—Ç–∫—É"]):
            return True
            
        # If the prompt is very large (context-heavy) and doesn't require high-level reasoning
        if len(prompt) > 2000 and "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞" not in prompt_lower and "—Å—Ç—Ä–∞—Ç–µ–≥–∏—è" not in prompt_lower:
            return True
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π ‚Äî –≤—Å—ë —Ä–∞–≤–Ω–æ –ø—Ä–æ–±—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏)
        if category is not None:
            return True
            
        return False

    async def run_local_llm(self, prompt: str, system_prompt: str = "", category: Optional[str] = None, images: Optional[list] = None, max_retries: int = 2, model: Optional[str] = None, model_hint: Optional[str] = None, is_vip: bool = False) -> Optional[tuple]:
        """
        –ó–∞–ø—É—Å–∫–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é LLM –º–æ–¥–µ–ª—å.
        –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: MLX API Server (HTTP) –∏ Ollama ‚Äî –æ–±–∞ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è (–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞).
        model: –µ—Å–ª–∏ –∑–∞–¥–∞–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç—Ç—É –º–æ–¥–µ–ª—å –∏ –ø–µ—Ä–µ–±–∏—Ä–∞–µ–º —É–∑–ª—ã (MLX/Ollama) –ø–æ–∫–∞ –æ–¥–∏–Ω –Ω–µ –æ—Ç–≤–µ—Ç–∏—Ç.
        model_hint: –ø–æ–¥—Å–∫–∞–∑–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∞–Ω—Å–∞–º–±–ª–µ–º).
        is_vip: –µ—Å–ª–∏ True, –∑–∞–ø—Ä–æ—Å –∏–¥–µ—Ç —á–µ—Ä–µ–∑ VIP-–∫–æ—Ä–∏–¥–æ—Ä (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∏ –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏).
        
        Returns:
            tuple: (response, routing_source)
        """
        # VIP-–∫–æ—Ä–∏–¥–æ—Ä: —Ñ–æ—Ä—Å–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
        if is_vip:
            category = "vip"
            logger.info("üåü [VIP ROUTE] –ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ VIP-–∫–æ—Ä–∏–¥–æ—Ä (–ò–≤–∞–Ω/–°–æ–≤–µ—Ç)")

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–¥–∞–Ω–∞ —è–≤–Ω–æ
        if model is None and model_hint:
            model = model_hint

        # –ú–û–ù–°–¢–†-–õ–û–ì–ò–ö–ê: –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–æ—É—Ç–∏–Ω–≥–∞
        if getattr(self, 'force_local', False):
            logger.info("üöÄ [MONSTER] –§–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥ –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.")
        logger.info("[ROUTER] ========== LocalAIRouter.run_local_llm() ==========")
        logger.info("[ROUTER] Input model: %s", model)
        logger.info("[ROUTER] Category: %s", category)
        logger.info("[ROUTER] Prompt length: %d chars", len(prompt))
        logger.info("[ROUTER] Prompt preview: %s...", prompt[:150])
        
        # üîÑ –î–ò–ù–ê–ú–ò–ß–ï–°–ö–û–ï –û–ë–ù–û–í–õ–ï–ù–ò–ï –°–ü–ò–°–ö–ê –ú–û–î–ï–õ–ï–ô (–µ—Å–ª–∏ –∏—Å—Ç—ë–∫ TTL)
        # –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–¥—Ö–≤–∞—Ç—ã–≤–∞—Ç—å –Ω–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∑–∞–º–µ—á–∞—Ç—å —É–¥–∞–ª—ë–Ω–Ω—ã–µ
        await self._refresh_available_models()
        
        # –ü–†–ò–û–†–ò–¢–ï–¢: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å MLX API Server –∏ Ollama —á–µ—Ä–µ–∑ HTTP —Ä–æ—É—Ç–∏–Ω–≥
        # MLX Router –Ω–∞–ø—Ä—è–º—É—é –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ (—Ç—Ä–µ–±—É–µ—Ç –º–æ–¥—É–ª—å mlx)
        # –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º MLX API Server —á–µ—Ä–µ–∑ HTTP (—É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ nodes)
        """Call local LLM (Ollama style) with automatic failover, retry logic and node selection."""
        
        # –ú–æ–¥–µ–ª—å: –ø–∞—Ä–∞–º–µ—Ç—Ä –≤—ã–∑–æ–≤–∞, –∏–ª–∏ _preferred_model –æ—Ç –≤–æ—Ä–∫–µ—Ä–∞ (–±–∞—Ç—á–∏ –ø–æ –º–æ–¥–µ–ª–∏ ‚Äî –º–µ–Ω—å—à–µ load/unload)
        initial_model = model or getattr(self, '_preferred_model', None)
        
        if images and MODEL_MAP.get("vision"):
            model = MODEL_MAP["vision"]
            logger.info("[ROUTER] Using vision model: %s", model)
            initial_model = model  # Vision - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ
        
        # –ö—ç—à: —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö –ø—Ä–æ–º–ø—Ç–æ–≤ –±–µ–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        prompt_cache_key = None
        if len(prompt) <= 1000 and not images:
            raw_key = f"{prompt}|{category or ''}|{model or ''}"
            prompt_cache_key = hashlib.sha256(raw_key.encode()).hexdigest()[:32]
            now = time.time()
            if prompt_cache_key in self._prompt_cache:
                ts = self._prompt_cache_meta.get(prompt_cache_key, 0)
                if (now - ts) < self._prompt_cache_ttl:
                    self._prompt_cache_hits += 1
                    logger.debug("‚úÖ [CACHE HIT] LocalAIRouter prompt cache")
                    return self._prompt_cache[prompt_cache_key]
            self._prompt_cache_misses += 1
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        task_type = self._determine_task_type(prompt, category)
        
        # A/B Testing: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ ML-—Ä–æ—É—Ç–∏–Ω–≥
        use_ml_routing = False
        if get_ab_test and self.ml_model:
            ab_test = await get_ab_test(ml_ratio=0.5)  # 50% ML, 50% heuristic
            use_ml_routing = ab_test.should_use_ml()
        
        # ML Prediction: –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –º–∞—Ä—à—Ä—É—Ç (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω A/B —Ç–µ—Å—Ç)
        ml_predicted_route = None
        ml_confidence = None
        if use_ml_routing and self.ml_model:
            ml_predicted_route, ml_confidence = await self.predict_optimal_route(prompt, category)
            if ml_predicted_route and ml_confidence > 0.7:
                logger.info(f"ü§ñ [ML ROUTER] Using ML prediction: {ml_predicted_route} (confidence: {ml_confidence:.2f})")
        elif self.ml_model:
            logger.debug("üìä [HEURISTIC ROUTER] A/B test: using heuristic routing")
        
        # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è memory_manager (—Ç–æ–ª—å–∫–æ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏)
        if self._memory_manager is None and self._memory_manager_url:
            try:
                from model_memory_manager import get_memory_manager
                self._memory_manager = get_memory_manager(self._memory_manager_url)
                logger.debug("‚úÖ ModelMemoryManager –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (–ª–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)")
            except Exception as e:
                logger.debug(f"‚ö†Ô∏è ModelMemoryManager –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                self._memory_manager = None
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –≤—ã–±–æ—Ä–æ–º —É–∑–ª–∞ (–¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞)
        if self._memory_manager:
            available_mb = await self._memory_manager.get_available_memory_mb()
            if available_mb < 200:  # MIN_FREE_MEMORY_MB
                logger.warning(f"‚ö†Ô∏è [MEMORY] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –Ω–µ—Ö–≤–∞—Ç–∫–∞ –ø–∞–º—è—Ç–∏: {available_mb}MB, –∑–∞–ø—É—Å–∫–∞–µ–º –æ—á–∏—Å—Ç–∫—É...")
                await self._memory_manager.emergency_memory_cleanup()
        
        # Discover healthy nodes (with caching)
        healthy_nodes = await self.check_health()
        if not healthy_nodes:
            logger.error("‚ùå No healthy local nodes found!")
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏–µ –æ fallback –≤ –æ–±–ª–∞–∫–æ
            if get_collector:
                collector = await get_collector()
                await collector.collect_routing_decision(
                    task_type=task_type,
                    prompt_length=len(prompt),
                    category=category,
                    selected_route="cloud",  # Fallback –≤ –æ–±–ª–∞–∫–æ
                    success=False,
                    features={"reason": "no_healthy_nodes", "ml_predicted": ml_predicted_route}
                )
            return None
        
        # –£–ú–ù–´–ô –í–´–ë–û–† –£–ó–õ–ê: —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–∞ –≤—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á–∏
        # 1. Load Balancing: –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —É–∑–µ–ª –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∑–∫–∏ (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if get_load_balancer:
            load_balancer = get_load_balancer()
            best_node = load_balancer.select_best_node(healthy_nodes)
            if best_node and best_node != healthy_nodes[0]:
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ª—É—á—à–∏–π —É–∑–µ–ª –≤ –Ω–∞—á–∞–ª–æ
                healthy_nodes.remove(best_node)
                healthy_nodes.insert(0, best_node)
                logger.info(f"‚öñÔ∏è [LOAD BALANCER] –í—ã–±—Ä–∞–Ω —É–∑–µ–ª: {best_node['name']} –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∑–∫–∏")
        
        # 2. ML Prediction: –µ—Å–ª–∏ ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–ª –º–∞—Ä—à—Ä—É—Ç, —É—á–∏—Ç—ã–≤–∞–µ–º –µ–≥–æ
        if ml_predicted_route and ml_predicted_route != "cloud":
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º —É–∑–ª—ã, —á—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –±—ã–ª –ø–µ—Ä–≤—ã–º (–Ω–æ –ø–æ—Å–ª–µ load balancer)
            predicted_node = None
            other_nodes = []
            for node in healthy_nodes:
                if node.get('routing_key') == ml_predicted_route:
                    predicted_node = node
                else:
                    other_nodes.append(node)
            
            if predicted_node and predicted_node != healthy_nodes[0]:
                # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π —É–∑–µ–ª –Ω–∞ –≤—Ç–æ—Ä–æ–µ –º–µ—Å—Ç–æ (–ø–æ—Å–ª–µ load balancer)
                healthy_nodes.remove(predicted_node)
                healthy_nodes.insert(1, predicted_node)
                logger.info(f"ü§ñ [ML ROUTER] –£—á–∏—Ç—ã–≤–∞–µ–º ML-–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_node['name']}")
        
        # 3. –í—ã–±–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–∏–ø–∞ –∑–∞–¥–∞—á–∏ (reasoning ‚Üí MLX, fast ‚Üí Ollama, –∏ —Ç.–¥.)
        # –≠—Ç–æ —É–∂–µ –¥–µ–ª–∞–µ—Ç—Å—è –≤ _select_model –Ω–∞ –æ—Å–Ω–æ–≤–µ node_type
        
        # 4. –ë–ê–õ–ê–ù–°–ò–†–û–í–ö–ê: –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º —É–∑–ª—ã –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MLX –∏ Ollama
        mlx_nodes = [n for n in healthy_nodes if "11435" in n['url'] or "mlx" in n['url'].lower()]
        ollama_nodes = [n for n in healthy_nodes if "11434" in n['url'] or "ollama" in n['url'].lower()]
        other_nodes = [n for n in healthy_nodes if n not in mlx_nodes and n not in ollama_nodes]
        # –õ–æ–≥–∏—Ä—É–µ–º —Ä–∞–∑ –≤ 5 –º–∏–Ω, –µ—Å–ª–∏ MLX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî —á—Ç–æ–±—ã –±—ã–ª–æ –≤–∏–¥–Ω–æ, —á—Ç–æ –∑–∞–¥–∞—á–∏ –∏–¥—É—Ç —Ç–æ–ª—å–∫–æ –≤ Ollama
        if not mlx_nodes and ollama_nodes:
            _t = time.time()
            if not hasattr(LocalAIRouter, "_last_no_mlx_log") or (_t - LocalAIRouter._last_no_mlx_log) > 300:
                logger.warning(
                    "‚ö†Ô∏è [ROUTER] MLX API Server (–ø–æ—Ä—Ç 11435) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ‚Äî –≤—Å–µ –∑–∞–¥–∞—á–∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ Ollama. "
                    "–ó–∞–ø—É—Å–∫: scripts/start_mlx_api_server.sh; –ø—Ä–æ–≤–µ—Ä–∫–∞: curl -s http://localhost:11435/health"
                )
                LocalAIRouter._last_no_mlx_log = _t
        prefer_ollama_due_to_mlx_overload = False
        
        # 4.1 –ï—Å–ª–∏ MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω (–æ—á–µ—Ä–µ–¥—å/rate limit) ‚Äî –ø—Ä–æ–±—É–µ–º Ollama –ø–µ—Ä–≤—ã–º
        if mlx_nodes and ollama_nodes:
            try:
                if await self._is_mlx_overloaded():
                    prefer_ollama_due_to_mlx_overload = True
                    healthy_nodes = ollama_nodes + mlx_nodes + other_nodes
                    logger.info(
                        "üîÑ [ROUTER] MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω ‚Äî –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç Ollama –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ "
                        "(–º–µ–Ω—å—à–µ 429, –±—ã—Å—Ç—Ä–µ–µ –æ—Ç–≤–µ—Ç)"
                    )
            except Exception as e:
                logger.debug(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏ MLX: {e}")
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –æ–±–∞ —Ç–∏–ø–∞ —É–∑–ª–æ–≤ –∏ –Ω–µ –ø–æ—Å—Ç–∞–≤–∏–ª–∏ Ollama –ø–µ—Ä–≤—ã–º ‚Äî —á–µ—Ä–µ–¥—É–µ–º –∏—Ö
        if mlx_nodes and ollama_nodes and not prefer_ollama_due_to_mlx_overload:
            balanced_nodes = []
            max_len = max(len(mlx_nodes), len(ollama_nodes))
            for i in range(max_len):
                if i < len(mlx_nodes):
                    balanced_nodes.append(mlx_nodes[i])
                if i < len(ollama_nodes):
                    balanced_nodes.append(ollama_nodes[i])
            # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É–∑–ª—ã –≤ –∫–æ–Ω–µ—Ü
            balanced_nodes.extend(other_nodes)
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ—Ä—è–¥–æ–∫, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–≤—ã–π —É–∑–µ–ª –æ—Ç load balancer –µ—Å–ª–∏ –æ–Ω –±—ã–ª –≤—ã–±—Ä–∞–Ω
            if healthy_nodes and balanced_nodes:
                first_node = healthy_nodes[0]
                if first_node in balanced_nodes:
                    balanced_nodes.remove(first_node)
                    balanced_nodes.insert(0, first_node)
                healthy_nodes = balanced_nodes
                logger.info(f"‚öñÔ∏è [BALANCED] –ü–µ—Ä–µ–º–µ—à–∞–Ω—ã —É–∑–ª—ã –¥–ª—è —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è MLX ({len(mlx_nodes)}) –∏ Ollama ({len(ollama_nodes)})")

        # –£–º–Ω—ã–π –≤—ã–±–æ—Ä —É–∑–ª–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–¥–∞—á–∏ –∏ –∑–∞–≥—Ä—É–∑–∫–∏
        # –°–∏—Å—Ç–µ–º–∞ —Å–∞–º–∞ –≤—ã–±–µ—Ä–µ—Ç –ª—É—á—à–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ (MLX –∏–ª–∏ Ollama) –Ω–∞ –æ—Å–Ω–æ–≤–µ:
        # 1. –¢–∏–ø–∞ –∑–∞–¥–∞—á–∏ (reasoning ‚Üí –º–æ—â–Ω–∞—è –º–æ–¥–µ–ª—å, fast ‚Üí –ª–µ–≥–∫–∞—è)
        # 2. –ó–∞–≥—Ä—É–∑–∫–∏ —É–∑–ª–∞ (load balancing)
        # 3. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
        # 4. –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –º–µ–∂–¥—É MLX –∏ Ollama
        
        # –£–õ–£–ß–®–ï–ù–ò–ï: –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫ (–∏–∑ worker'–∞), –ø—Ä–æ–±—É–µ–º –µ–≥–æ –ø–µ—Ä–≤—ã–º
        preferred_source = getattr(self, '_preferred_source', None)
        if preferred_source:
            # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —É–∑–µ–ª –≤ –Ω–∞—á–∞–ª–æ
            preferred_nodes = [n for n in healthy_nodes if 
                              (preferred_source == 'mlx' and ("11435" in n['url'] or "mlx" in n['url'].lower())) or
                              (preferred_source == 'ollama' and ("11434" in n['url'] or "ollama" in n['url'].lower()))]
            if preferred_nodes:
                for node in preferred_nodes:
                    if node in healthy_nodes:
                        healthy_nodes.remove(node)
                        healthy_nodes.insert(0, node)
                logger.info(f"üéØ [PREFERRED] –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫: {preferred_source}")
        
        # Try each node with retry logic
        start_time = time.time()
        for node in healthy_nodes:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —ç—Ç–æ Ollama –∏–ª–∏ MLX API Server
            is_ollama = "11434" in node['url'] or "ollama" in node['url'].lower()
            is_mlx = "11435" in node['url'] or "mlx" in node['url'].lower()
            
            # –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–´–ô –í–´–ë–û–† –ú–û–î–ï–õ–ò –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–∏—Ä–æ–≤—ã—Ö –ø—Ä–∞–∫—Ç–∏–∫
            # –í–ê–ñ–ù–û: MLX –∏ Ollama –∏–º–µ—é—Ç –†–ê–ó–ù–´–ï –º–æ–¥–µ–ª–∏! –í—ã–±–∏—Ä–∞–µ–º –¥–ª—è –ö–ê–ñ–î–û–ì–û —É–∑–ª–∞ –æ—Ç–¥–µ–ª—å–Ω–æ!
            node_type = "mlx" if is_mlx else "ollama" if is_ollama else "unknown"
            
            # 1. –ú–æ–¥–µ–ª—å –æ—Ç –≤–æ—Ä–∫–µ—Ä–∞ (_preferred_model –ø—Ä–∏ –±–∞—Ç—á–∞—Ö –ø–æ –º–æ–¥–µ–ª–∏) –∏–ª–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä –≤—ã–∑–æ–≤–∞
            current_model = None
            if initial_model:
                # –í–æ—Ä–∫–µ—Ä –∑–∞–¥–∞—ë—Ç –º–æ–¥–µ–ª—å –ø–æ —Å–∫–∞–Ω–µ—Ä—É (source+model) ‚Äî –ø—Ä–∏–Ω–∏–º–∞–µ–º –¥–ª—è –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ —Ç–∏–ø–∞ —É–∑–ª–∞
                if is_mlx or is_ollama:
                    current_model = initial_model
                else:
                    if initial_model in (list(MLX_MODELS.values()) + list(OLLAMA_MODELS.values())):
                        current_model = initial_model
                    else:
                        logger.debug(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {initial_model} –Ω–µ –≤ fallback –¥–ª—è {node_type}, –≤—ã–±–∏—Ä–∞–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
                        current_model = None
            
            # 2. –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–¥–∞–Ω–∞ –∏–ª–∏ –Ω–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–∞ - –≤—ã–±–∏—Ä–∞–µ–º –¥–ª—è —ç—Ç–æ–≥–æ —É–∑–ª–∞
            if not current_model:
                # –í–ê–ñ–ù–û: –î–ª—è REASONING –∑–∞–¥–∞—á –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º _select_model()
                # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (scanner –∏–ª–∏ fallback; MLX —Ç–æ–ª—å–∫–æ –ª—ë–≥–∫–∏–µ)
                if category == "reasoning":
                    current_model = self._select_model(prompt, category, node_type=node_type)
                    logger.info(f"üéØ [REASONING] –£–∑–µ–ª: {node_type} | –ú–æ–¥–µ–ª—å: {current_model} (–ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω—ã–π –≤—ã–±–æ—Ä –¥–ª—è reasoning)")
                else:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º (–±—ã—Å—Ç—Ä—ã–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–º–∏)
                    # Intelligent router –æ—Ç–∫–ª—é—á—ë–Ω ‚Äî –æ–Ω –≤—ã–±–∏—Ä–∞–µ—Ç —Ç—è–∂—ë–ª—ã–µ –º–æ–¥–µ–ª–∏ –∏ –ø–µ—Ä–µ–≥—Ä—É–∂–∞–µ—Ç —Å–∏—Å—Ç–µ–º—É
                    use_intelligent = os.getenv('USE_INTELLIGENT_ROUTER', 'false').lower() in ('true', '1', 'yes')
                    if use_intelligent:
                        try:
                            from intelligent_model_router import get_intelligent_router
                            intelligent_router = get_intelligent_router()
                            
                            available_models = []
                            if is_mlx:
                                available_models = list(MLX_MODELS.values())
                            elif is_ollama:
                                available_models = list(OLLAMA_MODELS.values())
                            
                            if available_models:
                                optimize_mode = os.getenv('INTELLIGENT_ROUTER_OPTIMIZE', 'speed')
                                optimal_model, _task_cat, confidence = await intelligent_router.select_optimal_model(
                                    prompt=prompt,
                                    category=category or "",
                                    available_models=available_models,
                                    optimize_for=optimize_mode
                                )
                                
                                if optimal_model and confidence > 0.5:
                                    current_model = optimal_model
                                    logger.info(f"üß† [INTELLIGENT ROUTER] –£–∑–µ–ª: {node_type} | –ú–æ–¥–µ–ª—å: {current_model} (confidence: {confidence:.2f})")
                                else:
                                    current_model = self._select_model(prompt, category, node_type=node_type)
                            else:
                                current_model = self._select_model(prompt, category, node_type=node_type)
                        except Exception as e:
                            logger.debug(f"Intelligent router failed: {e}, using fallback")
                            current_model = self._select_model(prompt, category, node_type=node_type)
                    else:
                        # –ü—Ä–æ—Å—Ç–æ–π –≤—ã–±–æ—Ä –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º (–ª—ë–≥–∫–∏–µ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–≤—ã–º–∏)
                        current_model = self._select_model(prompt, category, node_type=node_type)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–æ–≥–æ —É–∑–ª–∞
            model = current_model
            logger.info(f"üéØ [SMART SELECTION] –£–∑–µ–ª: {node['name']} | –ú–æ–¥–µ–ª—å: {model} | –¢–∏–ø –∑–∞–¥–∞—á–∏: {category or 'auto'}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º /api/chat –¥–ª—è Ollama (–±–æ–ª–µ–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π endpoint)
            if is_ollama or is_mlx:
                node_url = f"{node['url']}/api/chat"
                logger.info(f"üè† [LOCAL ROUTE] Node: {node['name']} | Model: {model} | Endpoint: /api/chat")
                
                messages = []
                if system_prompt:
                    messages.append({"role": "system", "content": system_prompt})
                messages.append({"role": "user", "content": prompt})
                
                payload = {
                    "model": model,
                    "messages": messages,
                    "stream": False
                }
                payload["keep_alive"] = _get_keep_alive(model)
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ–º /api/generate
                node_url = f"{node['url']}/api/generate"
                logger.info(f"üè† [LOCAL ROUTE] Node: {node['name']} | Model: {model} | Endpoint: /api/generate")
                
                full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
                payload = {
                    "model": model,
                    "prompt": full_prompt,
                    "stream": False
                }
                payload["keep_alive"] = _get_keep_alive(model)
            if images:
                payload["images"] = images
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ—Å—É—Ä—Å–æ–≤ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º (–æ—Å–æ–±–µ–Ω–Ω–æ –¥–ª—è MLX)
            if is_mlx:
                try:
                    from resource_monitor import get_resource_monitor
                    monitor = get_resource_monitor()
                    mlx_health = await monitor.get_mlx_health()
                    
                    # –ï—Å–ª–∏ MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –µ–≥–æ
                    if monitor.should_throttle_mlx(mlx_health):
                        logger.warning(
                            f"‚ö†Ô∏è [RESOURCE] MLX –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω: "
                            f"RAM={mlx_health.get('system', {}).get('ram', {}).get('used_percent', 0):.1f}%, "
                            f"CPU={mlx_health.get('system', {}).get('cpu', {}).get('percent', 0):.1f}%, "
                            f"Active={mlx_health.get('active_requests', 0)}/{mlx_health.get('max_concurrent', 5)}"
                        )
                        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —ç—Ç–æ—Ç —É–∑–µ–ª, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π
                except Exception as e:
                    logger.debug(f"Resource monitoring failed: {e}")
            
            # Load Balancing: –æ—Ç–º–µ—á–∞–µ–º –Ω–∞—á–∞–ª–æ –∑–∞–ø—Ä–æ—Å–∞
            if get_load_balancer:
                load_balancer = get_load_balancer()
                load_balancer.start_request(node.get('routing_key', ''))
            
            # Retry logic with exponential backoff
            for attempt in range(max_retries + 1):
                try:
                    async with httpx.AsyncClient() as client:
                        request_start = time.time()
                        headers = {"X-Request-Priority": "high"}
                        # –¢–∞–π–º–∞—É—Ç –ø–æ –º–µ—Ç—Ä–∏–∫–∞–º —ç—Ç–æ–π –º–æ–¥–µ–ª–∏ (load/processing —Å –∑–∞–ø–∞—Å–æ–º); —É –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ —Å–≤–æ–∏ –∑–Ω–∞—á–µ–Ω–∏—è
                        _node_timeout = float(os.getenv("LOCAL_ROUTER_LLM_TIMEOUT", "300"))
                        
                        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–ª—è reasoning –∑–∞–¥–∞—á (–°–æ–≤–µ—Ç, —Å—Ç—Ä–∞—Ç–µ–≥–∏—è)
                        if category == "reasoning":
                            _node_timeout = max(_node_timeout, 600.0)
                            logger.info(f"üïí [REASONING] –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç –¥–æ {_node_timeout}—Å")

                        # –ú–û–ù–°–¢–†-–õ–û–ì–ò–ö–ê: –ï—Å–ª–∏ —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –¥–æ 10 –º–∏–Ω—É—Ç
                        if getattr(self, 'force_local', False):
                            _node_timeout = 600.0
                            logger.info("üöÄ [MONSTER] –£–≤–µ–ª–∏—á–µ–Ω —Ç–∞–π–º–∞—É—Ç HTTP –¥–æ 600—Å –¥–ª—è —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —Ä–æ—É—Ç–∏–Ω–≥–∞.")
                        
                        try:
                            from available_models_scanner import get_model_metrics
                            from app.model_performance_probe import get_timeout_estimate_from_metrics_dict
                            _source = "ollama" if node.get("routing_key") == "ollama_studio" else "mlx"
                            _m = get_model_metrics(model, _source)
                            if _m:
                                _node_timeout = get_timeout_estimate_from_metrics_dict(
                                    max_tokens=2048, metrics_dict=_m
                                )
                        except Exception:
                            pass
                        
                        # –ú–û–ù–°–¢–†-–õ–û–ì–ò–ö–ê: –ï—Å–ª–∏ —Ñ–æ—Ä—Å–∏—Ä–æ–≤–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥ –∏–ª–∏ —ç—Ç–æ REASONING/VIP, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è ReadTimeout
                        if getattr(self, 'force_local', False) or category in ("reasoning", "vip"):
                            logger.info(f"üöÄ [STREAMING] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è (Heartbeat) [Category: {category}]...")
                            full_response = []
                            
                            # –í–∫–ª—é—á–∞–µ–º —Å—Ç—Ä–∏–º–∏–Ω–≥ –≤ –ø–æ–ª–µ–∑–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ
                            payload["stream"] = True
                            
                            async with client.stream("POST", node_url, json=payload, headers=headers, timeout=httpx.Timeout(600.0, connect=30.0)) as response:
                                if response.status_code == 200:
                                    async for line in response.aiter_lines():
                                        if not line: continue
                                        try:
                                            chunk = json.loads(line)
                                            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ Ollama/MLX
                                            if "message" in chunk:
                                                content = chunk["message"].get("content", "")
                                            elif "response" in chunk:
                                                content = chunk.get("response", "")
                                            else:
                                                content = ""
                                            
                                            if content:
                                                full_response.append(content)
                                            
                                            if chunk.get("done"): break
                                        except Exception:
                                            continue
                                    
                                    result = "".join(full_response)
                                    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π –æ–±—ä–µ–∫—Ç –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∫–æ–¥–æ–º –Ω–∏–∂–µ
                                    class MockResponse:
                                        def __init__(self, text, status_code):
                                            self.text = text
                                            self.status_code = status_code
                                        def json(self):
                                            return {"message": {"content": self.text}}
                                    
                                    response = MockResponse(result, 200)
                                else:
                                    # –ï—Å–ª–∏ –æ—à–∏–±–∫–∞ ‚Äî —Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏
                                    error_text = await response.aread()
                                    logger.error(f"Streaming error: {response.status_code}")
                        else:
                            # –û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ª–µ–≥–∫–∏—Ö –∑–∞–¥–∞—á
                            response = await client.post(
                                node_url,
                                json=payload,
                                headers=headers,
                                timeout=httpx.Timeout(_node_timeout, connect=30.0)
                            )
                        latency_ms = (time.time() - request_start) * 1000
                        
                        # Load Balancing: –æ–±–Ω–æ–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏
                        if get_load_balancer:
                            load_balancer = get_load_balancer()
                            load_balancer.update_node_load(
                                node['name'],
                                node.get('routing_key', ''),
                                latency_ms / 1000.0,  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–µ–∫—É–Ω–¥—ã
                                success=(response.status_code == 200)
                            )
                            load_balancer.end_request(node.get('routing_key', ''))
                        
                        logger.info("[ROUTER] HTTP response status: %d from %s", response.status_code, node['name'])
                        
                        if response.status_code == 200:
                            result_data = response.json()
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤
                            if "message" in result_data:
                                # –§–æ—Ä–º–∞—Ç /api/chat
                                result = result_data["message"].get("content", "")
                                logger.info("[ROUTER] Response format: /api/chat, content length: %d", len(result))
                            elif "response" in result_data:
                                # –§–æ—Ä–º–∞—Ç /api/generate
                                result = result_data.get("response", "")
                                logger.info("[ROUTER] Response format: /api/generate, content length: %d", len(result))
                            else:
                                result = str(result_data)
                                logger.info("[ROUTER] Response format: unknown, raw data length: %d", len(result))
                            
                            logger.info("[ROUTER] Response preview: %s...", result[:200] if result else "(empty)")
                            
                            # –ó–∞—â–∏—Ç–∞ –æ—Ç —ç—Ö–æ: –µ—Å–ª–∏ —Å–µ—Ä–≤–µ—Ä/–º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—Ä–æ–º–ø—Ç –∫–∞–∫ –æ—Ç–≤–µ—Ç ‚Äî –Ω–µ —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—Ö–æ–º, –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —É–∑–µ–ª
                            if result and self._is_echo_response(result, prompt):
                                logger.warning("[ROUTER] ‚ö†Ô∏è –≠—Ö–æ-–æ—Ç–≤–µ—Ç –æ—Ç %s (–º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –ø—Ä–æ–º–ø—Ç), –ø—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —É–∑–µ–ª", node['name'])
                                continue
                            
                            if result:
                                routing_source = node.get('routing_key', 'ollama_studio')
                                performance_score = node.get('performance_score', 0.8)
                                logger.info("[ROUTER] ‚úÖ [SUCCESS] Node: %s, Model: %s, Latency: %.2fms, Performance: %.2f", 
                                           node['name'], model, latency_ms, performance_score)
                                
                                # –û—Ç–º–µ—á–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–∞–º—è—Ç–∏
                                if self._memory_manager and node.get('routing_key') == 'local_server' and model:
                                    await self._memory_manager.mark_model_used(model)
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ—à–µ–Ω–∏–µ —Ä–æ—É—Ç–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è ML
                                if get_collector:
                                    collector = await get_collector()
                                    await collector.collect_routing_decision(
                                        task_type=task_type,
                                        prompt_length=len(prompt),
                                        category=category,
                                        selected_route=routing_source,
                                        performance_score=performance_score,
                                        latency_ms=latency_ms,
                                        success=True,
                                        features={
                                            "model": model,
                                            "node_name": node['name'],
                                            "node_priority": node.get('priority', 0),
                                            "attempt": attempt + 1
                                        }
                                    )
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
                                # –≠—Ç–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ worker'–µ –¥–ª—è –∑–∞–ø–∏—Å–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                                if hasattr(self, '_current_task_id'):
                                    try:
                                        from model_performance_tracker import get_performance_tracker
                                        tracker = get_performance_tracker()
                                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ metadata –∑–∞–¥–∞—á–∏ (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –≤ worker'–µ)
                                        self._used_model = model
                                    except:
                                        pass
                                
                                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –ø—Ä–∏ —É—Å–ø–µ—Ö–µ (–∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç)
                                if prompt_cache_key and len(result) < 5000:
                                    self._evict_prompt_cache_if_needed()
                                    self._prompt_cache[prompt_cache_key] = (result, routing_source)
                                    self._prompt_cache_meta[prompt_cache_key] = time.time()
                                # Return (response, routing_source) tuple
                                return result, routing_source
                            else:
                                logger.warning("[ROUTER] ‚ö†Ô∏è Node %s returned empty response for model %s", node['name'], model)
                        else:
                            # Log error response body for debugging
                            try:
                                error_text = response.text[:500]
                                logger.error("[ROUTER] ‚ùå Node %s returned HTTP %d: %s", node['name'], response.status_code, error_text)
                            except:
                                logger.error("[ROUTER] ‚ùå Node %s returned HTTP %d", node['name'], response.status_code)
                except asyncio.TimeoutError:
                    logger.warning("[ROUTER] ‚è±Ô∏è Timeout: Node %s, Model %s (attempt %d/%d)", 
                                  node['name'], model, attempt + 1, max_retries + 1)
                    if attempt < max_retries:
                        await asyncio.sleep(2 ** attempt)  # Exponential backoff
                    continue
                except httpx.ConnectError as e:
                    logger.error("[ROUTER] ‚ùå Connection failed to %s: %s", node_url, e)
                    if attempt < max_retries:
                        await asyncio.sleep(2 ** attempt)
                    continue
                except Exception as e:
                    logger.error("[ROUTER] ‚ùå Exception calling Node %s: %s: %s", node['name'], type(e).__name__, e)
                    if attempt < max_retries:
                        await asyncio.sleep(2 ** attempt)
                    continue
            
            # If we exhausted retries for this node, try next
            logger.warning(f"‚ö†Ô∏è Node {node['name']} failed after {max_retries + 1} attempts, trying next...")

        # –í—Å–µ —É–∑–ª—ã –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∏ - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ—É–¥–∞—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ
        total_latency = (time.time() - start_time) * 1000
        if get_collector:
            collector = await get_collector()
            await collector.collect_routing_decision(
                task_type=task_type,
                prompt_length=len(prompt),
                category=category,
                selected_route="cloud",  # Fallback –≤ –æ–±–ª–∞–∫–æ
                latency_ms=total_latency,
                success=False,
                features={"reason": "all_nodes_failed"}
            )
        
        return None, None
    
    async def run_local_llm_streaming(
        self,
        prompt: str,
        system_prompt: str = "",
        category: Optional[str] = None,
        images: Optional[list] = None,
        model: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        Call local LLM with streaming support.
        Returns AsyncGenerator that yields response chunks.
        
        Args:
            prompt: User prompt
            system_prompt: System prompt
            category: Task category
            images: List of images (for vision models)
            model: Specific model to use (if None, selected automatically)
        
        Yields:
            Response chunks as strings
        """
        # Select model
        if model is None:
            model = self._select_model(prompt, category)
        if images and MODEL_MAP.get("vision"):
            model = MODEL_MAP["vision"]
        
        # Discover healthy nodes
        healthy_nodes = await self.check_health()
        if not healthy_nodes:
            logger.error("‚ùå [STREAMING] No healthy local nodes found!")
            return
        
        # Select best node (use first one, load balancer would be ideal but keep it simple for streaming)
        node = healthy_nodes[0]
        node_url = f"{node['url']}/api/generate"
        logger.info(f"üåä [STREAMING] Node: {node['name']} | Model: {model}")
        
        full_prompt = f"{system_prompt}\n\nUser: {prompt}\nAssistant:"
        payload = {
            "model": model,
            "prompt": full_prompt,
            "stream": True
        }
        payload["keep_alive"] = _get_keep_alive(model)
        if images:
            payload["images"] = images
        
        try:
            async with httpx.AsyncClient(timeout=300.0) as client:
                async with client.stream(
                    'POST',
                    node_url,
                    json=payload
                ) as response:
                    if response.status_code != 200:
                        logger.error(f"‚ùå [STREAMING] Error: {response.status_code}")
                        return
                    
                    async for line in response.aiter_lines():
                        if not line:
                            continue
                        
                        try:
                            chunk_data = json.loads(line)
                            if 'response' in chunk_data:
                                yield chunk_data['response']
                            if chunk_data.get('done', False):
                                break
                        except json.JSONDecodeError:
                            continue
        except Exception as e:
            logger.error(f"‚ùå [STREAMING] Error: {e}")
            return
    
    def _determine_task_type(self, prompt: str, category: Optional[str] = None) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –∑–∞–¥–∞—á–∏ –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö"""
        prompt_lower = prompt.lower()
        
        if category == "coding" or "–∫–æ–¥" in prompt_lower or "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä—É–π" in prompt_lower:
            return "coding"
        elif category == "reasoning" or "–ø–æ–¥—É–º–∞–π" in prompt_lower or "–ª–æ–≥–∏–∫–∞" in prompt_lower:
            return "reasoning"
        else:
            return "general"
