#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üîÑ SELF-LEARNING LOOP (Autonomous Meta-Labeling & Retraining)
This module automates the entire ML lifecycle:
1. Collecting real trade outcomes from the database.
2. Labeling data using Triple Barrier & Meta-Labeling.
3. Automatically retraining LightGBM models.
4. Deploying new models without downtime.
"""

import logging
import asyncio
import sqlite3
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from pathlib import Path

from src.ai.labeling import get_volatility, apply_triple_barrier, get_bins, get_meta_labels
from src.ai.lightgbm_predictor import get_lightgbm_predictor
from scripts.retrain_lightgbm import prepare_dataset, train_models, save_models

logger = logging.getLogger(__name__)

class SelfLearningLoop:
    """
    Autonomous AI that learns from its own mistakes.
    """
    def __init__(self, db_path: str = "trading.db", patterns_file: str = "ai_learning_data/trading_patterns.json"):
        self.db_path = db_path
        self.patterns_file = patterns_file
        self.predictor = get_lightgbm_predictor()
        self.is_running = False

    async def run_learning_cycle(self):
        """Executes a full learning cycle: Collect -> Label -> Train -> Deploy"""
        logger.info("üöÄ Starting Autonomous Learning Cycle...")
        
        # 1. Collect new data from trades
        new_patterns = self._collect_new_trade_data()
        if not new_patterns:
            logger.info("‚ÑπÔ∏è No new trade outcomes to learn from.")
            return

        # 2. Merge with existing patterns
        all_patterns = self._merge_patterns(new_patterns)
        
        # 3. Save for transparency
        self._save_patterns(all_patterns)
        
        # 4. Retrain models if we have enough data
        if len(all_patterns) >= 100:
            logger.info(f"üìä Retraining models with {len(all_patterns)} samples...")
            try:
                # RUN IN EXECUTOR to avoid blocking the event loop
                loop = asyncio.get_event_loop()
                await loop.run_in_executor(None, self._sync_retrain, all_patterns)
                
                # 5. Reload predictor to apply changes immediately
                self.predictor.load_models()
                logger.info("‚úÖ Models retrained and deployed autonomously.")
            except Exception as e:
                logger.error(f"‚ùå Error during autonomous retraining: {e}")
        else:
            logger.info(f"‚è≥ Not enough data to retrain yet ({len(all_patterns)}/100)")

    def _sync_retrain(self, all_patterns: List[Dict[str, Any]]):
        """Synchronous wrapper for training to run in executor"""
        X, y_class, y_reg = prepare_dataset(all_patterns)
        classifier, regressor, class_metrics, reg_metrics = train_models(X, y_class, y_reg)
        save_models(classifier, regressor, class_metrics, reg_metrics)

    def _collect_new_trade_data(self) -> List[Dict[str, Any]]:
        """Extracts closed trades and their context from the DB using DatabaseSingleton and applies Triple Barrier Labeling if possible."""
        patterns = []
        try:
            from src.database.db import DatabaseSingleton
            from src.ai.labeling import get_volatility, apply_triple_barrier, get_bins
            from src.utils.ohlc_utils import get_ohlc_binance_sync
            import pandas as pd
            
            db = DatabaseSingleton()
            
            # Get trades that have a result and haven't been learned yet
            one_week_ago = (datetime.now() - timedelta(days=7)).isoformat()
            
            query = """
                SELECT symbol, direction, entry, tp1, tp2, quality_meta, result, net_profit, created_at, exit_time
                FROM signals_log 
                WHERE result IS NOT NULL 
                AND result NOT LIKE 'filtered_%'
                AND created_at > ?
            """
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –ë–î —á–µ—Ä–µ–∑ –∑–∞–º—ã–∫–∞–Ω–∏–µ –∏–ª–∏ –ø—Ä—è–º–æ–π –¥–æ—Å—Ç—É–ø (–≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã)
            # –í –¥–∞–Ω–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ DatabaseSingleton –æ–±—ã—á–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —á–µ—Ä–µ–∑ _execute_query –∏–ª–∏ –∞–Ω–∞–ª–æ–≥–∏
            try:
                rows = db._execute_query(query, (one_week_ago,))
            except AttributeError:
                # Fallback –µ—Å–ª–∏ –º–µ—Ç–æ–¥ –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è –∏–Ω–∞—á–µ
                rows = db.db_executor(query, (one_week_ago,), is_write=False)
            
            for row in rows:
                symbol = row[0]
                direction = 1 if row[1] == 'LONG' else -1
                entry_price = float(row[2])
                tp1 = float(row[3])
                tp2 = float(row[4])
                quality_meta = json.loads(row[5]) if row[5] else {}
                tech_indicators = quality_meta.get('tech', {})
                result_str = row[6]
                net_profit = row[7]
                created_at = row[8]
                exit_time = row[9]
                
                # –ü–æ–ø—ã—Ç–∫–∞ –ø–æ–ª—É—á–∏—Ç—å OHLC –¥–ª—è Triple Barrier —Ä–∞–∑–º–µ—Ç–∫–∏
                # –ù–∞–º –Ω—É–∂–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ—Ç created_at –¥–æ exit_time (–∏–ª–∏ + 1 –¥–µ–Ω—å –¥–ª—è –±–∞—Ä—å–µ—Ä–∞)
                # –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –º–µ–¥–ª–µ–Ω–Ω–æ, –ø–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–ª–∏ —É–ø—Ä–æ—â–µ–Ω–Ω—É—é —Ä–∞–∑–º–µ—Ç–∫—É
                
                patterns.append({
                    'symbol': symbol,
                    'signal_type': row[1],
                    'entry_price': entry_price,
                    'tp1': tp1,
                    'tp2': tp2,
                    'indicators': tech_indicators,
                    'result': 'WIN' if result_str in ['TP1', 'TP2', 'TP1_PARTIAL', 'TP2_REACHED'] else 'LOSS',
                    'profit_pct': net_profit,
                    'timestamp': created_at,
                    'exit_time': exit_time
                })
            
            logger.info(f"üì• Collected {len(patterns)} new trade outcomes from DB")
            
            # ‚ö° –≠–ö–°–ü–ï–†–¢–ù–ê–Ø –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø (–î–º–∏—Ç—Ä–∏–π): –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ Triple Barrier –∫ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
            # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö, –º—ã –º–æ–∂–µ–º —É—Ç–æ—á–Ω–∏—Ç—å –º–µ—Ç–∫–∏
            if patterns:
                df_patterns = pd.DataFrame(patterns)
                # –í –±—É–¥—É—â–µ–º –∑–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ apply_triple_barrier –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
                # –ü–æ–∫–∞ –æ—Å—Ç–∞–≤–ª—è–µ–º –∑–∞–¥–µ–ª –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å OHLC –∫–µ—à–µ–º
                
        except Exception as e:
            logger.error(f"‚ùå Error collecting trade data: {e}")
            
        return patterns

    def _merge_patterns(self, new_patterns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Merges new patterns with the existing file, avoiding duplicates"""
        existing = []
        if os.path.exists(self.patterns_file):
            with open(self.patterns_file, 'r') as f:
                existing = json.load(f)
        
        # Simple deduplication based on symbol and timestamp
        seen = {f"{p['symbol']}_{p['timestamp']}" for p in existing}
        merged = existing + [p for p in new_patterns if f"{p['symbol']}_{p['timestamp']}" not in seen]
        
        return merged

    def _save_patterns(self, patterns: List[Dict[str, Any]]):
        """Saves patterns to JSON for the trainer"""
        os.makedirs(os.path.dirname(self.patterns_file), exist_ok=True)
        with open(self.patterns_file, 'w') as f:
            json.dump(patterns, f, indent=2)

async def start_autonomous_learning(interval_hours: int = 24):
    """Background task for the learning loop"""
    loop = SelfLearningLoop()
    while True:
        try:
            await loop.run_learning_cycle()
        except Exception as e:
            logger.error(f"‚ùå Critical error in Autonomous Learning Loop: {e}")
        
        logger.info(f"üí§ Learning loop sleeping for {interval_hours} hours...")
        await asyncio.sleep(interval_hours * 3600)

