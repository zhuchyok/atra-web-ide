#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚ö° PERFORMANCE BENCHMARKS
–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
"""

import asyncio
import time
import statistics
import psutil
import logging
from datetime import datetime, timedelta
from src.shared.utils.datetime_utils import get_utc_now
from typing import Dict, List, Any, Optional
import concurrent.futures
import threading
import multiprocessing
from dataclasses import dataclass
import json

logger = logging.getLogger(__name__)

@dataclass
class BenchmarkResult:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –±–µ–Ω—á–º–∞—Ä–∫–∞"""
    name: str
    iterations: int
    total_time: float
    avg_time: float
    min_time: float
    max_time: float
    p95_time: float
    p99_time: float
    throughput: float  # operations per second
    memory_usage: float  # MB
    cpu_usage: float  # percentage
    success_rate: float  # percentage

class PerformanceBenchmark:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.results: List[BenchmarkResult] = []
        self.process = psutil.Process()
        
    def measure_execution_time(self, func, *args, **kwargs):
        """–ò–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏"""
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        return result, end_time - start_time
    
    async def measure_async_execution_time(self, func, *args, **kwargs):
        """–ò–∑–º–µ—Ä—è–µ—Ç –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
        start_time = time.time()
        result = await func(*args, **kwargs)
        end_time = time.time()
        return result, end_time - start_time
    
    def get_memory_usage(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ MB"""
        memory_info = self.process.memory_info()
        return memory_info.rss / 1024 / 1024  # Convert to MB
    
    def get_cpu_usage(self):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö"""
        return self.process.cpu_percent()
    
    def run_benchmark(self, name: str, func, iterations: int = 1000, 
                     *args, **kwargs) -> BenchmarkResult:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è —Ñ—É–Ω–∫—Ü–∏–∏"""
        logger.info("–ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–∞: %s (%d –∏—Ç–µ—Ä–∞—Ü–∏–π)", name, iterations)
        
        # –ò–∑–º–µ—Ä—è–µ–º –±–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
        initial_memory = self.get_memory_usage()
        initial_cpu = self.get_cpu_usage()
        
        execution_times = []
        successes = 0
        
        start_time = time.time()
        
        for i in range(iterations):
            try:
                _, exec_time = self.measure_execution_time(func, *args, **kwargs)
                execution_times.append(exec_time)
                successes += 1
            except Exception as e:
                logger.warning("–û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ %d: %s", i, e)
        
        end_time = time.time()
        
        # –ò–∑–º–µ—Ä—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
        final_memory = self.get_memory_usage()
        final_cpu = self.get_cpu_usage()
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_time = end_time - start_time
        avg_time = statistics.mean(execution_times) if execution_times else 0
        min_time = min(execution_times) if execution_times else 0
        max_time = max(execution_times) if execution_times else 0
        
        # –ü–µ—Ä—Ü–µ–Ω—Ç–∏–ª–∏
        sorted_times = sorted(execution_times)
        p95_time = sorted_times[int(len(sorted_times) * 0.95)] if sorted_times else 0
        p99_time = sorted_times[int(len(sorted_times) * 0.99)] if sorted_times else 0
        
        throughput = successes / total_time if total_time > 0 else 0
        memory_usage = final_memory - initial_memory
        cpu_usage = (final_cpu - initial_cpu) / iterations if iterations > 0 else 0
        success_rate = (successes / iterations) * 100 if iterations > 0 else 0
        
        result = BenchmarkResult(
            name=name,
            iterations=iterations,
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            p95_time=p95_time,
            p99_time=p99_time,
            throughput=throughput,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            success_rate=success_rate
        )
        
        self.results.append(result)
        logger.info("–ë–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: %s (%.2f ops/sec)", name, throughput)
        
        return result
    
    async def run_async_benchmark(self, name: str, func, iterations: int = 1000, 
                                 *args, **kwargs) -> BenchmarkResult:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏"""
        logger.info("–ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞: %s (%d –∏—Ç–µ—Ä–∞—Ü–∏–π)", name, iterations)
        
        initial_memory = self.get_memory_usage()
        initial_cpu = self.get_cpu_usage()
        
        execution_times = []
        successes = 0
        
        start_time = time.time()
        
        for i in range(iterations):
            try:
                _, exec_time = await self.measure_async_execution_time(func, *args, **kwargs)
                execution_times.append(exec_time)
                successes += 1
            except Exception as e:
                logger.warning("–û—à–∏–±–∫–∞ –≤ –∏—Ç–µ—Ä–∞—Ü–∏–∏ %d: %s", i, e)
        
        end_time = time.time()
        
        final_memory = self.get_memory_usage()
        final_cpu = self.get_cpu_usage()
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_time = end_time - start_time
        avg_time = statistics.mean(execution_times) if execution_times else 0
        min_time = min(execution_times) if execution_times else 0
        max_time = max(execution_times) if execution_times else 0
        
        sorted_times = sorted(execution_times)
        p95_time = sorted_times[int(len(sorted_times) * 0.95)] if sorted_times else 0
        p99_time = sorted_times[int(len(sorted_times) * 0.99)] if sorted_times else 0
        
        throughput = successes / total_time if total_time > 0 else 0
        memory_usage = final_memory - initial_memory
        cpu_usage = (final_cpu - initial_cpu) / iterations if iterations > 0 else 0
        success_rate = (successes / iterations) * 100 if iterations > 0 else 0
        
        result = BenchmarkResult(
            name=name,
            iterations=iterations,
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            p95_time=p95_time,
            p99_time=p99_time,
            throughput=throughput,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            success_rate=success_rate
        )
        
        self.results.append(result)
        logger.info("–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: %s (%.2f ops/sec)", name, throughput)
        
        return result
    
    def run_concurrent_benchmark(self, name: str, func, iterations: int = 1000, 
                                max_workers: int = 10, *args, **kwargs) -> BenchmarkResult:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –±–µ–Ω—á–º–∞—Ä–∫ —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ–º"""
        logger.info("–ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –±–µ–Ω—á–º–∞—Ä–∫–∞: %s (%d –∏—Ç–µ—Ä–∞—Ü–∏–π, %d workers)", 
                   name, iterations, max_workers)
        
        initial_memory = self.get_memory_usage()
        initial_cpu = self.get_cpu_usage()
        
        execution_times = []
        successes = 0
        
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = []
            
            for i in range(iterations):
                future = executor.submit(self.measure_execution_time, func, *args, **kwargs)
                futures.append(future)
            
            for future in concurrent.futures.as_completed(futures):
                try:
                    _, exec_time = future.result()
                    execution_times.append(exec_time)
                    successes += 1
                except Exception as e:
                    logger.warning("–û—à–∏–±–∫–∞ –≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏: %s", e)
        
        end_time = time.time()
        
        final_memory = self.get_memory_usage()
        final_cpu = self.get_cpu_usage()
        
        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        total_time = end_time - start_time
        avg_time = statistics.mean(execution_times) if execution_times else 0
        min_time = min(execution_times) if execution_times else 0
        max_time = max(execution_times) if execution_times else 0
        
        sorted_times = sorted(execution_times)
        p95_time = sorted_times[int(len(sorted_times) * 0.95)] if sorted_times else 0
        p99_time = sorted_times[int(len(sorted_times) * 0.99)] if sorted_times else 0
        
        throughput = successes / total_time if total_time > 0 else 0
        memory_usage = final_memory - initial_memory
        cpu_usage = (final_cpu - initial_cpu) / iterations if iterations > 0 else 0
        success_rate = (successes / iterations) * 100 if iterations > 0 else 0
        
        result = BenchmarkResult(
            name=name,
            iterations=iterations,
            total_time=total_time,
            avg_time=avg_time,
            min_time=min_time,
            max_time=max_time,
            p95_time=p95_time,
            p99_time=p99_time,
            throughput=throughput,
            memory_usage=memory_usage,
            cpu_usage=cpu_usage,
            success_rate=success_rate
        )
        
        self.results.append(result)
        logger.info("–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –±–µ–Ω—á–º–∞—Ä–∫ –∑–∞–≤–µ—Ä—à–µ–Ω: %s (%.2f ops/sec)", name, throughput)
        
        return result
    
    def generate_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –ø–æ –≤—Å–µ–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º"""
        report = f"""
# üìä –û–¢–ß–ï–¢ –ü–û –ë–ï–ù–ß–ú–ê–†–ö–ê–ú –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò
–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {get_utc_now().strftime('%Y-%m-%d %H:%M:%S')}

## üìà –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

| –ë–µ–Ω—á–º–∞—Ä–∫ | –ò—Ç–µ—Ä–∞—Ü–∏–∏ | Avg Time | P95 Time | P99 Time | Throughput | Success Rate |
|----------|----------|----------|----------|----------|------------|--------------|
"""
        
        for result in self.results:
            report += f"| {result.name} | {result.iterations} | {result.avg_time:.3f}s | {result.p95_time:.3f}s | {result.p99_time:.3f}s | {result.throughput:.2f} ops/sec | {result.success_rate:.1f}% |\n"
        
        report += "\n## üîç –î–µ—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n\n"
        
        for result in self.results:
            report += f"""
### {result.name}

- **–ò—Ç–µ—Ä–∞—Ü–∏–∏:** {result.iterations}
- **–û–±—â–µ–µ –≤—Ä–µ–º—è:** {result.total_time:.3f}s
- **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è:** {result.avg_time:.3f}s
- **–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è:** {result.min_time:.3f}s
- **–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è:** {result.max_time:.3f}s
- **P95 –≤—Ä–µ–º—è:** {result.p95_time:.3f}s
- **P99 –≤—Ä–µ–º—è:** {result.p99_time:.3f}s
- **–ü—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å:** {result.throughput:.2f} ops/sec
- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏:** {result.memory_usage:.2f} MB
- **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU:** {result.cpu_usage:.2f}%
- **–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞:** {result.success_rate:.1f}%

"""
        
        return report
    
    def save_report(self, filename: Optional[str] = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –≤ —Ñ–∞–π–ª"""
        if filename is None:
            timestamp = get_utc_now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_benchmark_report_{timestamp}.md"
        
        report = self.generate_report()
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info("–û—Ç—á–µ—Ç –ø–æ –±–µ–Ω—á–º–∞—Ä–∫–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: %s", filename)
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: %s", e)
    
    def save_json_report(self, filename: Optional[str] = None):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ—Ç—á–µ—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ"""
        if filename is None:
            timestamp = get_utc_now().strftime("%Y%m%d_%H%M%S")
            filename = f"performance_benchmark_report_{timestamp}.json"
        
        json_data = {
            'timestamp': get_utc_now().isoformat(),
            'results': [
                {
                    'name': result.name,
                    'iterations': result.iterations,
                    'total_time': result.total_time,
                    'avg_time': result.avg_time,
                    'min_time': result.min_time,
                    'max_time': result.max_time,
                    'p95_time': result.p95_time,
                    'p99_time': result.p99_time,
                    'throughput': result.throughput,
                    'memory_usage': result.memory_usage,
                    'cpu_usage': result.cpu_usage,
                    'success_rate': result.success_rate
                }
                for result in self.results
            ]
        }
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, indent=2, ensure_ascii=False)
            logger.info("JSON –æ—Ç—á–µ—Ç –ø–æ –±–µ–Ω—á–º–∞—Ä–∫–∞–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω: %s", filename)
        except Exception as e:
            logger.error("–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è JSON –æ—Ç—á–µ—Ç–∞: %s", e)

# –¢–µ—Å—Ç–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
def test_simple_calculation():
    """–ü—Ä–æ—Å—Ç–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    return sum(range(1000))

def test_data_processing():
    """–§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    data = [i * 2 for i in range(1000)]
    return sum(data)

def test_string_processing():
    """–§—É–Ω–∫—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫"""
    text = "test" * 1000
    return text.upper()

async def test_async_operation():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    await asyncio.sleep(0.001)  # –ò–º–∏—Ç–∞—Ü–∏—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ä–∞–±–æ—Ç—ã
    return "async_result"

async def test_async_data_processing():
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"""
    data = [i * 3 for i in range(1000)]
    await asyncio.sleep(0.001)
    return sum(data)

def run_comprehensive_benchmarks():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –≤—Å–µ –±–µ–Ω—á–º–∞—Ä–∫–∏"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–æ–≤ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    
    benchmark = PerformanceBenchmark()
    
    # –°–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
    benchmark.run_benchmark("Simple Calculation", test_simple_calculation, 10000)
    benchmark.run_benchmark("Data Processing", test_data_processing, 5000)
    benchmark.run_benchmark("String Processing", test_string_processing, 3000)
    
    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
    benchmark.run_concurrent_benchmark("Concurrent Calculation", test_simple_calculation, 5000, 5)
    benchmark.run_concurrent_benchmark("Concurrent Data Processing", test_data_processing, 3000, 10)
    
    # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
    async def run_async_benchmarks():
        await benchmark.run_async_benchmark("Async Operation", test_async_operation, 2000)
        await benchmark.run_async_benchmark("Async Data Processing", test_async_data_processing, 1500)
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏
    asyncio.run(run_async_benchmarks())
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç—ã
    benchmark.save_report()
    benchmark.save_json_report()
    
    # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É
    print(benchmark.generate_report())
    
    logger.info("‚úÖ –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–µ –±–µ–Ω—á–º–∞—Ä–∫–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã")
    
    return benchmark.results

if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # –ó–∞–ø—É—Å–∫ –±–µ–Ω—á–º–∞—Ä–∫–æ–≤
    results = run_comprehensive_benchmarks()
    
    print(f"\nüéØ –í—ã–ø–æ–ª–Ω–µ–Ω–æ {len(results)} –±–µ–Ω—á–º–∞—Ä–∫–æ–≤")
    print("üìä –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª—ã")
