import aiohttp
import asyncio
import json
import logging
import os
import time
import traceback
from typing import List, Dict, Any, Optional, Tuple
from pydantic import ValidationError
from .base_agent import AgentAction, AgentFinish

logger = logging.getLogger(__name__)

# Debug mode: VICTORIA_DEBUG=true enables verbose logging
VICTORIA_DEBUG = os.getenv("VICTORIA_DEBUG", "false").lower() in ("true", "1", "yes")

# –ú–∏—Ä–æ–≤–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞: —Ç–æ–ª—å–∫–æ —ç—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Å—É—â–µ—Å—Ç–≤—É—é—Ç. –õ—é–±–æ–π –¥—Ä—É–≥–æ–π = –æ—Ç–∫–ª–æ–Ω—è–µ–º –∏ –ø—Ä–æ—Å–∏–º –ø–æ–≤—Ç–æ—Ä–∏—Ç—å.
ALLOWED_TOOLS = {"finish", "read_file", "list_directory", "run_terminal_cmd", "ssh_run"}

# === MODEL FALLBACK CONFIGURATION ===
# Ordered list of fallback models from smallest to largest
FALLBACK_MODELS_OLLAMA = [
    "phi3.5:3.8b",      # Fast, stable
    "tinyllama:1.1b-chat",  # Very small, always works
    "glm-4.7-flash:q8_0",   # Medium, good quality
    "qwen2.5-coder:32b",    # Large, may crash on limited RAM
]

FALLBACK_MODELS_MLX = [
    "phi3.5:3.8b",
    "qwen2.5:3b",
    "tinyllama:1.1b-chat",
    "phi3:mini-4k",
    "qwen2.5-coder:32b",
]

# Models that are known to crash on resource-limited systems
RESOURCE_HEAVY_MODELS = {
    "qwen2.5-coder:32b", "qwq:32b", "deepseek-r1-distill-llama:70b", 
    "llama3.3:70b", "command-r-plus:104b"
}

def _ollama_base_url() -> str:
    return os.getenv("OLLAMA_BASE_URL") or os.getenv("MAC_STUDIO_LLM_URL") or "http://localhost:11434"

def _mlx_base_url() -> str:
    """Get MLX API Server URL (default: 11435)"""
    is_docker = os.path.exists('/.dockerenv') or os.getenv('DOCKER_CONTAINER', 'false').lower() == 'true'
    if is_docker:
        return os.getenv("MLX_BASE_URL", "http://host.docker.internal:11435")
    return os.getenv("MLX_BASE_URL", "http://localhost:11435")


class OllamaExecutor:
    """–ò—Å–ø–æ–ª–Ω–∏—Ç–µ–ª—å –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ Ollama / MLX API —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º fallback"""
    
    def __init__(self, model: str = None, base_url: Optional[str] = None):
        # –ê–≤—Ç–æ–≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏: –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –±—É–¥–µ—Ç –≤—ã–±—Ä–∞–Ω–∞ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ —á–µ—Ä–µ–∑ —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ Ollama
        self.model = model or os.getenv("VICTORIA_MODEL") or os.getenv("VERONICA_MODEL") or "auto"
        self.base_url = base_url or _ollama_base_url()
        self._model_resolved = False  # –§–ª–∞–≥: –º–æ–¥–µ–ª—å —É–∂–µ –≤—ã–±—Ä–∞–Ω–∞ –∏–∑ –∞–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ —Å–ø–∏—Å–∫–∞
        
        # === FALLBACK CONFIGURATION ===
        self._failed_models: set = set()  # Models that have failed in this session
        self._fallback_attempts = 0
        self._max_fallback_attempts = 3
        self._last_successful_model: Optional[str] = None
        
        # MLX URL for fallback
        self._mlx_url = _mlx_base_url()
        self._use_mlx_fallback = os.getenv("USE_MLX_FALLBACK", "true").lower() == "true"
        
        logger.info(f"[EXECUTOR_INIT] ========== OllamaExecutor initialization ==========")
        logger.info(f"[EXECUTOR_INIT] Primary model: {self.model}")
        logger.info(f"[EXECUTOR_INIT] Ollama URL: {self.base_url}")
        logger.info(f"[EXECUTOR_INIT] MLX URL: {self._mlx_url}")
        logger.info(f"[EXECUTOR_INIT] MLX fallback enabled: {self._use_mlx_fallback}")
        
        self.system_prompt = """–¢–´ ‚Äî –í–ò–ö–¢–û–†–ò–Ø, TEAM LEAD ATRA. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º.

–°–¢–†–û–ì–û: –û—Ç–≤–µ—Ç ‚Äî –û–î–ò–ù JSON, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ. –ü–æ–ª–µ "tool" ‚Äî –¢–û–õ–¨–ö–û –æ–¥–Ω–æ –∏–∑: finish, read_file, list_directory, run_terminal_cmd, ssh_run. –î—Ä—É–≥–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ù–ï–¢ (–Ω–µ—Ç web_search, git_run, web_check, websocket –∏ —Ç.–¥.).

–§–û–†–ú–ê–¢: {"thought": "...", "tool": "...", "tool_input": {...}}

–ò–ù–°–¢–†–£–ú–ï–ù–¢–´ (—Ç–æ–ª—å–∫–æ —ç—Ç–∏):
1. finish - –ó–ê–í–ï–†–®–ò–¢–¨ –∑–∞–¥–∞—á—É. –ò—Å–ø–æ–ª—å–∑—É–π –°–†–ê–ó–£ –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤!
   {"tool": "finish", "tool_input": {"output": "–æ—Ç–≤–µ—Ç"}}
2. read_file - –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª. –ü—É—Ç—å –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω—ã–π: frontend/src/App.svelte, package.json (–ù–ï /path/to/!)
   {"tool": "read_file", "tool_input": {"file_path": "frontend/src/App.svelte"}}
3. list_directory - —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤. –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: "." –∏–ª–∏ "frontend" (–ù–ï /path/to/repository!)
   {"tool": "list_directory", "tool_input": {"directory": "."}}
4. run_terminal_cmd - –õ–û–ö–ê–õ–¨–ù–ê–Ø –∫–æ–º–∞–Ω–¥–∞ (ls, cat, find, docker ‚Äî –ù–ï ssh!)
   {"tool": "run_terminal_cmd", "tool_input": {"command": "ls -la"}}
5. ssh_run - –£–î–ê–õ–Å–ù–ù–´–ô —Å–µ—Ä–≤–µ—Ä (—Ç–æ–ª—å–∫–æ —Å —Ä–µ–∞–ª—å–Ω—ã–º host!)
   {"tool": "ssh_run", "tool_input": {"host": "IP", "command": "–∫–æ–º–∞–Ω–¥–∞"}}

–ó–ê–ü–†–ï–©–ï–ù–û: web_search, web_edit, git_run, write_file, web_review ‚Äî —Ç–∞–∫–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –ù–ï–¢! –ù–µ –≤—ã–¥—É–º—ã–≤–∞–π –ø—É—Ç–∏ /path/to/ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–π —Ä–µ–∞–ª—å–Ω—ã–µ: ., frontend, backend. –û—Ç–≤–µ—Ç ‚Äî –û–î–ò–ù JSON, –±–µ–∑ —Ç–µ–∫—Å—Ç–∞ –¥–æ/–ø–æ—Å–ª–µ.

–ü–†–ê–í–ò–õ–ê –í–´–ü–û–õ–ù–ï–ù–ò–Ø:
- –ü—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã ("–ø—Ä–∏–≤–µ—Ç", "—Å–∫–∞–∂–∏ –ø—Ä–∏–≤–µ—Ç") ‚Üí –°–†–ê–ó–£ finish
- "–ø–æ–∫–∞–∂–∏ —Ñ–∞–π–ª—ã" / "–≤—ã–≤–µ–¥–∏ —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤" ‚Üí run_terminal_cmd "ls -la", –∑–∞—Ç–µ–º finish
- –õ–û–ö–ê–õ–¨–ù–´–ï –∫–æ–º–∞–Ω–¥—ã (ls, cat, find, docker exec) ‚Üí run_terminal_cmd (–ù–ï ssh_run!)
- –£–î–ê–õ–Å–ù–ù–´–ï —Å–µ—Ä–≤–µ—Ä—ã (–ø–æ IP –∞–¥—Ä–µ—Å—É) ‚Üí ssh_run —Å host
- –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —É—Å–ª–æ–≤–∏—è! –í—ã–ø–æ–ª–Ω—è–π –¢–û–ß–ù–û —Ç–æ —á—Ç–æ –ø—Ä–æ—Å—è—Ç
- –ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–æ–º–∞–Ω–¥—ã ‚Üí –°–†–ê–ó–£ finish —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º

–í–ê–ñ–ù–û: docker exec, ls, cat, find - —ç—Ç–æ –õ–û–ö–ê–õ–¨–ù–´–ï –∫–æ–º–∞–Ω–¥—ã! –ò—Å–ø–æ–ª—å–∑—É–π run_terminal_cmd!

–ü–†–ò–ú–ï–†–´ –ü–†–ê–í–ò–õ–¨–ù–´–• –û–¢–í–ï–¢–û–í:
Q: "—Å–∫–∞–∂–∏ –ø—Ä–∏–≤–µ—Ç"
A: {"thought": "–ü—Ä–æ—Å—Ç–æ–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ", "tool": "finish", "tool_input": {"output": "–ü—Ä–∏–≤–µ—Ç! –Ø –í–∏–∫—Ç–æ—Ä–∏—è."}}

Q: "–≤—ã–≤–µ–¥–∏ —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤"
A: {"thought": "–ù—É–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å ls", "tool": "run_terminal_cmd", "tool_input": {"command": "ls -la"}}
(–ü–æ—Å–ª–µ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ ‚Üí finish —Å –≤—ã–≤–æ–¥–æ–º –∫–æ–º–∞–Ω–¥—ã)

Q: "–ø–æ–∫–∞–∂–∏ —Ñ–∞–π–ª—ã –≤ —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"
A: {"thought": "–í—ã–ø–æ–ª–Ω—é ls –¥–ª—è —Ç–µ–∫—É—â–µ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏", "tool": "run_terminal_cmd", "tool_input": {"command": "ls -la"}}
"""

    async def _check_model_available(self, base_url: str, model: str) -> bool:
        """Check if a model is available on the given server"""
        try:
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=5)) as session:
                async with session.get(f"{base_url}/api/tags") as response:
                    if response.status == 200:
                        data = await response.json()
                        models = [m.get('name', '') for m in data.get('models', [])]
                        return model in models or any(model in m for m in models)
        except Exception as e:
            logger.debug(f"[MODEL_CHECK] Failed to check {model} on {base_url}: {e}")
        return False

    async def _get_fallback_model(self) -> Tuple[Optional[str], Optional[str]]:
        """
        Get next available fallback model.
        Returns: (model_name, base_url) or (None, None) if no fallback available
        """
        logger.info(f"[FALLBACK] ========== Finding fallback model ==========")
        logger.info(f"[FALLBACK] Failed models this session: {self._failed_models}")
        logger.info(f"[FALLBACK] Fallback attempts: {self._fallback_attempts}/{self._max_fallback_attempts}")
        
        if self._fallback_attempts >= self._max_fallback_attempts:
            logger.error(f"[FALLBACK] Max fallback attempts reached ({self._max_fallback_attempts})")
            return None, None
        
        # Try MLX first (more stable for large models)
        if self._use_mlx_fallback:
            for model in FALLBACK_MODELS_MLX:
                if model not in self._failed_models:
                    if await self._check_model_available(self._mlx_url, model):
                        logger.info(f"[FALLBACK] ‚úÖ Found MLX model: {model}")
                        return model, self._mlx_url
        
        # Try Ollama (smaller models are more stable)
        for model in FALLBACK_MODELS_OLLAMA:
            if model not in self._failed_models:
                if await self._check_model_available(self.base_url, model):
                    logger.info(f"[FALLBACK] ‚úÖ Found Ollama model: {model}")
                    return model, self.base_url
        
        logger.error(f"[FALLBACK] ‚ùå No available fallback models found")
        return None, None

    async def ask(
        self,
        prompt: str,
        history: List[Dict[str, str]] = None,
        raw_response: bool = False,
        phase: Optional[str] = None,
        blocked_tools: Optional[List[str]] = None,
        model: Optional[str] = None,
        system: Optional[str] = None,
    ) -> Any:
        """
        Send request to LLM with automatic fallback on model crash.
        phase: –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ ‚Äî –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ü–µ–ª–∏ / –ø–ª–∞–Ω / —à–∞–≥ N, –ª–æ–≥–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ —Ç–∞–π–º–∞—É—Ç–µ.
        blocked_tools: –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ–ª—å–∑—è –≤—ã–±–∏—Ä–∞—Ç—å (–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ —Ü–∏–∫–ª–∞).
        model: –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞.
        system: –ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç.
        """
        return await self._ask_with_fallback(
            prompt=prompt,
            history=history,
            raw_response=raw_response,
            model=model or self.model,
            base_url=self.base_url,
            is_retry=False,
            phase=phase,
            blocked_tools=blocked_tools,
            system_override=system,
        )

    async def _ask_with_fallback(
        self,
        prompt: str,
        history: List[Dict[str, str]],
        raw_response: bool,
        model: str,
        base_url: str,
        is_retry: bool = False,
        phase: Optional[str] = None,
        blocked_tools: Optional[List[str]] = None,
        system_override: Optional[str] = None,
    ) -> Any:
        """Internal method with fallback support"""
        url = f"{base_url}/api/chat"
        system_content = system_override or self.system_prompt
        if blocked_tools:
            allowed = sorted(ALLOWED_TOOLS - set(blocked_tools))
            system_content += (
                f"\n\n‚ö†Ô∏è –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å (–∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω—ã –∏–∑-–∑–∞ —Ü–∏–∫–ª–∞): {', '.join(sorted(blocked_tools))}. "
                f"–î–æ—Å—Ç—É–ø–Ω—ã –¢–û–õ–¨–ö–û: {', '.join(allowed)}. –û—Ç–≤–µ—Ç—å JSON —Å tool –∏–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–ª–∏ finish."
            )
        messages = [{"role": "system", "content": system_content}]
        if history:
            messages.extend(history)
        messages.append({"role": "user", "content": prompt})
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": False,
            "options": { "temperature": 0.1 }
        }
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π keep_alive –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ (Singularity 10.0)
        def get_smart_keep_alive(m_name: str) -> Any:
            raw = os.getenv("VICTORIA_OLLAMA_KEEP_ALIVE") or os.getenv("OLLAMA_KEEP_ALIVE")
            if raw:
                try:
                    return int(raw) if str(raw).strip().lstrip("-").isdigit() else raw
                except: return raw
            
            key = (m_name or "").lower()
            if "70b" in key or "104b" in key or "next" in key: return 60
            if "32b" in key or "30b" in key or "qwq" in key: return 300
            if "7b" in key or "8b" in key or "14b" in key: return 600
            if "3b" in key or "1b" in key or "tiny" in key or "embedding" in key: return 3600
            return 300

        payload["keep_alive"] = get_smart_keep_alive(model)
        
        # === DETAILED DEBUG LOGGING ===
        logger.info(f"[LLM_CALL] ========== OllamaExecutor.ask() ==========")
        logger.info(f"[LLM_CALL] Model: {model}")
        logger.info(f"[LLM_CALL] URL: {url}")
        logger.info(f"[LLM_CALL] Is retry/fallback: {is_retry}")
        logger.info(f"[LLM_CALL] Failed models this session: {self._failed_models}")
        logger.info(f"[LLM_CALL] Prompt length: {len(prompt)} chars")
        logger.info(f"[LLM_CALL] Prompt preview: {prompt[:200]}...")
        if VICTORIA_DEBUG:
            logger.debug(f"[LLM_CALL] Full payload: {json.dumps(payload, ensure_ascii=False)[:1000]}")
        
        start_time = time.time()
        
        # –¢–∞–π–º–∞—É—Ç –Ω–∞ –æ–¥–∏–Ω –≤—ã–∑–æ–≤ LLM: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ OLLAMA_EXECUTOR_TIMEOUT (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 300 —Å)
        # connect=30 ‚Äî —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –∏–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –∫ host.docker.internal (–Ω–µ –æ–±—Ä—ã–≤–∞—Ç—å –¥–æ–ª–≥–∏–µ –æ—Ç–≤–µ—Ç—ã)
        _exec_timeout = float(os.getenv("OLLAMA_EXECUTOR_TIMEOUT", "300"))
        timeout = aiohttp.ClientTimeout(total=_exec_timeout, connect=30.0)
        async with aiohttp.ClientSession(timeout=timeout) as session:
            try:
                logger.info(f"[LLM_CALL] Sending request to {url}...")
                async with session.post(url, json=payload) as response:
                    elapsed = time.time() - start_time
                    logger.info(f"[LLM_RESPONSE] HTTP Status: {response.status}, Time: {elapsed:.2f}s")
                    
                    if response.status == 200:
                        result = await response.json()
                        content = result.get('message', {}).get('content', '')
                        
                        # === SUCCESS LOGGING ===
                        logger.info(f"[LLM_RESPONSE] ‚úÖ Success!")
                        logger.info(f"[LLM_RESPONSE] Model used: {model}")
                        logger.info(f"[LLM_RESPONSE] Content length: {len(content)} chars")
                        logger.info(f"[LLM_RESPONSE] Content preview: {content[:300]}...")
                        if VICTORIA_DEBUG:
                            logger.debug(f"[LLM_RESPONSE] Full content: {content[:2000]}")
                        
                        # Mark this model as successful
                        self._last_successful_model = model
                        
                        if raw_response:
                            return content
                        return self._parse_response(content, blocked_tools=blocked_tools)
                    else:
                        # Model crashed or error
                        error_body = await response.text()
                        logger.error(f"[LLM_ERROR] HTTP {response.status}: {error_body[:500]}")
                        
                        # Check for model crash indicators
                        crash_indicators = [
                            "model runner has unexpectedly stopped",
                            "resource limitations",
                            "internal error",
                            "out of memory",
                            "CUDA error",
                            "Metal error"
                        ]
                        
                        is_crash = any(ind.lower() in error_body.lower() for ind in crash_indicators)
                        
                        if is_crash or response.status == 500:
                            logger.warning(f"[LLM_CRASH] ‚ö†Ô∏è Model {model} crashed! Attempting fallback...")
                            self._failed_models.add(model)
                            self._fallback_attempts += 1
                            
                            # Try fallback
                            fallback_model, fallback_url = await self._get_fallback_model()
                            if fallback_model and fallback_url:
                                logger.info(f"[LLM_FALLBACK] üîÑ Retrying with model: {fallback_model} on {fallback_url}")
                                return await self._ask_with_fallback(
                                    prompt=prompt,
                                    history=history,
                                    raw_response=raw_response,
                                    model=fallback_model,
                                    base_url=fallback_url,
                                    is_retry=True,
                                    phase=phase,
                                    blocked_tools=blocked_tools,
                                    system_override=system_override,
                                )
                        
                        return {"error": f"Ollama HTTP {response.status}: {error_body[:200]}"}
                        
            except asyncio.TimeoutError:
                elapsed = time.time() - start_time
                phase_info = f" phase={phase}" if phase else ""
                logger.error(
                    "[LLM_ERROR] ‚è±Ô∏è Timeout after %.2fs for model %s%s",
                    elapsed, model, phase_info,
                )
                
                # Timeout on large model - try fallback
                if model in RESOURCE_HEAVY_MODELS:
                    logger.warning(f"[LLM_TIMEOUT] Large model {model} timed out, trying fallback...")
                    self._failed_models.add(model)
                    self._fallback_attempts += 1
                    
                    fallback_model, fallback_url = await self._get_fallback_model()
                    if fallback_model and fallback_url:
                        return await self._ask_with_fallback(
                            prompt=prompt,
                            history=history,
                            raw_response=raw_response,
                            model=fallback_model,
                            base_url=fallback_url,
                            is_retry=True,
                            phase=phase,
                            blocked_tools=blocked_tools,
                        )
                
                return {"error": f"Timeout: –º–æ–¥–µ–ª—å {model} –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∞ –∑–∞ {int(_exec_timeout)} —Å"}
                
            except aiohttp.ClientConnectorError as e:
                logger.error(f"[LLM_ERROR] üîå Connection failed to {url}: {e}")
                
                # If Ollama is down, try MLX
                if self._use_mlx_fallback and base_url != self._mlx_url:
                    logger.info(f"[LLM_FALLBACK] Ollama connection failed, trying MLX...")
                    fallback_model, fallback_url = await self._get_fallback_model()
                    if fallback_model and fallback_url:
                        return await self._ask_with_fallback(
                            prompt=prompt,
                            history=history,
                            raw_response=raw_response,
                            model=fallback_model,
                            base_url=fallback_url,
                            is_retry=True,
                            phase=phase,
                            blocked_tools=blocked_tools,
                        )
                
                return {"error": f"Connection failed to {url}: {e}"}
                
            except Exception as e:
                logger.error(f"[LLM_ERROR] ‚ùå Exception: {type(e).__name__}: {e}")
                logger.error(f"[LLM_ERROR] Traceback: {traceback.format_exc()}")
                return {"error": str(e)}

    def _parse_response(self, content: str, blocked_tools: Optional[List[str]] = None) -> Any:
        logger.info(f"[LLM_PARSE] Parsing response ({len(content)} chars)...")
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç–µ–≥–∏ <think>
        clean_content = content.strip()
        if "</think>" in clean_content:
            clean_content = clean_content.split("</think>")[-1].strip()
            logger.info(f"[LLM_PARSE] Removed <think> tags, now {len(clean_content)} chars")
        
        # –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π (–µ—Å–ª–∏ –ê–≥–µ–Ω—Ç –Ω–∞–ø–∏—Å–∞–ª —ç—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ)
        # –§–æ—Ä–º–∞—Ç: KNOWLEDGE: {"key": "value"}
        if "KNOWLEDGE:" in clean_content:
            try:
                k_part = clean_content.split("KNOWLEDGE:")[1].strip().split("\n")[0]
                knowledge_update = json.loads(k_part)
                # –≠—Ç–æ –±—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –≤ –±–∞–∑–æ–≤–æ–º –∫–ª–∞—Å—Å–µ (–Ω—É–∂–Ω–∞ —Å–≤—è–∑—å)
                logger.debug(f"üß† –ù–∞–π–¥–µ–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π: {knowledge_update}")
            except:
                pass

        # –ü—Ä–æ–±—É–µ–º —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
        try:
            start_idx = clean_content.find('{')
            end_idx = clean_content.rfind('}')
            
            if start_idx != -1 and end_idx != -1:
                json_str = clean_content[start_idx:end_idx+1]
                logger.info(f"[LLM_PARSE] Found JSON block at [{start_idx}:{end_idx+1}]")
                
                # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π JSON
                try:
                    data = json.loads(json_str)
                    logger.info(f"[LLM_PARSE] JSON parsed successfully, keys: {list(data.keys())}")
                except json.JSONDecodeError as je:
                    logger.warning(f"[LLM_PARSE] JSON decode failed: {je}, trying ast.literal_eval")
                    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–ª–∞ –æ–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ (Python style), –ø—Ä–æ–±—É–µ–º –∏—Å–ø—Ä–∞–≤–∏—Ç—å
                    import ast
                    try:
                        data = ast.literal_eval(json_str)
                        logger.info(f"[LLM_PARSE] ast.literal_eval succeeded")
                    except Exception as ae:
                        # –ï—Å–ª–∏ —Å–æ–≤—Å–µ–º –≤—Å—ë –ø–ª–æ—Ö–æ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞ –ê–≥–µ–Ω—Ç–æ–º
                        logger.error(f"[LLM_PARSE] Failed to parse JSON: {ae}")
                        logger.error(f"[LLM_PARSE] Raw JSON string: {json_str[:500]}")
                        return AgentFinish(output=clean_content, thought="Failed to parse JSON")
                
                thought = data.get("thought", "–†–∞—Å—Å—É–∂–¥–∞—é...")
                tool_input = data.get("tool_input") if isinstance(data.get("tool_input"), dict) else {}

                # –ß—É–∂–æ–π —Ñ–æ—Ä–º–∞—Ç (tool_execution, final_output) ‚Äî –Ω–µ –Ω–∞—à API, –∑–∞–≤–µ—Ä—à–∞–µ–º —Å –ø–æ–¥—Å–∫–∞–∑–∫–æ–π
                if "tool_execution" in data or "final_output" in data:
                    logger.warning(f"[LLM_PARSE] Invalid format detected: tool_execution/final_output")
                    return AgentFinish(
                        output="–ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ñ–æ—Ä–º–∞—Ç: {\"thought\": \"...\", \"tool\": \"–æ–¥–∏–Ω –∏–∑: finish, read_file, list_directory, run_terminal_cmd, ssh_run\", \"tool_input\": {...}}. –î—Ä—É–≥–∏—Ö –ø–æ–ª–µ–π –Ω–µ—Ç.",
                        thought=thought,
                    )

                # –ï—Å–ª–∏ —ç—Ç–æ –Ω–∞—à —Ñ–æ—Ä–º–∞—Ç
                if "tool" in data and "tool_input" in data:
                    raw_tool = data.get("tool")
                    tool_name = str(raw_tool).strip().lower() if raw_tool and not isinstance(raw_tool, list) else ""
                    # tool –∫–∞–∫ –º–∞—Å—Å–∏–≤ –∏–ª–∏ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç ‚Äî –æ—Ç–∫–ª–æ–Ω—è–µ–º (–º–∏—Ä–æ–≤–∞—è –ø—Ä–∞–∫—Ç–∏–∫–∞: strict schema)
                    if isinstance(raw_tool, list):
                        tool_name = (raw_tool[0] if raw_tool else "") or "unknown"
                    
                    logger.info(f"[LLM_PARSE] Detected tool: '{tool_name}', thought: '{thought[:50]}...'")
                    
                    if tool_name not in ALLOWED_TOOLS:
                        bad = raw_tool if isinstance(raw_tool, str) else (raw_tool[0] if isinstance(raw_tool, list) and raw_tool else raw_tool)
                        logger.warning(f"[LLM_PARSE] Unknown tool '{bad}' rejected")
                        return AgentFinish(
                            output=f"–î–æ—Å—Ç—É–ø–Ω—ã —Ç–æ–ª—å–∫–æ: finish, read_file, list_directory, run_terminal_cmd, ssh_run. –¢—ã —É–∫–∞–∑–∞–ª: {bad}. –û—Ç–≤–µ—Ç—å –æ–¥–Ω–∏–º JSON —Å tool: finish –∏ tool_input: {{\"output\": \"—Ç–≤–æ–π –∫—Ä–∞—Ç–∫–∏–π –æ—Ç–≤–µ—Ç\"}}.",
                            thought=thought,
                        )
                    if blocked_tools and tool_name in blocked_tools:
                        logger.warning(f"[LLM_PARSE] Blocked tool '{tool_name}' rejected (cycle prevention)")
                        allowed = sorted(ALLOWED_TOOLS - set(blocked_tools))
                        return AgentFinish(
                            output=f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç {tool_name} –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω –∏–∑-–∑–∞ —Ü–∏–∫–ª–∞. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ: {', '.join(allowed)}. –û—Ç–≤–µ—Ç—å JSON —Å tool: finish –∏–ª–∏ –¥—Ä—É–≥–∏–º –¥–æ—Å—Ç—É–ø–Ω—ã–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º.",
                            thought=thought,
                        )
                    if data["tool"] == "finish" or (data.get("tool") == "" and not tool_input):
                        out = (tool_input.get("output") if tool_input else None) or thought or "–ì–æ—Ç–æ–≤–æ"
                        logger.info(f"[LLM_PARSE] Returning AgentFinish: {str(out)[:100]}...")
                        return AgentFinish(output=out if isinstance(out, str) else str(out), thought=thought)
                    if tool_input is not None:
                        logger.info(f"[LLM_PARSE] Returning AgentAction: tool={tool_name}, input={str(tool_input)[:100]}")
                        return AgentAction(tool=tool_name, tool_input=data["tool_input"], thought=thought)
                
                # –ò—â–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –≤–æ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö –ø–æ–ª—è—Ö (action, next_step, step)
                for key in ["action", "next_step", "step"]:
                    if key in data and isinstance(data[key], dict):
                        nested = data[key]
                        if "tool" in nested and "tool_input" in nested:
                            logger.info(f"[LLM_PARSE] Found nested tool in '{key}': {nested['tool']}")
                            return AgentAction(tool=str(nested["tool"]), tool_input=nested["tool_input"], thought=thought)
                        if "command" in nested:
                            host = nested.get("host", "185.177.216.15")
                            logger.info(f"[LLM_PARSE] Found nested command in '{key}': {nested['command'][:50]}")
                            return AgentAction(tool="ssh_run", tool_input={"host": host, "command": nested["command"]}, thought=thought)

                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–∏ —Ñ–æ—Ä–º–∞—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å command –≤–º–µ—Å—Ç–æ tool)
                if "command" in data:
                    host = data.get("host", "185.177.216.15")
                    logger.info(f"[LLM_PARSE] Found top-level command: {data['command'][:50]}")
                    return AgentAction(tool="ssh_run", tool_input={"host": host, "command": data["command"]}, thought=thought)

                # –ï—Å–ª–∏ —ç—Ç–æ –ª—é–±–æ–π –¥—Ä—É–≥–æ–π JSON
                msg = data.get("response") or data.get("message") or data.get("output") or str(data)
                logger.info(f"[LLM_PARSE] Returning generic JSON response: {str(msg)[:100]}")
                return AgentFinish(output=msg, thought=thought)
            else:
                logger.warning(f"[LLM_PARSE] No JSON block found in content")
            
        except Exception as e:
            logger.error(f"[LLM_PARSE] ‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            logger.error(f"[LLM_PARSE] Content was: {content[:500]}")
            return AgentFinish(output=clean_content, thought=f"Parser Error: {str(e)}")
            
        # –ï—Å–ª–∏ –Ω–µ JSON –∏–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –Ω–µ —É–¥–∞–ª—Å—è - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
        logger.info(f"[LLM_PARSE] Returning raw text response")
        return AgentFinish(output=clean_content, thought="–¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç")
