#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏–∑ backup —Ñ–∞–π–ª–∞ —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏
"""

import json
import os
from datetime import datetime
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_patterns_safely(file_path, max_patterns=None):
    """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏"""
    try:
        logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if max_patterns and len(data) > max_patterns:
            logger.warning(f"–§–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç {len(data)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤, –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ {max_patterns}")
            data = data[:max_patterns]
        
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
        return data
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return []

def merge_patterns(current_file, backup_file, output_file, max_total_patterns=100000):
    """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∏–∑ –¥–≤—É—Ö —Ñ–∞–π–ª–æ–≤"""
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    current_patterns = load_patterns_safely(current_file)
    logger.info(f"–¢–µ–∫—É—â–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {len(current_patterns)}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º backup –ø–∞—Ç—Ç–µ—Ä–Ω—ã (–æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
    backup_patterns = load_patterns_safely(backup_file, max_patterns=50000)
    logger.info(f"Backup –ø–∞—Ç—Ç–µ—Ä–Ω—ã: {len(backup_patterns)}")
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    all_patterns = current_patterns + backup_patterns
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
    if len(all_patterns) > max_total_patterns:
        logger.warning(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ ({len(all_patterns)}) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –ª–∏–º–∏—Ç ({max_total_patterns})")
        all_patterns = all_patterns[:max_total_patterns]
    
    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ symbol + timestamp + signal_type
    unique_patterns = []
    seen = set()
    
    for pattern in all_patterns:
        key = (pattern.get('symbol', ''), 
               pattern.get('timestamp', ''), 
               pattern.get('signal_type', ''))
        
        if key not in seen:
            seen.add(key)
            unique_patterns.append(pattern)
    
    logger.info(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {len(unique_patterns)} –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(unique_patterns, f, indent=2, ensure_ascii=False)
        
        logger.info(f"–ü–∞—Ç—Ç–µ—Ä–Ω—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {output_file}")
        return True
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {output_file}: {e}")
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    try:
        from src.config.patterns import get_patterns_file_path
    except ImportError:
        from patterns_config import get_patterns_file_path
    current_file = get_patterns_file_path("main")
    backup_file = "/Users/zhuchyok/Documents/GITHUB/trading_patterns_backup_20251018_182917.json"
    output_file = get_patterns_file_path("merged")
    
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤
    if not os.path.exists(current_file):
        logger.error(f"–¢–µ–∫—É—â–∏–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {current_file}")
        return False
    
    if not os.path.exists(backup_file):
        logger.error(f"Backup —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {backup_file}")
        return False
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    success = merge_patterns(current_file, backup_file, output_file)
    
    if success:
        logger.info("‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤: {output_file}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        with open(output_file, 'r', encoding='utf-8') as f:
            merged_data = json.load(f)
        
        logger.info(f"üìä –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        logger.info(f"   - –í—Å–µ–≥–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {len(merged_data)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
        results = {}
        for pattern in merged_data:
            result = pattern.get('result', 'UNKNOWN')
            results[result] = results.get(result, 0) + 1
        
        for result, count in results.items():
            logger.info(f"   - {result}: {count}")
        
        return True
    else:
        logger.error("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤")
        return False

if __name__ == "__main__":
    main()
