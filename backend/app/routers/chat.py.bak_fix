"""
Chat Router - SSE —Å—Ç—Ä–∏–º–∏–Ω–≥ –¥–ª—è AI —á–∞—Ç–∞ (Singularity 9.0)
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Streaming –∏ Emotional Modulation
–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
"""
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import Optional, AsyncGenerator
import json
import logging

from app.services.victoria import VictoriaClient, get_victoria_client
from app.services.ollama import OllamaClient, get_ollama_client
from app.services.mlx import MLXClient, get_mlx_client
from app.services.streaming import StreamingProcessor, create_sse_event
from app.services.emotions import detect_emotion, get_adapted_prompt, Emotion
from app.services.cache import get_cache, cache_key
import httpx

logger = logging.getLogger(__name__)
router = APIRouter()


class ChatMessage(BaseModel):
    """–°–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç"""
    content: str = Field(..., min_length=1, max_length=10000)
    expert_name: Optional[str] = Field(default=None, max_length=100)
    model: Optional[str] = Field(default=None, max_length=100)
    use_victoria: bool = True


# –ü—Ä–æ—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –Ω—É–∂–µ–Ω –∞–≥–µ–Ω—Ç Victoria (–±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å —á–µ—Ä–µ–∑ Ollama)
SIMPLE_PATTERNS = [
    "–ø—Ä–∏–≤–µ—Ç", "hello", "hi", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä",
    "–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ —Ç—ã", "—á—Ç–æ —É–º–µ–µ—à—å", "–∫—Ç–æ —Ç—ã", "–ø–æ–º–æ–≥–∏", "—Ä–∞—Å—Å–∫–∞–∂–∏",
    "—Å–ø–∞—Å–∏–±–æ", "thanks", "–ø–æ–∫–∞", "bye", "good", "–æ–±—ä—è—Å–Ω–∏", "explain",
    "–Ω–∞–ø–∏—à–∏", "write", "–ø–æ–∫–∞–∂–∏", "show", "–∫–æ–¥", "code",
    "—Ñ—É–Ω–∫—Ü–∏", "function", "–∫–ª–∞—Å—Å", "class", "python", "javascript", "rust",
    "—á—Ç–æ —Ç–∞–∫–æ–µ", "what is", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç", "how does", "–∑–∞—á–µ–º", "–ø–æ—á–µ–º—É",
    "–≥–¥–µ", "when", "–∫–∞–∫–æ–π", "which", "—Å–∫–æ–ª—å–∫–æ"
]

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è Victoria Agent (—Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è, —Å–µ—Ä–≤–µ—Ä–∞)
VICTORIA_PATTERNS = [
    "—Ñ–∞–π–ª –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ", "ssh", "–ø–æ–¥–∫–ª—é—á–∏—Å—å", "–∑–∞–ø—É—Å—Ç–∏ –Ω–∞", "–≤—ã–ø–æ–ª–Ω–∏ –∫–æ–º–∞–Ω–¥—É",
    "—Å–æ–∑–¥–∞–π –ø—Ä–æ–µ–∫—Ç", "—Ä–∞–∑–≤–µ—Ä–Ω–∏", "deploy", "docker", "–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä",
    "–∫–æ—Ä–ø–æ—Ä–∞—Ü", "—Å–µ—Ä–≤–µ—Ä", "—Å—Ç–∞—Ç—É—Å", "–ø—Ä–æ–≤–µ—Ä—å", "victoria", "–≤–∏–∫—Ç–æ—Ä–∏—è",
    "–∞–≥–µ–Ω—Ç", "–∑–∞–¥–∞—á", "mac studio", "–º–∞–∫—Å—Ç—É–¥–∏–æ", "ollama", "mlx"
]


def is_simple_message(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã–º (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∞–≥–µ–Ω—Ç–∞)"""
    lower = content.lower().strip()
    
    # –ï—Å–ª–∏ —è–≤–Ω–æ –Ω—É–∂–µ–Ω Victoria Agent
    for pattern in VICTORIA_PATTERNS:
        if pattern in lower:
            return False
    
    # –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π - –ø—Ä–æ—Å—Ç—ã–µ (–±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å)
    if len(lower) < 200:
        return True
    
    # –î–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—Ä–æ—Å—Ç—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ —Ç–æ–∂–µ –ø—Ä–æ—Å—Ç—ã–µ
    for pattern in SIMPLE_PATTERNS:
        if pattern in lower:
            return True
    
    return False


def _select_model_for_chat(content: str, expert_name: Optional[str] = None) -> str:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä—Ç–∞
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ 8 –º–æ–¥–µ–ª–µ–π Mac Studio M4 Max:
    - complex/enterprise ‚Üí command-r-plus:104b (~65GB)
    - reasoning ‚Üí deepseek-r1-distill-llama:70b (~40GB)
    - complex ‚Üí llama3.3:70b (~40GB)
    - coding (high quality) ‚Üí qwen2.5-coder:32b (~20GB)
    - fast/general ‚Üí phi3.5:3.8b (~2.5GB)
    - fast (lightweight) ‚Üí phi3:mini-4k (~2GB)
    - fast/default ‚Üí qwen2.5:3b (~2GB)
    - fast (ultra-lightweight) ‚Üí tinyllama:1.1b-chat (~700MB)
    """
    content_lower = content.lower()
    
    # –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ, RAG, enterprise
    if any(word in content_lower for word in ["—Å–ª–æ–∂–Ω", "–∫–æ—Ä–ø–æ—Ä–∞—Ü", "rag", "enterprise", "–∫—Ä–∏—Ç–∏—á–Ω", "–≤–∞–∂–Ω", "—Å—Ç—Ä–∞—Ç–µ–≥"]):
        return "command-r-plus:104b"  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å
    
    # Reasoning, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ª–æ–≥–∏–∫–∞, –∞–Ω–∞–ª–∏–∑
    if any(word in content_lower for word in ["–ø–æ–¥—É–º–∞–π", "–ª–æ–≥–∏–∫–∞", "–ø–ª–∞–Ω–∏—Ä", "reasoning", "–∞–Ω–∞–ª–∏–∑", "–æ–±—ä—è—Å–Ω–∏", "–ø–æ—á–µ–º—É"]):
        return "deepseek-r1-distill-llama:70b"  # Reasoning
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏
    if any(word in content_lower for word in ["–∫–∞—á–µ—Å—Ç–≤", "–ª—É—á—à", "–æ–ø—Ç–∏–º–∞–ª—å–Ω", "–º–∞–∫—Å–∏–º–∞–ª—å–Ω", "–¥–µ—Ç–∞–ª—å–Ω"]):
        return "llama3.3:70b"  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    
    # –ö–æ–¥, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ (high quality)
    if any(word in content_lower for word in ["–∫–æ–¥", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä", "—Ñ—É–Ω–∫—Ü–∏", "–∫–ª–∞—Å—Å", "python", "javascript", "typescript", "–∞–ª–≥–æ—Ä–∏—Ç–º"]):
        return "qwen2.5-coder:32b"  # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥
    
    # –ë—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏, –æ–±—â–∏–µ (medium)
    if len(content) > 200 or any(word in content_lower for word in ["—Ä–∞—Å—Å–∫–∞–∂–∏", "–æ–±—ä—è—Å–Ω–∏", "–æ–ø–∏—Å–∞"]):
        return "phi3.5:3.8b"  # Fast, general
    
    # –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã, –ª–µ–≥–∫–∏–µ –∑–∞–¥–∞—á–∏ (lightweight)
    if len(content) < 200:
        return "phi3:mini-4k"  # Fast, lightweight
    
    # –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–µ (ultra-lightweight) - –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    if len(content) < 100:
        return "tinyllama:1.1b-chat"  # Ultra-lightweight
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
    return "qwen2.5:3b"  # Fast, default


async def _get_available_model(ideal_model: str, ollama: OllamaClient) -> str:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    
    Args:
        ideal_model: –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏
        ollama: OllamaClient –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
    
    Returns:
        –î–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å (–∏–¥–µ–∞–ª—å–Ω–∞—è –∏–ª–∏ fallback)
    """
    # –ú–∞–ø–ø–∏–Ω–≥ –∏–¥–µ–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω –≤ Ollama
    model_variants = {
        "command-r-plus:104b": ["command-r-plus", "command-r-plus:104b", "command-r", "c4ai-command-r-plus"],
        "deepseek-r1-distill-llama:70b": ["deepseek-r1-distill-llama", "deepseek-r1-distill", "deepseek-r1:70b", "deepseek-r1"],
        "llama3.3:70b": ["llama3.3", "llama3.3:70b", "llama-3.3", "llama"],
        "qwen2.5-coder:32b": ["qwen2.5-coder:32b", "qwen2.5-coder-32b", "qwen2.5-coder", "qwen-coder-32"],
        "phi3.5:3.8b": ["phi3.5", "phi3.5:3.8b", "phi-3.5", "phi3.5-mini"],
        "phi3:mini-4k": ["phi3:mini", "phi3-mini", "phi3:mini-4k", "phi-3-mini"],
        "qwen2.5:3b": ["qwen2.5:3b", "qwen2.5-3b", "qwen2.5", "qwen-3b"],
        "tinyllama:1.1b-chat": ["tinyllama", "tinyllama:1.1b", "tinyllama-1.1b", "tiny-llama"]
    }
    
    # Fallback —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–≤—Å–µ 8 –º–æ–¥–µ–ª–µ–π Mac Studio)
    # –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç —Ç–∞–±–ª–∏—Ü–µ –∏–∑ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    fallback_chains = {
        "command-r-plus:104b": ["llama3.3:70b", "qwen2.5-coder:7b"],
        "deepseek-r1-distill-llama:70b": ["deepseek-r1:7b", "qwen2.5-coder:7b"],
        "llama3.3:70b": ["deepseek-r1-distill-llama:70b", "qwen2.5-coder:7b"],
        "qwen2.5-coder:32b": ["qwen2.5-coder:7b", "qwen2.5-coder:3b"],
        "phi3.5:3.8b": ["phi4:latest", "qwen2.5-coder:3b"],
        "phi3:mini-4k": ["qwen2.5-coder:3b", "phi4:latest"],
        "qwen2.5:3b": ["qwen2.5-coder:3b", "phi4:latest"],
        "tinyllama:1.1b-chat": ["qwen2.5-coder:3b", "phi4:latest"]
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    try:
        models = await ollama.list_models()
        available_names = [m.get("name", "") for m in models]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if ideal_model in available_names:
            logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {ideal_model}")
            return ideal_model
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        variants = model_variants.get(ideal_model, [ideal_model])
        for variant in variants:
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–∞
            if variant in available_names:
                logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç –º–æ–¥–µ–ª–∏: {variant} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                return variant
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            for name in available_names:
                variant_base = variant.split(":")[0].split("-")[0]
                name_base = name.split(":")[0].split("-")[0]
                if variant_base in name_base or name_base in variant_base:
                    if len(variant_base) > 3:  # –ò–∑–±–µ–≥–∞–µ–º —Å–ª–∏—à–∫–æ–º –æ–±—â–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                        logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ö–æ–∂—É—é –º–æ–¥–µ–ª—å: {name} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                        return name
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ü–µ–ø–æ—á–∫—É
        fallbacks = fallback_chains.get(ideal_model, ["qwen2.5-coder:7b", "qwen2.5-coder:3b", "phi4:latest"])
        for fallback in fallbacks:
            if fallback in available_names:
                logger.info(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback: {fallback} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                return fallback
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è fallback
            for name in available_names:
                fallback_base = fallback.split(":")[0].split("-")[0]
                name_base = name.split(":")[0].split("-")[0]
                if fallback_base in name_base or name_base in fallback_base:
                    if len(fallback_base) > 3:
                        logger.info(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ö–æ–∂–∏–π fallback: {name} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                        return name
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤ - –ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å (–∫—Ä–æ–º–µ embeddings –∏ vision)
        for name in available_names:
            if "embed" not in name.lower() and "dream" not in name.lower():
                logger.warning(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å: {name} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                return name
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–ø—É—Å—Ç—å Ollama –≤–µ—Ä–Ω–µ—Ç –æ—à–∏–±–∫—É)
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º {ideal_model}")
        return ideal_model
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        return ideal_model


class ChatResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –æ—Ç —á–∞—Ç–∞"""
    content: str
    expert_name: Optional[str] = None
    model: Optional[str] = None
    tokens_used: Optional[int] = None


async def sse_generator(
    message: ChatMessage,
    victoria: VictoriaClient,
    ollama: OllamaClient,
    mlx: MLXClient
) -> AsyncGenerator[str, None]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SSE —Å–æ–±—ã—Ç–∏–π (Singularity 9.0)
    
    Yields:
        SSE —Å–æ–±—ã—Ç–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ: data: {...}\n\n
    """
    # Singularity v9.0: –î–µ—Ç–µ–∫—Ü–∏—è —ç–º–æ—Ü–∏–π
    emotion, confidence = detect_emotion(message.content)
    emotion_data = {
        'emotion': emotion.value,
        'confidence': round(confidence, 2)
    }
    
    # Singularity v5.0: Streaming processor
    processor = StreamingProcessor(buffer_size=3, min_delay=0.03, max_delay=0.1)
    
    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± —ç–º–æ—Ü–∏–∏
        start_event = {
            'type': 'start', 
            'expert': message.expert_name,
            'emotion': emotion_data
        }
        yield f"data: {json.dumps(start_event)}\n\n"
        
        # –£–º–Ω—ã–π —Ä–æ—É—Ç–∏–Ω–≥: –ø—Ä–æ—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è -> Ollama, —Å–ª–æ–∂–Ω—ã–µ -> Victoria
        use_ollama_direct = is_simple_message(message.content) or not message.use_victoria
        
        if not use_ollama_direct:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Victoria
            result = await victoria.run(
                prompt=message.content,
                expert_name=message.expert_name
            )
            
            if "error" in result:
                # Fallback –æ—Ç–≤–µ—Ç –∫–æ–≥–¥–∞ Victoria –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
                logger.warning(f"Victoria –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –æ—Ç–≤–µ—Ç")
                fallback_response = f"–ü—Ä–∏–≤–µ—Ç! –Ø {message.expert_name or '–í–∏–∫—Ç–æ—Ä–∏—è'}. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å–µ–π—á–∞—Å –Ω–µ –º–æ–≥—É –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Victoria Agent. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–æ—Å—Ç–æ–π —Ä–µ–∂–∏–º —á–∞—Ç–∞."
                words = fallback_response.split()
                buffer = ""
                for i, word in enumerate(words):
                    buffer += word + " "
                    if i % 5 == 0:
                        yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
                        buffer = ""
                if buffer:
                    yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
            else:
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –ø–æ —á–∞—Å—Ç—è–º –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
                content = result.get("result", "") or result.get("response", "") or result.get("output", "")
                if not content:
                    logger.warning(f"Victoria –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç: {result}")
                    content = f"–ü—Ä–∏–≤–µ—Ç! –Ø {message.expert_name or '–í–∏–∫—Ç–æ—Ä–∏—è'}. –ü–æ–ª—É—á–∏–ª –≤–∞—à –∑–∞–ø—Ä–æ—Å, –Ω–æ –æ—Ç–≤–µ—Ç –ø–æ–∫–∞ –ø—É—Å—Ç–æ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                words = content.split()
                buffer = ""
                for i, word in enumerate(words):
                    buffer += word + " "
                    if i % 5 == 0:  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–ª–æ–≤
                        yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
                        buffer = ""
                
                if buffer:
                    yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Ollama/MLX –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è –ø—Ä–æ—Å—Ç—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            expert_prompt = ""
            if message.expert_name:
                expert_prompt = f"–¢—ã - {message.expert_name}, —ç–∫—Å–ø–µ—Ä—Ç ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n\n"
            
            full_prompt = expert_prompt + message.content
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∑–∞–¥–∞—á–∏
            # –°–Ω–∞—á–∞–ª–∞ –≤—ã–±–∏—Ä–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å, –ø–æ—Ç–æ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
            ideal_model = message.model or _select_model_for_chat(message.content, message.expert_name)
            logger.info(f"üéØ –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è '{message.content[:50]}...': {ideal_model}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            selected_model = await _get_available_model(ideal_model, ollama)
            logger.info(f"‚úÖ –í—ã–±—Ä–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {selected_model}")
            
            result = None
            try:
                logger.info(f"üöÄ –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama —Å –º–æ–¥–µ–ª—å—é: {selected_model}")
                result = await ollama.generate(
                    prompt=full_prompt,
                    model=selected_model,
                    system="–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
                    stream=False
                )
                logger.info(f"‚úÖ Ollama –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {bool(result and 'error' not in result)}")
            except Exception as e:
                logger.error(f"Ollama generate exception: {e}")
                result = {"error": str(e)}
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º MLX –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤ –ø—Ä–∏ –æ—à–∏–±–∫–µ Ollama
            if result is None:
                result = {"error": "No response from Ollama"}
            
            if isinstance(result, dict) and "error" in result:
                # –ü—Ä–æ–±—É–µ–º MLX –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç
                error_msg = result.get('error', 'Unknown error')
                logger.warning(f"Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω ({error_msg[:100]}), –ø—Ä–æ–±—É–µ–º MLX –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤")
                logger.info(f"MLX available check: {mlx.is_available()}")
                
                if mlx.is_available():
                    try:
                        logger.info("üçé [MLX] –ò—Å–ø–æ–ª—å–∑—É–µ–º MLX –∫–∞–∫ —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç")
                        mlx_result = await mlx.generate(
                            prompt=full_prompt,
                            system="–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
                            max_tokens=256
                        )
                        
                        if mlx_result and "response" in mlx_result:
                            # MLX —É—Å–ø–µ—à–Ω–æ –æ—Ç–≤–µ—Ç–∏–ª
                            content = mlx_result.get("response", "")
                            words = content.split()
                            chunk = ""
                            for i, word in enumerate(words):
                                chunk += word + " "
                                if i % 3 == 0 and chunk:
                                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                                    chunk = ""
                            if chunk:
                                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                        else:
                            # MLX —Ç–æ–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback
                            logger.warning("MLX —Ç–æ–∂–µ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π fallback")
                            expert_name = message.expert_name or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                            fallback_response = f"–ü—Ä–∏–≤–µ—Ç! –Ø {expert_name}. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å–µ–π—á–∞—Å –Ω–µ –º–æ–≥—É –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ AI-–º–æ–¥–µ–ª–∏ (Ollama –∏ MLX –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                            words = fallback_response.split()
                            chunk = ""
                            for i, word in enumerate(words):
                                chunk += word + " "
                                if i % 3 == 0 and chunk:
                                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                                    chunk = ""
                            if chunk:
                                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                    except Exception as e:
                        logger.error(f"MLX generate error: {e}")
                        # Fallback –ø—Ä–∏ –æ—à–∏–±–∫–µ MLX
                        expert_name = message.expert_name or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                        fallback_response = f"–ü—Ä–∏–≤–µ—Ç! –Ø {expert_name}. –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ MLX. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                        words = fallback_response.split()
                        chunk = ""
                        for i, word in enumerate(words):
                            chunk += word + " "
                            if i % 3 == 0 and chunk:
                                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                                chunk = ""
                        if chunk:
                            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                else:
                    # MLX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π fallback
                    logger.warning("MLX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π fallback")
                    expert_name = message.expert_name or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                    fallback_response = f"–ü—Ä–∏–≤–µ—Ç! –Ø {expert_name}. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —Å–µ–π—á–∞—Å –Ω–µ –º–æ–≥—É –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ AI-–º–æ–¥–µ–ª–∏ (Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, MLX –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω). –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama –Ω–∞ –ø–æ—Ä—Ç—É 11434."
                    words = fallback_response.split()
                    chunk = ""
                    for i, word in enumerate(words):
                        chunk += word + " "
                        if i % 3 == 0 and chunk:
                            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                            chunk = ""
                    if chunk:
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
            elif isinstance(result, dict) and result.get("response"):
                # –†–∞–∑–±–∏–≤–∞–µ–º –æ—Ç–≤–µ—Ç –Ω–∞ —á–∞—Å—Ç–∏ –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ —Å—Ç—Ä–∏–º–∏–Ω–≥–∞
                content = result.get("response", "")
                if not content:
                    content = "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç."
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ —Å–ª–æ–≤–∞–º –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                words = content.split()
                chunk = ""
                for i, word in enumerate(words):
                    chunk += word + " "
                    if i % 3 == 0 and chunk:  # –ö–∞–∂–¥—ã–µ 3 —Å–ª–æ–≤–∞
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                        chunk = ""
                if chunk:
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
            else:
                # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç Ollama: {result}")
                expert_name = message.expert_name or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                fallback_response = f"–ü—Ä–∏–≤–µ—Ç! –Ø {expert_name}. –ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞."
                words = fallback_response.split()
                chunk = ""
                for i, word in enumerate(words):
                    chunk += word + " "
                    if i % 3 == 0 and chunk:
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                        chunk = ""
                if chunk:
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"SSE error: {e}", exc_info=True)
        # –í–º–µ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤–∫–∏ error, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º fallback –æ—Ç–≤–µ—Ç
        expert_name = message.expert_name or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
        fallback_response = f"–ü—Ä–∏–≤–µ—Ç! –Ø {expert_name}. –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)[:100]}. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        words = fallback_response.split()
        chunk = ""
        for i, word in enumerate(words):
            chunk += word + " "
            if i % 3 == 0 and chunk:
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                chunk = ""
        if chunk:
            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
        yield f"data: {json.dumps({'type': 'end'})}\n\n"


@router.post("/send", response_model=ChatResponse)
async def send_message(
    message: ChatMessage,
    victoria: VictoriaClient = Depends(get_victoria_client)
) -> ChatResponse:
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ (–Ω–µ-—Å—Ç—Ä–∏–º–∏–Ω–≥)
    
    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç —á–∞—Ç–∞
    """
    try:
        result = await victoria.run(
            prompt=message.content,
            expert_name=message.expert_name
        )
        
        if "error" in result:
            raise HTTPException(
                status_code=500,
                detail=result["error"]
            )
        
        return ChatResponse(
            content=result.get("result", "") or result.get("response", ""),
            expert_name=message.expert_name,
            model=result.get("model")
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat send error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error while processing chat message"
        )


@router.post("/stream")
async def stream_message(
    message: ChatMessage,
    victoria: VictoriaClient = Depends(get_victoria_client),
    ollama: OllamaClient = Depends(get_ollama_client),
    mlx: MLXClient = Depends(get_mlx_client)
) -> StreamingResponse:
    """
    SSE —Å—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ (Singularity 9.0)
    
    Returns:
        StreamingResponse —Å SSE —Å–æ–±—ã—Ç–∏—è–º–∏
    """
    return StreamingResponse(
        sse_generator(message, victoria, ollama, mlx),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.get("/status")
async def chat_status(
    victoria: VictoriaClient = Depends(get_victoria_client),
    ollama: OllamaClient = Depends(get_ollama_client)
) -> dict:
    """
    –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤ —á–∞—Ç–∞
    
    Returns:
        –°—Ç–∞—Ç—É—Å Victoria –∏ Ollama
    """
    try:
        victoria_health = await victoria.health()
        ollama_health = await ollama.health()
        
        return {
            "victoria": victoria_health,
            "ollama": ollama_health,
        }
    except Exception as e:
        logger.error(f"Chat status error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Error checking chat services status"
        )


@router.get("/models")
async def list_models(
    ollama: OllamaClient = Depends(get_ollama_client)
) -> dict:
    """
    –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π Ollama
    """
    cache = get_cache()
    cache_key = "ollama:models"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached = cache.get(cache_key)
    if cached is not None:
        return cached
    
    try:
        models = await ollama.list_models()
        result = {
            "models": [
                {
                    "name": m.get("name"),
                    "size": m.get("size"),
                    "modified": m.get("modified_at")
                }
                for m in models
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à (1 –º–∏–Ω—É—Ç–∞)
        cache.set(cache_key, result, ttl=60)
        
        return result
    except Exception as e:
        logger.error(f"List models error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Error fetching models list"
        )
