"""
Chat Router - SSE —Å—Ç—Ä–∏–º–∏–Ω–≥ –¥–ª—è AI —á–∞—Ç–∞ (Singularity 9.0)
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Streaming –∏ Emotional Modulation
–£–ª—É—á—à–µ–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
"""
from fastapi import APIRouter, Depends, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
from typing import Optional, AsyncGenerator
import json
import logging
import os
import asyncio

from app.config import get_settings
from app.services.victoria import VictoriaClient, get_victoria_client
from app.services.mlx import MLXClient, get_mlx_client
from app.services.ollama import OllamaClient, get_ollama_client
from app.services.knowledge_os import KnowledgeOSClient, get_knowledge_os_client
from app.services.streaming import StreamingProcessor, create_sse_event
from app.services.emotions import detect_emotion, get_adapted_prompt, Emotion
from app.services.cache import get_cache, cache_key
from app.services.query_classifier import classify_query, get_template_response, analyze_complexity
from app.services.rag_light import get_rag_light_service
from app.services.plan_cache import get_plan_cache_service, PlanCacheService
from app.services.conversation_context import get_conversation_context_manager
from app.metrics.agent_suggestions import agent_suggestion_metrics, AgentSuggestionMetric
from app.metrics.prometheus_metrics import (
    metrics as prometheus_metrics,
    PLAN_REQUESTS,
    PLAN_DURATION,
    PLAN_STEPS_COUNT,
)
from app.services.concurrency_limiter import (
    acquire_victoria_slot,
    release_victoria_slot,
    with_victoria_slot,
)
import httpx
import time
import uuid

logger = logging.getLogger(__name__)
router = APIRouter()

# –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–π –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏—è (—Å–∏–º–≤–æ–ª—ã)
_REPEAT_CHECK_MIN_LEN = 200
# –ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–æ–≤ –æ–¥–Ω–æ–π –∏ —Ç–æ–π –∂–µ —Ñ—Ä–∞–∑—ã ‚Äî –ø–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ–±—Ä–µ–∑–∞–µ–º
_MAX_REPEATS_ALLOWED = 2
# –î–ª–∏–Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ–≤—Ç–æ—Ä–∞ (–æ–¥–Ω–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ, –Ω–∞–ø—Ä. ¬´–í–∏–∫—Ç–æ—Ä–∏—è: –Ø - –≤–∏–∫—Ç–æ—Ä–∏—è, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç...¬ª)
_REPEAT_PATTERN_LEN = 100


def _truncate_repeated_response(content: str) -> str:
    """
    –û–±—Ä–µ–∑–∞–µ—Ç –æ—Ç–≤–µ—Ç, –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞—Ü–∏–∫–ª–∏–ª–∞—Å—å –Ω–∞ –æ–¥–Ω–æ–π —Ñ—Ä–∞–∑–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä ¬´–í–∏–∫—Ç–æ—Ä–∏—è: –Ø - –≤–∏–∫—Ç–æ—Ä–∏—è, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç...¬ª).
    –û—Å—Ç–∞–≤–ª—è–µ—Ç –Ω–µ –±–æ–ª–µ–µ _MAX_REPEATS_ALLOWED –ø–æ–≤—Ç–æ—Ä–æ–≤ –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–º–µ—Ç–∫—É.
    """
    if not content or len(content) < _REPEAT_CHECK_MIN_LEN:
        return content
    text = content.strip()
    pattern_len = min(_REPEAT_PATTERN_LEN, len(text) // 2)
    if pattern_len < 50:
        return content
    sample = text[:pattern_len]
    # –°—á–∏—Ç–∞–µ–º, —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø–æ–¥—Ä—è–¥ –≤ –Ω–∞—á–∞–ª–µ —Ç–µ–∫—Å—Ç–∞ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è —Ç–æ—Ç –∂–µ –±–ª–æ–∫
    start = 0
    repeat_count = 0
    while start + pattern_len <= len(text):
        if text[start : start + pattern_len] == sample:
            repeat_count += 1
            start += pattern_len
        else:
            break
    if repeat_count <= _MAX_REPEATS_ALLOWED:
        return content
    cut = pattern_len * _MAX_REPEATS_ALLOWED
    space_at = text.rfind(" ", 0, min(cut + 1, len(text)))
    if space_at > cut // 2:
        cut = space_at
    result = text[:cut].strip()
    if result and result[-1] not in ".!?":
        result += "."
    return result + "\n\n(–ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–æ)"


async def _log_chat_to_knowledge_os(prompt: str, response: str, expert_name: Optional[str] = None) -> None:
    """Fire-and-forget: –ª–æ–≥–∏—Ä—É–µ—Ç —á–∞—Ç –≤ interaction_logs (Singularity 9.0) —á–µ—Ä–µ–∑ Knowledge OS API."""
    try:
        settings = get_settings()
        url = f"{settings.knowledge_os_api_url.rstrip('/')}/api/log_interaction"
        logger.info("[LOG_INTERACTION] url=%s prompt_len=%s response_len=%s expert=%s", url, len(prompt), len(response), expert_name)
        async with httpx.AsyncClient(timeout=5.0) as client:
            r = await client.post(
                url,
                json={
                    "prompt": prompt[:10000],
                    "response": response[:20000],
                    "expert_name": expert_name,
                    "source": "web_ide",
                },
            )
        if r.status_code != 200:
            logger.warning("[LOG_INTERACTION] status=%s body=%s", r.status_code, (r.text or "")[:200])
    except Exception as e:
        logger.error("[LOG_INTERACTION] error=%s", e, exc_info=True)


class ChatMessage(BaseModel):
    """–°–æ–æ–±—â–µ–Ω–∏–µ –≤ —á–∞—Ç"""
    content: str = Field(..., min_length=1, max_length=10000)
    expert_name: Optional[str] = Field(default=None, max_length=100)
    model: Optional[str] = Field(default=None, max_length=100)
    use_victoria: bool = True
    mode: Optional[str] = Field(default="agent", description="agent | plan | ask ‚Äî –∫–∞–∫ –≤ Cursor")
    user_id: Optional[str] = Field(default=None, max_length=128, description="–î–ª—è A/B —Ç–µ—Å—Ç–æ–≤")
    session_id: Optional[str] = Field(default=None, max_length=128, description="–§–∞–∑–∞ 4.2: –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ (multi-turn)")


# –ü—Ä–æ—Å—Ç—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –Ω—É–∂–µ–Ω –∞–≥–µ–Ω—Ç Victoria (–±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å —á–µ—Ä–µ–∑ MLX)
SIMPLE_PATTERNS = [
    "–ø—Ä–∏–≤–µ—Ç", "hello", "hi", "–∑–¥—Ä–∞–≤—Å—Ç–≤—É–π", "–¥–æ–±—Ä—ã–π –¥–µ–Ω—å", "–¥–æ–±—Ä—ã–π –≤–µ—á–µ—Ä",
    "–∫–∞–∫ –¥–µ–ª–∞", "–∫–∞–∫ —Ç—ã", "—á—Ç–æ —É–º–µ–µ—à—å", "–∫—Ç–æ —Ç—ã", "–ø–æ–º–æ–≥–∏", "—Ä–∞—Å—Å–∫–∞–∂–∏",
    "—Å–ø–∞—Å–∏–±–æ", "thanks", "–ø–æ–∫–∞", "bye", "good", "–æ–±—ä—è—Å–Ω–∏", "explain",
    "–Ω–∞–ø–∏—à–∏", "write", "–ø–æ–∫–∞–∂–∏", "show", "–∫–æ–¥", "code",
    "—Ñ—É–Ω–∫—Ü–∏", "function", "–∫–ª–∞—Å—Å", "class", "python", "javascript", "rust",
    "—á—Ç–æ —Ç–∞–∫–æ–µ", "what is", "–∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç", "how does", "–∑–∞—á–µ–º", "–ø–æ—á–µ–º—É",
    "–≥–¥–µ", "when", "–∫–∞–∫–æ–π", "which", "—Å–∫–æ–ª—å–∫–æ"
]

# –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è Victoria Agent (—Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è, —Å–µ—Ä–≤–µ—Ä–∞)
VICTORIA_PATTERNS = [
    "—Ñ–∞–π–ª –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ", "ssh", "–ø–æ–¥–∫–ª—é—á–∏—Å—å", "–∑–∞–ø—É—Å—Ç–∏ –Ω–∞", "–≤—ã–ø–æ–ª–Ω–∏ –∫–æ–º–∞–Ω–¥—É",
    "—Å–æ–∑–¥–∞–π –ø—Ä–æ–µ–∫—Ç", "—Ä–∞–∑–≤–µ—Ä–Ω–∏", "deploy", "docker", "–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä",
    "–∫–æ—Ä–ø–æ—Ä–∞—Ü", "—Å–µ—Ä–≤–µ—Ä", "—Å—Ç–∞—Ç—É—Å", "–ø—Ä–æ–≤–µ—Ä—å", "victoria", "–≤–∏–∫—Ç–æ—Ä–∏—è",
    "–∞–≥–µ–Ω—Ç", "–∑–∞–¥–∞—á", "mac studio", "–º–∞–∫—Å—Ç—É–¥–∏–æ", "mlx"
]


def is_simple_message(content: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç—ã–º (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç –∞–≥–µ–Ω—Ç–∞)"""
    lower = content.lower().strip()
    
    # –ï—Å–ª–∏ —è–≤–Ω–æ –Ω—É–∂–µ–Ω Victoria Agent
    for pattern in VICTORIA_PATTERNS:
        if pattern in lower:
            return False
    
    # –ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π - –ø—Ä–æ—Å—Ç—ã–µ (–±—ã—Å—Ç—Ä—ã–π –ø—É—Ç—å)
    if len(lower) < 200:
        return True
    
    # –î–ª–∏–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –ø—Ä–æ—Å—Ç—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ —Ç–æ–∂–µ –ø—Ä–æ—Å—Ç—ã–µ
    for pattern in SIMPLE_PATTERNS:
        if pattern in lower:
            return True
    
    return False


def _select_model_for_chat(content: str, expert_name: Optional[str] = None) -> str:
    """
    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è –∏ —ç–∫—Å–ø–µ—Ä—Ç–∞
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—Å–µ 8 –º–æ–¥–µ–ª–µ–π Mac Studio M4 Max:
    - complex/enterprise ‚Üí command-r-plus:104b (~65GB)
    - reasoning ‚Üí deepseek-r1-distill-llama:70b (~40GB)
    - complex ‚Üí llama3.3:70b (~40GB)
    - coding (high quality) ‚Üí qwen2.5-coder:32b (~20GB)
    - fast/general ‚Üí phi3.5:3.8b (~2.5GB)
    - fast (lightweight) ‚Üí phi3:mini-4k (~2GB)
    - fast/default ‚Üí qwen2.5:3b (~2GB)
    - fast (ultra-lightweight) ‚Üí phi3:mini-4k (~2.3GB) (tinyllama –∏—Å–∫–ª—é—á–µ–Ω–∞ - —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤)
    """
    content_lower = content.lower()
    
    # –°–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏, –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω—ã–µ, RAG, enterprise
    if any(word in content_lower for word in ["—Å–ª–æ–∂–Ω", "–∫–æ—Ä–ø–æ—Ä–∞—Ü", "rag", "enterprise", "–∫—Ä–∏—Ç–∏—á–Ω", "–≤–∞–∂–Ω", "—Å—Ç—Ä–∞—Ç–µ–≥"]):
        return "command-r-plus:104b"  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å
    
    # Reasoning, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ, –ª–æ–≥–∏–∫–∞, –∞–Ω–∞–ª–∏–∑
    if any(word in content_lower for word in ["–ø–æ–¥—É–º–∞–π", "–ª–æ–≥–∏–∫–∞", "–ø–ª–∞–Ω–∏—Ä", "reasoning", "–∞–Ω–∞–ª–∏–∑", "–æ–±—ä—è—Å–Ω–∏", "–ø–æ—á–µ–º—É"]):
        return "deepseek-r1-distill-llama:70b"  # Reasoning
    
    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ, —Å–ª–æ–∂–Ω—ã–µ –∑–∞–¥–∞—á–∏
    if any(word in content_lower for word in ["–∫–∞—á–µ—Å—Ç–≤", "–ª—É—á—à", "–æ–ø—Ç–∏–º–∞–ª—å–Ω", "–º–∞–∫—Å–∏–º–∞–ª—å–Ω", "–¥–µ—Ç–∞–ª—å–Ω"]):
        return "llama3.3:70b"  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
    
    # –ö–æ–¥, –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ, —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ (high quality)
    if any(word in content_lower for word in ["–∫–æ–¥", "–ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä", "—Ä–µ—Ñ–∞–∫—Ç–æ—Ä", "—Ñ—É–Ω–∫—Ü–∏", "–∫–ª–∞—Å—Å", "python", "javascript", "typescript", "–∞–ª–≥–æ—Ä–∏—Ç–º"]):
        return "qwen2.5-coder:32b"  # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥
    
    # –ë—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏, –æ–±—â–∏–µ (medium)
    if len(content) > 200 or any(word in content_lower for word in ["—Ä–∞—Å—Å–∫–∞–∂–∏", "–æ–±—ä—è—Å–Ω–∏", "–æ–ø–∏—Å–∞"]):
        return "phi3.5:3.8b"  # Fast, general
    
    # –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã, –ª–µ–≥–∫–∏–µ –∑–∞–¥–∞—á–∏ (lightweight)
    if len(content) < 200:
        return "phi3:mini-4k"  # Fast, lightweight
    
    # –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–µ (ultra-lightweight) - –∫–æ—Ä–æ—Ç–∫–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    if len(content) < 100:
        return "phi3.5:3.8b"  # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å (tinyllama –∏—Å–∫–ª—é—á–µ–Ω–∞ - —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤)
    
    # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é - –±—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
    return "qwen2.5:3b"  # Fast, default


async def _get_available_model(ideal_model: str, mlx: MLXClient) -> str:
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç fallback –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    
    Args:
        ideal_model: –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –∑–∞–¥–∞—á–∏
        mlx: MLXClient –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏
    
    Returns:
        –î–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å (–∏–¥–µ–∞–ª—å–Ω–∞—è –∏–ª–∏ fallback)
    """
    # –ú–∞–ø–ø–∏–Ω–≥ –∏–¥–µ–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –≤–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω –≤ MLX
    model_variants = {
        "command-r-plus:104b": ["command-r-plus", "command-r-plus:104b", "command-r", "c4ai-command-r-plus"],
        "deepseek-r1-distill-llama:70b": ["deepseek-r1-distill-llama", "deepseek-r1-distill", "deepseek-r1:70b", "deepseek-r1"],
        "llama3.3:70b": ["llama3.3", "llama3.3:70b", "llama-3.3", "llama"],
        "qwen2.5-coder:32b": ["qwen2.5-coder:32b", "qwen2.5-coder-32b", "qwen2.5-coder", "qwen-coder-32"],
        "phi3.5:3.8b": ["phi3.5", "phi3.5:3.8b", "phi-3.5", "phi3.5-mini"],
        "phi3:mini-4k": ["phi3:mini", "phi3-mini", "phi3:mini-4k", "phi-3-mini"],
        "qwen2.5:3b": ["qwen2.5:3b", "qwen2.5-3b", "qwen2.5", "qwen-3b"],
        # "tinyllama:1.1b-chat": ["tinyllama", "tinyllama:1.1b", "tinyllama-1.1b", "tiny-llama"]  # –ò—Å–∫–ª—é—á–µ–Ω–∞ - —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ –∞–≥–µ–Ω—Ç–æ–≤
    }
    
    # Fallback —Ü–µ–ø–æ—á–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–º–æ–¥–µ–ª–∏ Mac Studio)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –†–ï–ê–õ–¨–ù–û —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ Ollama
    # Ollama: qwq:32b, qwen2.5-coder:32b, glm-4.7-flash:q8_0, llava:7b, phi3.5:3.8b, moondream:latest, tinyllama:1.1b-chat
    fallback_chains = {
        "command-r-plus:104b": ["llama3.3:70b", "qwen2.5-coder:32b", "phi3.5:3.8b"],
        "deepseek-r1-distill-llama:70b": ["qwq:32b", "qwen2.5-coder:32b", "phi3.5:3.8b"],
        "llama3.3:70b": ["deepseek-r1-distill-llama:70b", "qwen2.5-coder:32b", "phi3.5:3.8b"],
        "qwen2.5-coder:32b": ["qwq:32b", "phi3.5:3.8b", "tinyllama:1.1b-chat"],
        "qwq:32b": ["qwen2.5-coder:32b", "glm-4.7-flash:q8_0", "phi3.5:3.8b"],
        "glm-4.7-flash:q8_0": ["qwq:32b", "qwen2.5-coder:32b", "phi3.5:3.8b"],
        "phi3.5:3.8b": ["qwen2.5-coder:32b", "tinyllama:1.1b-chat"],
        "phi3:mini-4k": ["phi3.5:3.8b", "tinyllama:1.1b-chat"],
        "qwen2.5:3b": ["phi3.5:3.8b", "tinyllama:1.1b-chat"],
    }
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ MLX health
        mlx_health = await mlx.health()
        available_models = mlx_health.get("available_models", [])
        models = [{"name": m} for m in available_models] if available_models else []
        available_names = [m.get("name", "") for m in models]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        if ideal_model in available_names:
            logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å: {ideal_model}")
            return ideal_model
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –∏–º–µ–Ω –¥–ª—è –∏–¥–µ–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
        variants = model_variants.get(ideal_model, [ideal_model])
        for variant in variants:
            # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–∞
            if variant in available_names:
                logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—Ä–∏–∞–Ω—Ç –º–æ–¥–µ–ª–∏: {variant} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                return variant
            # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
            for name in available_names:
                variant_base = variant.split(":")[0].split("-")[0]
                name_base = name.split(":")[0].split("-")[0]
                if variant_base in name_base or name_base in variant_base:
                    if len(variant_base) > 3:  # –ò–∑–±–µ–≥–∞–µ–º —Å–ª–∏—à–∫–æ–º –æ–±—â–∏—Ö —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π
                        logger.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ö–æ–∂—É—é –º–æ–¥–µ–ª—å: {name} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                        return name
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback —Ü–µ–ø–æ—á–∫—É
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ Ollama
        fallbacks = fallback_chains.get(ideal_model, ["qwen2.5-coder:32b", "phi3.5:3.8b", "tinyllama:1.1b-chat"])
        for fallback in fallbacks:
            if fallback in available_names:
                logger.info(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º fallback: {fallback} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                return fallback
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–ª—è fallback
            for name in available_names:
                fallback_base = fallback.split(":")[0].split("-")[0]
                name_base = name.split(":")[0].split("-")[0]
                if fallback_base in name_base or name_base in fallback_base:
                    if len(fallback_base) > 3:
                        logger.info(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Ö–æ–∂–∏–π fallback: {name} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                        return name
        
        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ä–µ–∑–µ—Ä–≤ - –ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å (–∫—Ä–æ–º–µ embeddings –∏ vision)
        for name in available_names:
            if "embed" not in name.lower() and "dream" not in name.lower():
                logger.warning(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å: {name} (–≤–º–µ—Å—Ç–æ {ideal_model})")
                return name
        
        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–ø—É—Å—Ç—å MLX –≤–µ—Ä–Ω–µ—Ç –æ—à–∏–±–∫—É)
        logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º {ideal_model}")
        return ideal_model
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π: {e}")
        # –í —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–¥–µ–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å
        return ideal_model


async def _generate_via_mlx_or_ollama(
    full_prompt: str,
    ideal_model: str,
    mlx: MLXClient,
    ollama: OllamaClient,
    system: str = "–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
) -> tuple:
    """
    –¶–µ–ø–æ—á–∫–∞ –≤—ã–±–æ—Ä–∞: MLX ‚Üí Ollama ‚Üí (None = –ø–µ—Ä–µ—Ö–æ–¥ –Ω–∞ Victoria).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (content, source) –∏–ª–∏ (None, None).
    """
    # 1) MLX
    mlx_health = await mlx.health()
    if mlx_health.get("status") in ("healthy", "degraded"):
        try:
            result = await mlx.generate(
                prompt=full_prompt,
                system=system,
                max_tokens=512,
                model=ideal_model,
            )
            if result and isinstance(result, dict) and result.get("response"):
                return (result["response"].strip(), "mlx")
        except Exception as e:
            logger.debug(f"MLX generate failed: {e}")
    # 2) Ollama
    ollama_health = await ollama.health()
    if ollama_health.get("status") == "healthy":
        try:
            result = await ollama.generate(
                prompt=full_prompt,
                model=ideal_model,
                system=system,
                stream=False,
            )
            if result and isinstance(result, dict):
                text = result.get("response") or result.get("message", {}).get("content") or ""
                if isinstance(text, list):
                    text = "".join(c.get("text", "") for c in text if isinstance(c, dict))
                if text and "error" not in result:
                    return (text.strip(), "ollama")
        except Exception as e:
            logger.debug(f"Ollama generate failed: {e}")
    return (None, None)


class ChatResponse(BaseModel):
    """–û—Ç–≤–µ—Ç –æ—Ç —á–∞—Ç–∞"""
    content: str
    expert_name: Optional[str] = None
    model: Optional[str] = None
    tokens_used: Optional[int] = None


class PlanRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –ø–ª–∞–Ω–∞ —É –í–∏–∫—Ç–æ—Ä–∏–∏"""
    goal: str = Field(..., min_length=1, max_length=10000)


class PlanResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å –ø–ª–∞–Ω–æ–º"""
    plan: str
    status: str = "success"


async def sse_generator(
    message: ChatMessage,
    victoria: VictoriaClient,
    mlx: MLXClient,
    ollama: OllamaClient,
    knowledge_os: KnowledgeOSClient,
    plan_cache: PlanCacheService = None,
) -> AsyncGenerator[str, None]:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä SSE —Å–æ–±—ã—Ç–∏–π (Singularity 9.0)
    
    Yields:
        SSE —Å–æ–±—ã—Ç–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ: data: {...}\n\n
    """
    # Singularity v9.0: –î–µ—Ç–µ–∫—Ü–∏—è —ç–º–æ—Ü–∏–π
    emotion, confidence = detect_emotion(message.content)
    emotion_data = {
        'emotion': emotion.value,
        'confidence': round(confidence, 2)
    }
    
    # Singularity v5.0: Streaming processor
    processor = StreamingProcessor(buffer_size=3, min_delay=0.03, max_delay=0.1)
    
    def _flush_sse():
        """SSE comment ‚Äî –∑–∞—Å—Ç–∞–≤–ª—è–µ—Ç –ø—Ä–æ–∫—Å–∏/—Å–µ—Ä–≤–µ—Ä –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –±—É—Ñ–µ—Ä –∫–ª–∏–µ–Ω—Ç—É."""
        return ": \n\n"

    # Correlation ID –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏ (—á–∞—Ç ‚Üí Victoria ‚Üí Veronica). ARCHITECTURE_IMPROVEMENTS_ANALYSIS.
    correlation_id = str(uuid.uuid4())
    logger.info("[CHAT] correlation_id=%s goal_preview=%s", correlation_id[:8], (message.content or "")[:50])

    # –§–∞–∑–∞ 4, –ù–µ–¥–µ–ª—è 2: –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞ (multi-turn)
    session_id = getattr(message, "session_id", None) or getattr(message, "user_id", None)
    context_prefix = ""
    recent_messages: list = []
    if session_id:
        settings_ctx = get_settings()
        if getattr(settings_ctx, "conversation_context_enabled", True):
            ctx_mgr = get_conversation_context_manager()
            recent_messages = await ctx_mgr.get_recent(session_id, last_n=10)
            context_prefix = ctx_mgr.build_context_prefix(recent_messages)
    response_parts = []
    current_prompt = (context_prefix + message.content) if context_prefix else message.content

    async def _save_context_if_needed():
        if not session_id:
            return
        full_response = "".join(response_parts).strip()
        if not full_response:
            return
        try:
            ctx_mgr = get_conversation_context_manager()
            await ctx_mgr.append(session_id, "user", message.content)
            await ctx_mgr.append(session_id, "assistant", full_response)
        except Exception as e:
            logger.debug("Conversation context save failed: %s", e)

    try:
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞—á–∞–ª–æ —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ–± —ç–º–æ—Ü–∏–∏
        start_event = {
            'type': 'start',
            'expert': message.expert_name,
            'emotion': emotion_data
        }
        yield f"data: {json.dumps(start_event)}\n\n"
        yield _flush_sse()

        # –†–µ–∂–∏–º –∞–≥–µ–Ω—Ç–∞: —à–∞–≥–∏ (–º—ã—Å–ª–∏, –¥–µ–π—Å—Ç–≤–∏—è) –∫–∞–∫ –≤ Cursor ‚Äî –æ—Ç–ø—Ä–∞–≤–ª—è—é—Ç—Å—è –¥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        use_victoria = getattr(message, 'use_victoria', True)
        mode = (getattr(message, 'mode', None) or "agent").lower()
        logger.info(f"[SSE] use_victoria={use_victoria}, mode={mode}")

        # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ Victoria (agent/plan). –ü—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ‚Äî Agent fallback –Ω–∞ MLX/Ollama (–∫–∞–∫ –ø—Ä–æ—Å—Ç—ã–µ)
        victoria_available = True
        if use_victoria and mode in ("agent", "plan"):
            try:
                vh = await asyncio.wait_for(victoria.health(), timeout=5.0)
                if vh.get("status") not in ("healthy", "ok"):
                    victoria_available = False
            except (asyncio.TimeoutError, Exception) as e:
                logger.warning("Victoria health check failed: %s, fallback –Ω–∞ MLX/Ollama", e)
                victoria_available = False
            if not victoria_available and mode == "plan":
                # Plan —Ç—Ä–µ–±—É–µ—Ç Victoria, fallback –Ω–µ –ø—Ä–∏–º–µ–Ω–∏–º
                tip = "–ó–∞–ø—É—Å—Ç–∏—Ç–µ: docker-compose -f knowledge_os/docker-compose.yml up -d victoria-agent"
                yield f"data: {json.dumps({'type': 'error', 'content': f'Victoria –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. {tip}'})}\n\n"
                yield f"data: {json.dumps({'type': 'end'})}\n\n"
                return

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ Victoria (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
        if use_victoria:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Victoria —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –ø—Ä–æ–µ–∫—Ç–∞
            # Victoria —Å–∞–º–∞ –ø–æ–¥–∫–ª—é—á–∞–µ—Ç—Å—è –∫ Knowledge OS —á–µ—Ä–µ–∑ asyncpg connection pool
            project_context = os.getenv("PROJECT_NAME", "atra-web-ide")

            if mode == "plan":
                # –†–µ–∂–∏–º ¬´–¢–æ–ª—å–∫–æ –ø–ª–∞–Ω¬ª: –∫—ç—à (–§–∞–∑–∞ 3) –∏–ª–∏ –≤—ã–∑–æ–≤ Victoria
                settings = get_settings()
                cache = plan_cache or get_plan_cache_service()
                if getattr(settings, "plan_cache_enabled", True) and cache._maxsize > 0:
                    cached = await cache.get(current_prompt, project_context)
                    if cached:
                        plan_text = cached.get("result") or cached.get("response") or ""
                        if plan_text:
                            logger.info("[Plan] cache hit: '%s...'", (message.content or "")[:40])
                            yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ü–ª–∞–Ω –∏–∑ –∫—ç—à–∞', 'content': '–ò—Å–ø–æ–ª—å–∑—É—é —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–π –ø–ª–∞–Ω.'})}\n\n"
                            yield _flush_sse()
                            await asyncio.sleep(0.02)
                            for line in plan_text.split("\n"):
                                if line.strip():
                                    response_parts.append(line + chr(10))
                                    yield f"data: {json.dumps({'type': 'chunk', 'content': line + chr(10)})}\n\n"
                                    await asyncio.sleep(0.01)
                            await _save_context_if_needed()
                            yield f"data: {json.dumps({'type': 'end'})}\n\n"
                            return
                yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–°–æ—Å—Ç–∞–≤–ª—è—é –ø–ª–∞–Ω', 'content': '–ó–∞–ø—Ä–∞—à–∏–≤–∞—é –ø–ª–∞–Ω —É –í–∏–∫—Ç–æ—Ä–∏–∏ (–±–µ–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è).'})}\n\n"
                yield _flush_sse()
                await asyncio.sleep(0.05)
                try:
                    import time
                    t0 = time.perf_counter()
                    plan_result = await victoria.plan(goal=current_prompt, project_context=project_context)
                    gen_time = time.perf_counter() - t0
                    plan_text = (plan_result.get("result") or plan_result.get("response") or "") if plan_result.get("status") != "error" else ""
                    if not plan_text:
                        plan_text = plan_result.get("error", "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–ª–∞–Ω.")
                    for line in plan_text.split("\n"):
                        if line.strip():
                            response_parts.append(line + chr(10))
                            yield f"data: {json.dumps({'type': 'chunk', 'content': line + chr(10)})}\n\n"
                            await asyncio.sleep(0.02)
                    await _save_context_if_needed()
                    yield f"data: {json.dumps({'type': 'end'})}\n\n"
                    min_gen = getattr(settings, "plan_cache_min_gen_time", 2.0)
                    if plan_result.get("status") != "error" and plan_text and gen_time >= min_gen and cache._maxsize > 0:
                        await cache.set(current_prompt, plan_result, project_context, ttl=getattr(settings, "plan_cache_ttl", 3600))
                        logger.info("[Plan] saved to cache: '%s...' (gen_time=%.1fs)", (message.content or "")[:40], gen_time)
                    return
                except Exception as e:
                    logger.error(f"Plan mode error: {e}", exc_info=True)
                    yield f"data: {json.dumps({'type': 'error', 'content': str(e)})}\n\n"
                    yield f"data: {json.dumps({'type': 'end'})}\n\n"
                    return

            # –®–∞–≥: –º—ã—Å–ª—å (–∞–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞). correlation_id –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏.
            # Progress event (ARCHITECTURE_IMPROVEMENTS ¬ß2.1): { step, total, status } –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
            yield f"data: {json.dumps({'type': 'progress', 'step': 1, 'total': 4, 'status': 'analysis'})}\n\n"
            yield f"data: {json.dumps({'type': 'step', 'stepType': 'thought', 'title': '–ê–Ω–∞–ª–∏–∑ –∑–∞–ø—Ä–æ—Å–∞', 'content': '–ü—Ä–æ–≤–µ—Ä—è—é –∑–∞–ø—Ä–æ—Å, –ø–æ–¥–±–∏—Ä–∞—é —ç–∫—Å–ø–µ—Ä—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π (RAG).', 'correlation_id': correlation_id})}\n\n"
            yield _flush_sse()
            await asyncio.sleep(0.05)

            # –ï—Å–ª–∏ —É–∫–∞–∑–∞–Ω expert_name, –ø—Ä–æ–≤–µ—Ä—è–µ–º –µ–≥–æ –≤ Knowledge OS Database
            expert_data = None
            if message.expert_name:
                try:
                    expert_data = await knowledge_os.get_expert_by_name(message.expert_name)
                    if expert_data:
                        logger.info(f"‚úÖ –≠–∫—Å–ø–µ—Ä—Ç '{message.expert_name}' –Ω–∞–π–¥–µ–Ω –≤ Knowledge OS: {expert_data.get('role')}")
                        exploration_content = f"–≠–∫—Å–ø–µ—Ä—Ç: {message.expert_name} ({expert_data.get('role', '')})"
                        yield f"data: {json.dumps({'type': 'step', 'stepType': 'exploration', 'title': '–≠–∫—Å–ø–µ—Ä—Ç –Ω–∞–π–¥–µ–Ω', 'content': exploration_content})}\n\n"
                        yield _flush_sse()
                        await asyncio.sleep(0.05)
                except Exception as e:
                    logger.debug(f"–≠–∫—Å–ø–µ—Ä—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ Knowledge OS: {e}")

            # –®–∞–≥: –¥–µ–π—Å—Ç–≤–∏–µ (–∑–∞–ø—Ä–æ—Å –∫ Victoria)
            yield f"data: {json.dumps({'type': 'progress', 'step': 2, 'total': 4, 'status': 'executing'})}\n\n"
            yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ó–∞–ø—Ä–æ—Å –∫ Victoria Agent', 'content': '–§–æ—Ä–º–∏—Ä—É—é –ø–ª–∞–Ω –∏ –∑–∞–ø—Ä–∞—à–∏–≤–∞—é –æ—Ç–≤–µ—Ç —É –∞–≥–µ–Ω—Ç–∞.'})}\n\n"
            yield _flush_sse()
            await asyncio.sleep(0.05)

            # –†–µ–∂–∏–º Ask: –≥–æ—Ä—è—á–∏–π –ø—É—Ç—å (—à–∞–±–ª–æ–Ω—ã) ‚Üí MLX ‚Üí Ollama ‚Üí Victoria
            if mode == "ask":
                # –ì–æ—Ä—è—á–∏–π –ø—É—Ç—å: –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç—å) ‚Äî —à–∞–±–ª–æ–Ω –±–µ–∑ LLM
                classification = classify_query(message.content)
                if classification.get("type") == "simple":
                    template = get_template_response(message.content, message.expert_name)
                    if template:
                        query_preview = (message.content or "")[:30].replace("\n", " ")
                        logger.info("[Ask] Hot path: simple query '%s' -> template (no LLM)", query_preview)
                        yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç', 'content': '–®–∞–±–ª–æ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç (–±–µ–∑ –≤—ã–∑–æ–≤–∞ LLM).'})}\n\n"
                        yield _flush_sse()
                        await asyncio.sleep(0.02)
                        for word in template.split():
                            response_parts.append(word + " ")
                            yield f"data: {json.dumps({'type': 'chunk', 'content': word + ' '})}\n\n"
                            await asyncio.sleep(0.01)
                        await _save_context_if_needed()
                        yield f"data: {json.dumps({'type': 'end'})}\n\n"
                        return
                # RAG-light –¥–ª—è —Ñ–∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–§–∞–∑–∞ 2)
                if classification.get("type") == "factual":
                    settings = get_settings()
                    if getattr(settings, "rag_light_enabled", True):
                        rag_light = get_rag_light_service(knowledge_os)
                        if rag_light.enabled:
                            yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç', 'content': '–ò—â—É –æ—Ç–≤–µ—Ç –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π (RAG-light)...'})}\n\n"
                            yield _flush_sse()
                            await asyncio.sleep(0.02)
                            try:
                                fast_answer = await rag_light.fast_fact_answer(
                                    message.content,
                                    timeout_ms=getattr(settings, "rag_light_timeout_ms", 200),
                                    user_id=getattr(message, "user_id", None),
                                )
                                if fast_answer:
                                    yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ù–∞–π–¥–µ–Ω–æ –≤ –ë–ó', 'content': '–û—Ç–≤–µ—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.'})}\n\n"
                                    yield _flush_sse()
                                    await asyncio.sleep(0.02)
                                    for word in fast_answer.split():
                                        response_parts.append(word + " ")
                                        yield f"data: {json.dumps({'type': 'chunk', 'content': word + ' '})}\n\n"
                                        await asyncio.sleep(0.01)
                                    await _save_context_if_needed()
                                    yield f"data: {json.dumps({'type': 'end'})}\n\n"
                                    logger.info("[Ask] RAG-light path: factual query -> answer from KB")
                                    return
                            except Exception as e:
                                logger.debug("RAG-light failed, falling back to MLX/Ollama: %s", e)
                # –ü–æ–¥—Å–∫–∞–∑–∫–∞ –ø–µ—Ä–µ–π—Ç–∏ –≤ –ê–≥–µ–Ω—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ (–§–∞–∑–∞ 2, –¥–µ–Ω—å 3‚Äì4)
                settings = get_settings()
                if getattr(settings, "agent_suggestion_enabled", True):
                    enhanced = analyze_complexity(message.content)
                    if enhanced.get("suggest_agent") and enhanced.get("complexity_score", 0) >= getattr(settings, "agent_suggestion_threshold", 0.6):
                        suggestion_text = (
                            "–≠—Ç–æ—Ç –∑–∞–ø—Ä–æ—Å —Ç—Ä–µ–±—É–µ—Ç –≥–ª—É–±–æ–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. "
                            "–î–ª—è –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–µ–∂–∏–º ¬´–ê–≥–µ–Ω—Ç¬ª. –ü—Ä–æ–¥–æ–ª–∂–∞—é –≤ —Ç–µ–∫—É—â–µ–º —Ä–µ–∂–∏–º–µ‚Ä¶"
                        )
                        yield f"data: {json.dumps({'type': 'step', 'stepType': 'thought', 'title': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è', 'content': suggestion_text})}\n\n"
                        yield _flush_sse()
                        delay_ms = getattr(settings, "agent_suggestion_delay_ms", 500)
                        await asyncio.sleep(delay_ms / 1000.0)
                        logger.info(
                            "[Ask] Agent suggestion shown: query='%s...', score=%.2f",
                            (message.content or "")[:30],
                            enhanced.get("complexity_score", 0),
                        )
                        try:
                            agent_suggestion_metrics.add_suggestion(
                                AgentSuggestionMetric(
                                    query=(message.content or "")[:500],
                                    suggested=True,
                                    complexity_score=enhanced.get("complexity_score", 0),
                                    reason=enhanced.get("complexity_reason", ""),
                                    user_action="unknown",
                                )
                            )
                        except Exception:
                            pass
                # –û–±—ã—á–Ω—ã–π –ø—É—Ç—å: MLX ‚Üí Ollama ‚Üí Victoria
                yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ë—ã—Å—Ç—Ä—ã–π –æ—Ç–≤–µ—Ç', 'content': '–ü—Ä–æ–≤–µ—Ä—è—é MLX –∏ Ollama, –ø—Ä–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ ‚Äî Victoria.'})}\n\n"
                yield _flush_sse()
                await asyncio.sleep(0.05)
                expert_prompt = f"–¢—ã - {message.expert_name}, —ç–∫—Å–ø–µ—Ä—Ç ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ.\n\n" if message.expert_name else ""
                full_prompt = expert_prompt + current_prompt
                ideal_model = message.model or _select_model_for_chat(message.content, message.expert_name)
                content, source = await _generate_via_mlx_or_ollama(
                    full_prompt, ideal_model, mlx, ollama,
                    system="–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
                )
                if content:
                    content = _truncate_repeated_response(content)
                    words = content.split()
                    chunk = ""
                    for i, word in enumerate(words):
                        chunk += word + " "
                        if i % 3 == 0 and chunk:
                            response_parts.append(chunk)
                            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                            chunk = ""
                            await asyncio.sleep(0.02)
                    if chunk:
                        response_parts.append(chunk)
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                    await _save_context_if_needed()
                    yield f"data: {json.dumps({'type': 'end'})}\n\n"
                    asyncio.create_task(_log_chat_to_knowledge_os(message.content, content, message.expert_name))
                    return
                # MLX –∏ Ollama –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∏ (—Ç–∞–π–º–∞—É—Ç, –æ—à–∏–±–∫–∞ –∏–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–Ω—è—Ç–∞) ‚Äî –æ—Ç–≤–µ—á–∞–µ–º —á–µ—Ä–µ–∑ Victoria
                yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç', 'content': 'MLX –∏ Ollama –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∏ –≤–æ–≤—Ä–µ–º—è ‚Äî –æ—Ç–≤–µ—á–∞—é —á–µ—Ä–µ–∑ Victoria.'})}\n\n"
                yield _flush_sse()
                await asyncio.sleep(0.05)
                logger.info("[Ask] MLX/Ollama –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∏, fallback –Ω–∞ Victoria")

            # üèõ BOARD OF DIRECTORS CONSULT: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
            board_decision_text = None
            correlation_id = None
            
            try:
                from app.services.strategic_classifier import is_strategic_question
                
                is_strategic, reason = is_strategic_question(message.content)
                logger.info(f"[STRATEGIC_CLASSIFIER] is_strategic={is_strategic}, reason={reason}, question='{message.content[:100]}...'")
                
                if is_strategic:
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º correlation_id –¥–ª—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∏
                    import uuid
                    correlation_id = str(uuid.uuid4())
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, —á—Ç–æ –∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É–µ–º—Å—è —Å –°–æ–≤–µ—Ç–æ–º
                    yield f"data: {json.dumps({'type': 'step', 'stepType': 'thought', 'title': '–ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤', 'content': '–≠—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏–π. –ö–æ–Ω—Å—É–ª—å—Ç–∏—Ä—É—é—Å—å —Å –°–æ–≤–µ—Ç–æ–º –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è...'})}\n\n"
                    yield _flush_sse()
                    await asyncio.sleep(0.1)
                    
                    # –í—ã–∑–æ–≤ Knowledge OS API: POST /api/board/consult
                    settings_board = get_settings()
                    board_api_url = f"{settings_board.knowledge_os_api_url.rstrip('/')}/api/board/consult"
                    api_key = os.getenv('API_KEY', 'your-secret-api-key')
                    
                    try:
                        async with httpx.AsyncClient(timeout=45.0) as client:
                            board_response = await client.post(
                                board_api_url,
                                json={
                                    "question": message.content,
                                    "session_id": session_id,
                                    "user_id": getattr(message, 'user_id', None),
                                    "correlation_id": correlation_id,
                                    "source": "chat",
                                },
                                headers={"X-API-Key": api_key}
                            )
                            board_response.raise_for_status()
                            board_result = board_response.json()
                            
                            board_decision_text = board_result.get("directive_text", "")
                            structured_decision = board_result.get("structured_decision", {})
                            risk_level = board_result.get("risk_level")
                            recommend_review = board_result.get("recommend_human_review", False)
                            
                            logger.info(f"[BOARD_CONSULT] success correlation_id={correlation_id} decision='{structured_decision.get('decision', '')[:80]}...' risk={risk_level}")
                            
                            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                            decision_summary = structured_decision.get("decision", board_decision_text[:150])
                            board_step_content = f"–°–æ–≤–µ—Ç –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –ø—Ä–∏–Ω—è–ª —Ä–µ—à–µ–Ω–∏–µ:\n\n{decision_summary}"
                            if recommend_review:
                                board_step_content += "\n\n‚ö†Ô∏è –°–æ–≤–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —Ä–µ—à–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–æ–º (–≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫ –∏–ª–∏ –Ω–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)."
                            
                            yield f"data: {json.dumps({'type': 'step', 'stepType': 'observation', 'title': '–†–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞', 'content': board_step_content})}\n\n"
                            yield _flush_sse()
                            await asyncio.sleep(0.1)
                            
                    except httpx.HTTPError as e:
                        logger.error(f"[BOARD_CONSULT] HTTP error: {e}")
                        yield f"data: {json.dumps({'type': 'step', 'stepType': 'error', 'title': '–û—à–∏–±–∫–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –°–æ–≤–µ—Ç–∞', 'content': f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ä–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞: {str(e)}. –ü—Ä–æ–¥–æ–ª–∂–∞—é —Å Victoria...'})}\n\n"
                        yield _flush_sse()
                        board_decision_text = None
                    except Exception as e:
                        logger.error(f"[BOARD_CONSULT] error: {e}", exc_info=True)
                        yield f"data: {json.dumps({'type': 'step', 'stepType': 'error', 'title': '–û—à–∏–±–∫–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –°–æ–≤–µ—Ç–∞', 'content': f'–û—à–∏–±–∫–∞ –ø—Ä–∏ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏ –°–æ–≤–µ—Ç–∞: {str(e)}. –ü—Ä–æ–¥–æ–ª–∂–∞—é —Å Victoria...'})}\n\n"
                        yield _flush_sse()
                        board_decision_text = None
            except ImportError:
                logger.warning("[STRATEGIC_CLASSIFIER] strategic_classifier module not found, skipping board consult")
            except Exception as e:
                logger.error(f"[STRATEGIC_CLASSIFIER] unexpected error: {e}", exc_info=True)
            
            # –ï—Å–ª–∏ –ø–æ–ª—É—á–µ–Ω–æ —Ä–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞, —Ñ–æ—Ä–º–∏—Ä—É–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è Victoria
            if board_decision_text:
                board_prompt_block = f"""
[–†–ï–®–ï–ù–ò–ï –°–û–í–ï–¢–ê –î–ò–†–ï–ö–¢–û–†–û–í]
{board_decision_text}
[/–†–ï–®–ï–ù–ò–ï]

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {message.content}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –≤—ã—à–µ. 
–ú–æ–∂–µ—à—å –Ω–∞—á–∞—Ç—å —Å —Ñ—Ä–∞–∑—ã "–ü–æ —Ä–µ—à–µ–Ω–∏—é –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤..." –∏ –¥–∞–ª–µ–µ —Ä–∞–∑–≤–∏—Ç—å –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º —Ä–µ—à–µ–Ω–∏—è.
–ï—Å–ª–∏ —Ä–µ—à–µ–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —á–µ–ª–æ–≤–µ–∫–æ–º, –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–ø–æ–º—è–Ω–∏ —ç—Ç–æ –≤ –∫–æ–Ω—Ü–µ.
"""
                current_prompt = board_prompt_block
            
            # –í—ã–∑—ã–≤–∞–µ–º Victoria —Å —Ç–∞–π–º–∞—É—Ç–æ–º (—Ç—è–∂—ë–ª—ã–µ –º–æ–¥–µ–ª–∏: –ø—Ä–æ–≥—Ä–µ–≤ + –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏)
            # session_id –∏ chat_history –¥–ª—è Victoria (–∫–æ–Ω—Ç—Ä–∞–∫—Ç POST /run, —Å–≤—è–∑–Ω—ã–π –¥–∏–∞–ª–æ–≥)
            chat_history_vic = []
            if session_id and recent_messages:
                ctx_mgr = get_conversation_context_manager()
                chat_history_vic = ctx_mgr.to_victoria_chat_history(recent_messages)
            settings = get_settings()
            try:
                result = await asyncio.wait_for(
                    victoria.run(
                        prompt=current_prompt,
                        expert_name=message.expert_name,
                        project_context=project_context,
                        session_id=session_id,
                        chat_history=chat_history_vic if chat_history_vic else None,
                        correlation_id=correlation_id,
                    ),
                    timeout=settings.victoria_timeout  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 600 —Å–µ–∫
                )
            except asyncio.TimeoutError:
                logger.error(f"Victoria timeout –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ (limit {settings.victoria_timeout}s): {message.content[:50]}")
                yield f"data: {json.dumps({'type': 'progress', 'step': 4, 'total': 4, 'status': 'error'})}\n\n"
                error_event = {
                    'type': 'error',
                    'content': '–ü—Ä–µ–≤—ã—à–µ–Ω–æ –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç Victoria. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å.'
                }
                yield f"data: {json.dumps(error_event)}\n\n"
                yield f"data: {json.dumps({'type': 'end'})}\n\n"
                return
            except Exception as e:
                logger.error(f"Victoria error: {e}", exc_info=True)
                result = {"error": str(e)}
            
            # –£—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –æ—Ç Victoria ‚Äî –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –≤ —á–∞—Ç–µ –∫–∞–∫ —à–∞–≥ –∏ —Ç–µ–∫—Å—Ç
            if result.get("status") == "needs_clarification":
                questions = result.get("clarification_questions") or result.get("raw", {}).get("clarification_questions") or []
                text = "–ù—É–∂–Ω–æ —É—Ç–æ—á–Ω–µ–Ω–∏–µ:\n\n" + "\n".join(f"‚Ä¢ {q}" for q in questions) if questions else "–í–∏–∫—Ç–æ—Ä–∏—è –ø—Ä–æ—Å–∏—Ç —É—Ç–æ—á–Ω–∏—Ç—å –∑–∞–¥–∞—á—É."
                yield f"data: {json.dumps({'type': 'step', 'stepType': 'clarification', 'title': '–£—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã', 'content': text})}\n\n"
                yield _flush_sse()
                await asyncio.sleep(0.05)
                for line in text.split("\n"):
                    if line.strip():
                        response_parts.append(line + chr(10))
                        yield f"data: {json.dumps({'type': 'chunk', 'content': line + chr(10)})}\n\n"
                        await asyncio.sleep(0.02)
                await _save_context_if_needed()
                yield f"data: {json.dumps({'type': 'progress', 'step': 4, 'total': 4, 'status': 'complete'})}\n\n"
                yield f"data: {json.dumps({'type': 'end'})}\n\n"
                return

            if "error" in result:
                # Fallback: MLX ‚Üí Ollama (Victoria —É–∂–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)
                yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ó–∞–ø–∞—Å–Ω–æ–π –≤–∞—Ä–∏–∞–Ω—Ç', 'content': 'Victoria –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä—è—é MLX –∏ Ollama.'})}\n\n"
                yield _flush_sse()
                await asyncio.sleep(0.05)
                logger.warning("Victoria –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º MLX ‚Üí Ollama –∫–∞–∫ fallback")

                expert_prompt = ""
                if message.expert_name:
                    expert_prompt = f"–¢—ã - {message.expert_name}, —ç–∫—Å–ø–µ—Ä—Ç ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n\n"
                full_prompt = expert_prompt + current_prompt
                ideal_model = message.model or _select_model_for_chat(message.content, message.expert_name)
                content, source = await _generate_via_mlx_or_ollama(
                    full_prompt, ideal_model, mlx, ollama,
                    system="–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
                )

                if not content:
                    fallback_response = (
                        f"–ü—Ä–∏–≤–µ—Ç! –Ø {message.expert_name or '–í–∏–∫—Ç–æ—Ä–∏—è'}. "
                        "–°–µ–π—á–∞—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã Victoria, MLX –∏ Ollama. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–ª–∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
                    )
                    words = fallback_response.split()
                    buffer = ""
                    for i, word in enumerate(words):
                        buffer += word + " "
                        if i % 5 == 0:
                            response_parts.append(buffer)
                            yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
                            buffer = ""
                    if buffer:
                        response_parts.append(buffer)
                        yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
                    await _save_context_if_needed()
                    yield f"data: {json.dumps({'type': 'end'})}\n\n"
                    return
                result = {"response": content, "source": source}
            else:
                # Victoria —É—Å–ø–µ—à–Ω–æ –æ—Ç–≤–µ—Ç–∏–ª–∞
                yield f"data: {json.dumps({'type': 'progress', 'step': 4, 'total': 4, 'status': 'complete'})}\n\n"
                yield f"data: {json.dumps({'type': 'step', 'stepType': 'action', 'title': '–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞', 'content': '–§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç.'})}\n\n"
                yield _flush_sse()
                await asyncio.sleep(0.05)
                content = result.get("result", "") or result.get("response", "") or result.get("output", "")
                if not content:
                    logger.warning(f"Victoria –≤–µ—Ä–Ω—É–ª–∞ –ø—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç: {result}")
                    content = f"–ü—Ä–∏–≤–µ—Ç! –Ø {message.expert_name or '–í–∏–∫—Ç–æ—Ä–∏—è'}. –ü–æ–ª—É—á–∏–ª–∞ –≤–∞—à –∑–∞–ø—Ä–æ—Å, –Ω–æ –æ—Ç–≤–µ—Ç –ø–æ–∫–∞ –ø—É—Å—Ç–æ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."
                else:
                    # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é..." –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å –≤ –æ—Ç–≤–µ—Ç–µ
                    if "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é" in content:
                        # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ—Å–ª–µ "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é..."
                        parts = content.split("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é")
                        if len(parts) > 1:
                            content = parts[-1].strip()
                            if not content:
                                content = "–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –≤–∞—à –∑–∞–ø—Ä–æ—Å..."
                    content = _truncate_repeated_response(content)
                
                # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞ –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                words = content.split()
                buffer = ""
                for i, word in enumerate(words):
                    buffer += word + " "
                    if i % 5 == 0:  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∫–∞–∂–¥—ã–µ 5 —Å–ª–æ–≤
                        response_parts.append(buffer)
                        yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
                        buffer = ""
                        await asyncio.sleep(0.05)  # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
                
                if buffer:
                    response_parts.append(buffer)
                    yield f"data: {json.dumps({'type': 'chunk', 'content': buffer})}\n\n"
                
                # –õ–æ–≥–∏—Ä—É–µ–º –≤ interaction_logs (Singularity 9.0)
                asyncio.create_task(_log_chat_to_knowledge_os(message.content, content, message.expert_name))
                await _save_context_if_needed()
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è (progress —É–∂–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤—ã—à–µ)
                yield f"data: {json.dumps({'type': 'end'})}\n\n"
                return
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –æ—Ç MLX fallback (–µ—Å–ª–∏ Victoria –±—ã–ª–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞)
            if result and isinstance(result, dict) and result.get("response"):
                content = result.get("response", "")
                content = _truncate_repeated_response(content)
                source = result.get("source", "unknown")
                model_used = result.get("model", "unknown")
                logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç {source} (–º–æ–¥–µ–ª—å: {model_used}) —á–µ—Ä–µ–∑ fallback")
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ —Å–ª–æ–≤–∞–º –¥–ª—è –ø–ª–∞–≤–Ω–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                words = content.split()
                chunk = ""
                for i, word in enumerate(words):
                    chunk += word + " "
                    if i % 3 == 0 and chunk:  # –ö–∞–∂–¥—ã–µ 3 —Å–ª–æ–≤–∞
                        response_parts.append(chunk)
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                        chunk = ""
                        await asyncio.sleep(0.05)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
                if chunk:
                    response_parts.append(chunk)
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                
                # –õ–æ–≥–∏—Ä—É–µ–º –≤ interaction_logs (Singularity 9.0)
                asyncio.create_task(_log_chat_to_knowledge_os(message.content, content, message.expert_name))
                await _save_context_if_needed()
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ–±—ã—Ç–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                yield f"data: {json.dumps({'type': 'end'})}\n\n"
                return
        else:
            # –ï—Å–ª–∏ use_victoria=False: MLX ‚Üí Ollama (–±–µ–∑ Victoria)
            expert_prompt = ""
            if message.expert_name:
                expert_prompt = f"–¢—ã - {message.expert_name}, —ç–∫—Å–ø–µ—Ä—Ç ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n\n"
            full_prompt = expert_prompt + current_prompt
            ideal_model = message.model or _select_model_for_chat(message.content, message.expert_name)
            logger.info(f"üéØ –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è '{message.content[:50]}...': {ideal_model}")

            content, source = await _generate_via_mlx_or_ollama(
                full_prompt, ideal_model, mlx, ollama,
                system="–¢—ã - –ø–æ–ª–µ–∑–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ATRA. –û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º.",
            )

            if not content:
                logger.warning("MLX –∏ Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã (use_victoria=False)")
                expert_name = message.expert_name or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
                fallback_response = (
                    f"–ü—Ä–∏–≤–µ—Ç! –Ø {expert_name}. –°–µ–π—á–∞—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã MLX –∏ Ollama. "
                    "–ó–∞–ø—É—Å—Ç–∏—Ç–µ –æ–¥–∏–Ω –∏–∑ —Å–µ—Ä–≤–µ—Ä–æ–≤ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ä–µ–∂–∏–º –ê–≥–µ–Ω—Ç (Victoria)."
                )
                words = fallback_response.split()
                chunk = ""
                for i, word in enumerate(words):
                    chunk += word + " "
                    if i % 3 == 0 and chunk:
                        response_parts.append(chunk)
                        yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                        chunk = ""
                if chunk:
                    response_parts.append(chunk)
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                await _save_context_if_needed()
                yield f"data: {json.dumps({'type': 'end'})}\n\n"
                return
            # –û—Ç–≤–µ—Ç –æ—Ç MLX –∏–ª–∏ Ollama
            logger.info(f"‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –æ—Ç {source}")
            content = _truncate_repeated_response(content)
            words = content.split()
            chunk = ""
            for i, word in enumerate(words):
                chunk += word + " "
                if i % 3 == 0 and chunk:
                    response_parts.append(chunk)
                    yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                    chunk = ""
                    await asyncio.sleep(0.05)
            if chunk:
                response_parts.append(chunk)
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
            asyncio.create_task(_log_chat_to_knowledge_os(message.content, content, message.expert_name))
            await _save_context_if_needed()
            yield f"data: {json.dumps({'type': 'end'})}\n\n"
            return

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ (–¥–ª—è –ø—É—Ç–µ–π –±–µ–∑ return –≤—ã—à–µ)
        yield f"data: {json.dumps({'type': 'end'})}\n\n"
        
    except Exception as e:
        logger.error(f"SSE error: {e}", exc_info=True)
        # –í–º–µ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤–∫–∏ error, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º fallback –æ—Ç–≤–µ—Ç
        expert_name = message.expert_name or "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"
        fallback_response = f"–ü—Ä–∏–≤–µ—Ç! –Ø {expert_name}. –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {str(e)[:100]}. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        words = fallback_response.split()
        chunk = ""
        for i, word in enumerate(words):
            chunk += word + " "
            if i % 3 == 0 and chunk:
                response_parts.append(chunk)
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
                chunk = ""
        if chunk:
            response_parts.append(chunk)
            yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"
        try:
            await _save_context_if_needed()
        except Exception:
            pass
        yield f"data: {json.dumps({'type': 'end'})}\n\n"


@router.post("/send", response_model=ChatResponse)
async def send_message(
    message: ChatMessage,
    victoria: VictoriaClient = Depends(get_victoria_client)
) -> ChatResponse:
    """
    –û—Ç–ø—Ä–∞–≤–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏–µ (–Ω–µ-—Å—Ç—Ä–∏–º–∏–Ω–≥)
    
    Returns:
        –û—Ç–≤–µ—Ç –æ—Ç —á–∞—Ç–∞
    """
    try:
        correlation_id = str(uuid.uuid4())
        session_id = getattr(message, "session_id", None) or getattr(message, "user_id", None)
        chat_history_vic = []
        if session_id:
            ctx_mgr = get_conversation_context_manager()
            recent = await ctx_mgr.get_recent(session_id, last_n=10)
            chat_history_vic = ctx_mgr.to_victoria_chat_history(recent)

        prompt_for_victoria = message.content
        try:
            from app.services.strategic_classifier import is_strategic_question
            is_strategic, _ = is_strategic_question(message.content)
            if is_strategic:
                settings_send = get_settings()
                board_api_url = f"{settings_send.knowledge_os_api_url.rstrip('/')}/api/board/consult"
                api_key = os.environ.get("API_KEY", "your-secret-api-key")
                async with httpx.AsyncClient(timeout=45.0) as client:
                    board_response = await client.post(
                        board_api_url,
                        json={
                            "question": message.content,
                            "session_id": session_id,
                            "user_id": getattr(message, "user_id", None),
                            "correlation_id": correlation_id,
                            "source": "chat",
                        },
                        headers={"X-API-Key": api_key},
                    )
                    board_response.raise_for_status()
                    board_result = board_response.json()
                    directive_text = board_result.get("directive_text", "")
                    if directive_text:
                        prompt_for_victoria = f"""[–†–ï–®–ï–ù–ò–ï –°–û–í–ï–¢–ê –î–ò–†–ï–ö–¢–û–†–û–í]
{directive_text}
[/–†–ï–®–ï–ù–ò–ï]

–ó–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {message.content}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è: –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ —Ä–µ—à–µ–Ω–∏–µ –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤ –≤—ã—à–µ. –ú–æ–∂–µ—à—å –Ω–∞—á–∞—Ç—å —Å —Ñ—Ä–∞–∑—ã "–ü–æ —Ä–µ—à–µ–Ω–∏—é –°–æ–≤–µ—Ç–∞ –î–∏—Ä–µ–∫—Ç–æ—Ä–æ–≤..."."""
        except Exception as e:
            logger.debug("Board consult skipped for send_message: %s", e)

        result = await victoria.run(
            prompt=prompt_for_victoria,
            expert_name=message.expert_name,
            session_id=session_id,
            chat_history=chat_history_vic if chat_history_vic else None,
            correlation_id=correlation_id,
        )
        
        if "error" in result:
            raise HTTPException(
                status_code=500,
                detail=result["error"]
            )
        
        content = result.get("result", "") or result.get("response", "")
        asyncio.create_task(_log_chat_to_knowledge_os(message.content, content, message.expert_name))
        return ChatResponse(
            content=content,
            expert_name=message.expert_name,
            model=result.get("model")
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Chat send error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Internal server error while processing chat message"
        )


@router.post("/plan", response_model=PlanResponse)
@prometheus_metrics.track_request(mode="plan", endpoint="plan")
async def get_plan(
    body: PlanRequest,
    victoria: VictoriaClient = Depends(get_victoria_client),
    plan_cache: PlanCacheService = Depends(get_plan_cache_service),
) -> PlanResponse:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–ª–∞–Ω –ø–æ –∑–∞–¥–∞—á–µ (–±–µ–∑ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è). –ö–∞–∫ –≤–∫–ª–∞–¥–∫–∞ ¬´–ü–ª–∞–Ω¬ª –≤ Cursor.
    –§–∞–∑–∞ 3: –ø—Ä–∏ –≤–∫–ª—é—á—ë–Ω–Ω–æ–º –∫—ç—à–µ –ø–æ–≤—Ç–æ—Ä–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –ø–æ —Ç–æ–º—É –∂–µ goal –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –∏–∑ –∫—ç—à–∞.
    """
    try:
        settings = get_settings()
        project_context = settings.project_name
        PLAN_REQUESTS.inc()
        t0 = time.perf_counter()
        if getattr(settings, "plan_cache_enabled", True) and plan_cache._maxsize > 0:
            cached = await plan_cache.get(body.goal, project_context)
            if cached:
                plan = cached.get("result", "") or cached.get("response", "") or ""
                if plan:
                    logger.info("[Plan] cache hit (POST /plan): '%s...'", (body.goal or "")[:40])
                    PLAN_DURATION.observe(time.perf_counter() - t0)
                    return PlanResponse(plan=plan, status="success")
        acquired = await acquire_victoria_slot()
        if not acquired:
            raise HTTPException(
                status_code=503,
                detail="Service busy (Victoria limit). Retry later.",
                headers={"Retry-After": "60"},
            )
        try:
            result = await victoria.plan(goal=body.goal, project_context=project_context)
        finally:
            release_victoria_slot()
        gen_time = time.perf_counter() - t0
        PLAN_DURATION.observe(gen_time)
        if result.get("status") == "error":
            raise HTTPException(status_code=500, detail=result.get("error", "Plan failed"))
        plan = result.get("result", "") or result.get("response", "") or ""
        steps = result.get("steps") or []
        if isinstance(steps, list) and steps:
            PLAN_STEPS_COUNT.observe(len(steps))
        min_gen = getattr(settings, "plan_cache_min_gen_time", 2.0)
        if plan and gen_time >= min_gen and plan_cache._maxsize > 0:
            await plan_cache.set(body.goal, result, project_context, ttl=getattr(settings, "plan_cache_ttl", 3600))
            logger.info("[Plan] saved to cache (POST /plan): '%s...' (gen_time=%.1fs)", (body.goal or "")[:40], gen_time)
        return PlanResponse(plan=plan, status="success")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Plan error: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail="Internal server error while getting plan")


@router.post("/stream", response_model=None)
@prometheus_metrics.track_request(mode="stream", endpoint="stream")
async def stream_message(
    message: ChatMessage,
    victoria: VictoriaClient = Depends(get_victoria_client),
    mlx: MLXClient = Depends(get_mlx_client),
    ollama: OllamaClient = Depends(get_ollama_client),
    knowledge_os: KnowledgeOSClient = Depends(get_knowledge_os_client),
):
    """
    SSE —Å—Ç—Ä–∏–º–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞ (Singularity 9.0).
    –¶–µ–ø–æ—á–∫–∞ –≤—ã–±–æ—Ä–∞: MLX ‚Üí Ollama ‚Üí Victoria.
    –ü—Ä–∏ –ø–µ—Ä–µ–≥—Ä—É–∑–∫–µ (–ª–∏–º–∏—Ç —Å–ª–æ—Ç–æ–≤ Victoria) ‚Äî 503.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç StreamingResponse –∏–ª–∏ JSONResponse(503).
    """
    acquired = await acquire_victoria_slot()
    if not acquired:
        return JSONResponse(
            status_code=503,
            content={
                "error": "service_busy",
                "detail": "Too many concurrent requests. Retry later.",
            },
            headers={"Retry-After": "60"},
        )
    plan_cache = get_plan_cache_service()
    return StreamingResponse(
        with_victoria_slot(
            sse_generator(message, victoria, mlx, ollama, knowledge_os, plan_cache)
        ),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",
        }
    )


@router.get("/status")
async def chat_status(
    victoria: VictoriaClient = Depends(get_victoria_client),
    mlx: MLXClient = Depends(get_mlx_client),
    ollama: OllamaClient = Depends(get_ollama_client),
) -> dict:
    """
    –°—Ç–∞—Ç—É—Å —Å–µ—Ä–≤–∏—Å–æ–≤ —á–∞—Ç–∞ (—Ü–µ–ø–æ—á–∫–∞: MLX ‚Üí Ollama ‚Üí Victoria).
    """
    try:
        victoria_health = await victoria.health()
        mlx_health = await mlx.health()
        ollama_health = await ollama.health()
        return {
            "victoria": victoria_health,
            "mlx": mlx_health,
            "ollama": ollama_health,
        }
    except Exception as e:
        logger.error(f"Chat status error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Error checking chat services status"
        )


@router.get("/classify")
async def classify_query_endpoint(q: str = "") -> dict:
    """
    –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ (–¥–ª—è —Ç–µ—Å—Ç–æ–≤ –∏ –æ—Ç–ª–∞–¥–∫–∏).
    GET /api/chat/classify?q=–ø—Ä–∏–≤–µ—Ç
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç classification + suggest_agent, complexity_score, complexity_reason (–§–∞–∑–∞ 2).
    """
    from app.services.query_classifier import classify_query, get_template_response, analyze_complexity
    classification = analyze_complexity(q)
    template = get_template_response(q, None) if classification.get("type") == "simple" else None
    return {
        "query": q[:200],
        "classification": classification,
        "template_response": template,
    }


@router.get("/agent-suggestions/stats")
async def agent_suggestions_stats() -> dict:
    """
    –ú–µ—Ç—Ä–∏–∫–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–µ—Ä–µ—Ö–æ–¥–∞ –≤ —Ä–µ–∂–∏–º –ê–≥–µ–Ω—Ç (–§–∞–∑–∞ 2, –¥–µ–Ω—å 3‚Äì4).
    GET /api/chat/agent-suggestions/stats
    """
    return agent_suggestion_metrics.get_stats()


@router.get("/mode/health")
async def mode_health(
    victoria: VictoriaClient = Depends(get_victoria_client),
    mlx: MLXClient = Depends(get_mlx_client),
    ollama: OllamaClient = Depends(get_ollama_client),
) -> dict:
    """
    –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –±—ç–∫–µ–Ω–¥–æ–≤ –ø–æ —Ä–µ–∂–∏–º–∞–º (Ask / Agent / Plan).
    –î–ª—è –≥—Ä–∞–¥–∞—Ü–∏–∏ –ø—É—Ç–µ–π –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞.
    """
    try:
        victoria_health = await victoria.health()
        mlx_health = await mlx.health()
        ollama_health = await ollama.health()
        v_ok = victoria_health.get("status") == "ok"
        m_ok = mlx_health.get("status") in ("healthy", "degraded")
        o_ok = ollama_health.get("status") == "healthy"
        return {
            "ask": {
                "mlx": mlx_health.get("status", "unknown"),
                "ollama": ollama_health.get("status", "unknown"),
                "victoria_fallback": victoria_health.get("status", "unknown"),
                "hot_path_available": True,
                "_available": m_ok or o_ok or v_ok,  # —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –±—ç–∫–µ–Ω–¥ –¥–ª—è Ask
            },
            "agent": {
                "victoria": victoria_health.get("status", "unknown"),
                "fallback_mlx_ollama": m_ok or o_ok,
                "_available": v_ok or (m_ok or o_ok),  # Victoria –∏–ª–∏ fallback
            },
            "plan": {
                "victoria_plan": victoria_health.get("status", "unknown"),
                "_available": v_ok,
            },
        }
    except Exception as e:
        logger.error(f"Mode health error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Error checking mode health"
        )


@router.get("/mlx/metrics")
async def get_mlx_metrics(
    mlx: MLXClient = Depends(get_mlx_client)
) -> dict:
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ MLX API Server
    
    Returns:
        –ü–æ–ª–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∑–∫–µ, –ø–∞–º—è—Ç–∏, –º–æ–¥–µ–ª—è—Ö –∏ –∑–∞–ø—Ä–æ—Å–∞—Ö
    """
    try:
        mlx_health = await mlx.health()
        return mlx_health
    except Exception as e:
        logger.error(f"MLX metrics error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Error getting MLX metrics: {str(e)}"
        )


@router.get("/models")
async def list_models(
    mlx: MLXClient = Depends(get_mlx_client)
) -> dict:
    """
    –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    Returns:
        –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π MLX
    """
    cache = get_cache()
    cache_key = "mlx:models"
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cached = cache.get(cache_key)
    if cached is not None:
        return cached
    
    try:
        mlx_health = await mlx.health()
        # MLX API Server –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –≤ health
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –∏–∑ MLX API Server
        import httpx
        async with httpx.AsyncClient(timeout=5.0) as client:
            mlx_response = await client.get("http://localhost:11435/api/tags")
            if mlx_response.status_code == 200:
                mlx_data = mlx_response.json()
                models_list = mlx_data.get("models", [])
            else:
                models_list = []
        
        result = {
            "models": [
                {
                    "name": m.get("name", "unknown"),
                    "size": m.get("size"),
                    "modified": m.get("modified_at")
                }
                for m in models
            ]
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à (1 –º–∏–Ω—É—Ç–∞)
        cache.set(cache_key, result, ttl=60)
        
        return result
    except Exception as e:
        logger.error(f"List models error: {e}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail="Error fetching models list"
        )
