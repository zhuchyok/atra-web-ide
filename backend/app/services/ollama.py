"""
Ollama Client (–£–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)
HTTP –∫–ª–∏–µ–Ω—Ç –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö LLM –º–æ–¥–µ–ª–µ–π
Retry logic, timeout handling, connection pooling
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ Ollama Cloud Models –∏ Claude Code Integration
"""
import os
import httpx
from typing import AsyncGenerator, Optional, List
import logging
import json
import asyncio

from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class OllamaClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è Ollama API —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Cloud Models"""
    
    # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ Mac Studio M4 Max (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä)
    MODELS = {
        "complex": "command-r-plus:104b",              # ~65GB - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ—â–Ω–æ—Å—Ç—å
        "enterprise": "command-r-plus:104b",            # ~65GB - RAG, –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ—Å—Ç—å
        "reasoning": "deepseek-r1-distill-llama:70b", # ~40GB - Reasoning, –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
        "complex_alt": "llama3.3:70b",                 # ~40GB - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        "coding": "qwen2.5-coder:32b",                 # ~20GB - –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –∫–æ–¥
        "fast": "phi3.5:3.8b",                         # ~2.5GB - –ë—ã—Å—Ç—Ä—ã–µ –∑–∞–¥–∞—á–∏
        "fast_light": "phi3:mini-4k",                  # ~2GB - –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã
        "default": "qwen2.5:3b",                       # ~2GB - –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        "tiny": "tinyllama:1.1b-chat"                  # ~700MB - –û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–µ
    }
    
    # Cloud Models (Ollama Cloud - —Ä–∞–±–æ—Ç–∞—é—Ç –±–µ–∑ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ GPU)
    CLOUD_MODELS = {
        "cloud_large": "gpt-oss:120b-cloud",           # 120B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - Cloud
        "cloud_medium": "gpt-oss:20b-cloud",           # 20B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ - Cloud
        "cloud_coding": "qwen3-coder-cloud",           # Coding –º–æ–¥–µ–ª—å - Cloud
        "cloud_reasoning": "glm-4.7-cloud"             # Reasoning –º–æ–¥–µ–ª—å - Cloud
    }
    
    # Claude Code —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    CLAUDE_CODE_MODELS = [
        "qwen3-coder",
        "glm-4.7",
        "gpt-oss:20b",
        "gpt-oss:120b"
    ]
    
    # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —á–∞—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
    FAST_MODEL = "qwen2.5:3b"  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±—ã—Å—Ç—Ä—É—é –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def __init__(self, base_url: Optional[str] = None, use_cloud: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            base_url: URL Ollama —Å–µ—Ä–≤–µ—Ä–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏–∑ settings)
            use_cloud: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Ollama Cloud API (https://ollama.com)
        """
        if use_cloud:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Ollama Cloud API
            self.base_url = "https://ollama.com"
            self.api_key = os.getenv("OLLAMA_API_KEY")
            if not self.api_key:
                logger.warning("‚ö†Ô∏è OLLAMA_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–ª—è Cloud API")
        else:
            self.base_url = base_url or settings.ollama_url
            self.api_key = None
        
        self.timeout = httpx.Timeout(
            settings.ollama_timeout,
            connect=10.0
        )
        self.max_retries = 2
        self.retry_delay = 2.0
        self.use_cloud = use_cloud
    
    async def _retry_request(self, func, *args, **kwargs):
        """–ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–∞"""
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                return await func(*args, **kwargs)
            except (httpx.HTTPError, httpx.TimeoutException) as e:
                last_error = e
                if attempt < self.max_retries - 1:
                    delay = self.retry_delay * (2 ** attempt)
                    logger.warning(
                        f"Ollama request failed (attempt {attempt + 1}/{self.max_retries}), "
                        f"retrying in {delay}s: {e}"
                    )
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Ollama request failed after {self.max_retries} attempts: {e}")
        
        raise last_error
    
    async def list_models(self) -> List[dict]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        async def _make_request():
            headers = {}
            if self.use_cloud and self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
            
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/api/tags",
                    headers=headers
                )
                response.raise_for_status()
                return response.json()
        
        try:
            data = await self._retry_request(_make_request)
            return data.get("models", [])
        except httpx.HTTPError as e:
            logger.error(f"Ollama list_models error: {e}")
            return []
    
    async def pull_model(self, model_name: str) -> dict:
        """
        –ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å (pull) –∏–∑ Ollama
        
        Args:
            model_name: –ò–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "gpt-oss:120b-cloud")
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏
        """
        async def _make_request():
            headers = {}
            if self.use_cloud and self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(300.0)) as client:
                response = await client.post(
                    f"{self.base_url}/api/pull",
                    json={"name": model_name},
                    headers=headers
                )
                response.raise_for_status()
                return response.json()
        
        try:
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
            return await self._retry_request(_make_request)
        except httpx.HTTPError as e:
            logger.error(f"Ollama pull_model error: {e}")
            return {"error": str(e)}
    
    def get_anthropic_compatible_config(self) -> dict:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è Anthropic-compatible API (Claude Code)
        
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è Claude Code
        """
        return {
            "ANTHROPIC_AUTH_TOKEN": "ollama",
            "ANTHROPIC_API_KEY": "",
            "ANTHROPIC_BASE_URL": self.base_url or "http://localhost:11434"
        }
    
    async def generate(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None,
        stream: bool = False
    ) -> dict:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏
        
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            model: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            system: –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            stream: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–∏–º–∏–Ω–≥
        
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        model = model or settings.default_model
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        if system:
            payload["system"] = system
        
        async def _make_request():
            headers = {}
            if self.use_cloud and self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
            
            logger.info(f"üì§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –≤ Ollama: {self.base_url}/api/generate")
            logger.info(f"üì¶ Payload: model={payload.get('model')}, prompt_length={len(payload.get('prompt', ''))}")
            async with httpx.AsyncClient(timeout=self.timeout) as client:
                response = await client.post(
                    f"{self.base_url}/api/generate",
                    json=payload,
                    headers=headers
                )
                logger.info(f"üì• –û—Ç–≤–µ—Ç Ollama: HTTP {response.status_code}")
                if response.status_code != 200:
                    logger.error(f"‚ùå –û—à–∏–±–∫–∞ Ollama: {response.status_code} - {response.text[:200]}")
                response.raise_for_status()
                return response.json()
        
        try:
            return await self._retry_request(_make_request)
        except httpx.HTTPError as e:
            logger.error(f"Ollama generate error: {e}")
            return {"error": str(e)}
    
    async def generate_stream(
        self,
        prompt: str,
        model: Optional[str] = None,
        system: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        –°—Ç—Ä–∏–º–∏–Ω–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
        Yields:
            JSON —Å—Ç—Ä–æ–∫–∏ —Å —á–∞—Å—Ç—è–º–∏ –æ—Ç–≤–µ—Ç–∞
        """
        model = model or settings.default_model
        
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": True
        }
        if system:
            payload["system"] = system
        
        headers = {}
        if self.use_cloud and self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"
        
        async with httpx.AsyncClient(timeout=self.timeout) as client:
            try:
                async with client.stream(
                    "POST",
                    f"{self.base_url}/api/generate",
                    json=payload,
                    headers=headers
                ) as response:
                    response.raise_for_status()
                    async for line in response.aiter_lines():
                        if line:
                            yield line
            except httpx.HTTPError as e:
                logger.error(f"Ollama stream error: {e}")
                yield json.dumps({"error": str(e)})
    
    async def health(self) -> dict:
        """Health check Ollama"""
        async def _make_request():
            headers = {}
            if self.use_cloud and self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
            
            async with httpx.AsyncClient(timeout=httpx.Timeout(5.0)) as client:
                response = await client.get(
                    f"{self.base_url}/api/tags",
                    headers=headers
                )
                response.raise_for_status()
                return {"status": "healthy", "mode": "cloud" if self.use_cloud else "local"}
        
        try:
            return await self._retry_request(_make_request)
        except httpx.HTTPError as e:
            return {"status": "unhealthy", "error": str(e), "mode": "cloud" if self.use_cloud else "local"}


# Singleton instance
ollama_client = OllamaClient()


async def get_ollama_client() -> OllamaClient:
    """Dependency –¥–ª—è FastAPI"""
    return ollama_client
